{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "174c4f90-96d6-4c38-aacb-b3cf8b774d3b",
   "metadata": {},
   "source": [
    "Here we:\n",
    "- Generate P2,P3,P4,P5,P6"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1525888-6665-481b-8791-3fe88a8c5574",
   "metadata": {},
   "source": [
    "Future Tasks:\n",
    "\n",
    "- Can use the code here to get higher MSA functions, where we avoid generating all of Pn.\n",
    "We do this by, if for example we want $g_n \\in \\rho^{-1}(g_{n-1})$ we need to make choices of which $g_i \\in \\rho^{-1}(g_{i-1})$ for $2 \\le i \\le n-1$ to take."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be280e86-74e7-4066-bb2b-466dd64f7522",
   "metadata": {
    "tags": [],
    "toc-hr-collapsed": true
   },
   "source": [
    "# Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "9a2f9a56-4741-4b92-92f2-8923095041b5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import itertools\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "import time\n",
    "\n",
    "#General use functions:\n",
    "def set_to_string(s):\n",
    "    s = list(s)\n",
    "    s.sort()\n",
    "    # Convert the list of numbers to a list of strings\n",
    "    my_list=s\n",
    "    my_list = [str(i) for i in my_list]\n",
    "    # Use the join() function to convert the list of strings to a single string\n",
    "    result = ''.join(my_list)\n",
    "    return result\n",
    "def string_to_set(s):\n",
    "    my_list = list(s)\n",
    "    # Convert the list of strings to a list of integers\n",
    "    my_list = list(map(int,my_list))\n",
    "    # Convert the list to a set\n",
    "    my_set = set(my_list)\n",
    "    return my_set\n",
    "\n",
    "def get_powerset_str(n):\n",
    "    s=set(range(1,n+1))\n",
    "    power_set = [set(x)  for r in range(len(s) + 1) for x in itertools.combinations(s, r)]\n",
    "    power_set_str=[set_to_string(x) for x in power_set if len(x)>0]\n",
    "\n",
    "    return power_set_str\n",
    "\n",
    "def get_n_np1_powersets(n):\n",
    "    s=set(range(1,n+1))\n",
    "    power_set = [set(x)  for r in range(len(s) + 1) for x in itertools.combinations(s, r)]\n",
    "    power_set_str=[set_to_string(x) for x in power_set if len(x)>0]\n",
    "\n",
    "    np1=n+1\n",
    "    sp1=set(range(1,n+2))\n",
    "    power_set_np1 = [set(x)  for r in range(np1+1 + 1) for x in itertools.combinations(sp1, r)]\n",
    "    power_set_str_np1=[set_to_string(x) for x in power_set_np1 if len(x)>0]\n",
    "\n",
    "    return power_set_str,power_set_str_np1\n",
    "def sorted_frozenset(s):\n",
    "    t=sorted(s)\n",
    "    r=frozenset(t)\n",
    "    return r\n",
    "\n",
    "# Functions for primitive closed sets:\n",
    "def fmin(A,B,f_dict):\n",
    "    #A included in B\n",
    "    # global f_dict\n",
    "    \n",
    "    valA=f_dict[A]\n",
    "    valB=f_dict[B]\n",
    "    compBA=set_to_string(string_to_set(B)-string_to_set(A)) # B minus A\n",
    "    if len(compBA)==0: # to avoide when taking the complement gives empty set\n",
    "        return False\n",
    "    valBA=f_dict[compBA]\n",
    "    if valB==valA+valBA:\n",
    "        return True\n",
    "    else:\n",
    "        return False\n",
    "def fmax(A,B,f_dict):\n",
    "    #A included in B\n",
    "    # global f_dict\n",
    "    \n",
    "    valA=f_dict[A]\n",
    "    valB=f_dict[B]\n",
    "    compBA=set_to_string(string_to_set(B)-string_to_set(A)) # B minus A\n",
    "    \n",
    "    #A has to be a subset of B first\n",
    "    if len(compBA)==0: # to avoide when taking the complement gives empty set\n",
    "        return False\n",
    "    valBA=f_dict[compBA]\n",
    "    if valB==valA+valBA+1:\n",
    "        return True\n",
    "    else:\n",
    "        return False\n",
    "def next_j(ji,f_dict):\n",
    "    #ji is the ith set on constructing the closure. \n",
    "    # print(\"ji\",ji)\n",
    "    # global f\n",
    "    my_list=[]    \n",
    "    for x in power_set_str:\n",
    "        for y in ji:\n",
    "            if fmin(y,x,f_dict) and string_to_set(y).issubset(string_to_set(x)):\n",
    "                # print(\"y is\",y)\n",
    "                my_list.append(x)\n",
    "                # print(f\"{x} containing {y} f min \")\n",
    "    for x in power_set_str:\n",
    "        for y in ji:\n",
    "            if fmax(x,y,f_dict) and string_to_set(x).issubset(string_to_set(y)):\n",
    "                my_list.append(x)\n",
    "    jip1=set(my_list).union(ji)\n",
    "    return jip1    \n",
    "def rec_j(T,S,f_dict):\n",
    "    # T=Ji and S=Ji-1, |T|\\ge |S|.\n",
    "    C=string_to_set(T)-string_to_set(S) #T \\S\n",
    "    C={str(x) for x in C}    \n",
    "    # print(\"C\",C,f\"S term {S}\",f\"T term {T}\")\n",
    "    if len(C)==0: #Check if closed set\n",
    "        return T # return closed set\n",
    "    if len(C)>0:\n",
    "        U=next_j(T,f_dict) #next_j(T)\n",
    "        Z=rec_j(U,T,f_dict)\n",
    "    # print(T)\n",
    "    return Z\n",
    "def get_barj(J,f_dict):\n",
    "    j0=J\n",
    "    j1=next_j(j0,f_dict)\n",
    "    T=rec_j(j1,j0,f_dict)\n",
    "    return T\n",
    "\n",
    "# Main: making use of primitives to get all closed sets:\n",
    "def Recur(Ibar,indicators,Eg,Memory,dict_PC):\n",
    "    \"\"\"\n",
    "    inputs:\n",
    "    returns:\n",
    "    \"\"\"\n",
    "    # global dict_PC\n",
    "        \n",
    "    # To save recalulating we uses this series of checks, if fails checks end recursion turn.\n",
    "    \n",
    "    if indicators in Memory:# Memortisation for preventing repeated calculations.\n",
    "        # print(\"Fails Memory check\")\n",
    "        # print(f\"\\n At this failure we have \\n Eg: {len(Eg)} \\n Memory: {len(Memory)} \\n\")\n",
    "        return Eg,Memory\n",
    "    \n",
    "    y=indicators[-1]\n",
    "    # print(f\"term taken for primitive closed set we are unioning {y}\")\n",
    "    P_y=dict_PC[y] # the primitive closed set wrt y\n",
    "    Jbar= Ibar.union(P_y)\n",
    "    \n",
    "    if len(Jbar)==len(power_set_str): # We always have the power set in Eg\n",
    "        # print(f\"Fails as {Jbar} == poweset\")\n",
    "        # print(f\"\\n At this failure we have \\n Eg: {len(Eg)} \\n Memory: {len(Memory)} \\n\")\n",
    "        return Eg,Memory\n",
    "              \n",
    "    if Jbar in Eg:\n",
    "        # print(f\"Fails as {Jbar} in Eg\")\n",
    "        # print(f\"\\n At this failure we have \\n Eg: {len(Eg)} \\n Memory: {len(Memory)} \\n\")\n",
    "        return Eg,Memory\n",
    "    else:\n",
    "        # print(\"Passes Checks \\n\")\n",
    "        #Add the new data to memory\n",
    "        set_indicators=set([sorted_frozenset(tuple(indicators)),])\n",
    "        Memory=Memory.union(set_indicators)\n",
    "        \n",
    "        Jbar_union=set([sorted_frozenset(Jbar),])#Correct format for union\n",
    "        #Add new data to Eg\n",
    "        Eg=Eg.union(Jbar_union) # will ge an issue with where defines and globals.\n",
    "\n",
    "        #Move onto the next recursion.\n",
    "        power_complment=set(power_set_str)-Jbar\n",
    "        for w in power_complment: # pick one in the complement to to continus exhaustive method\n",
    "            new_indicators=indicators+(w,) #want to add it at end\n",
    "            Eg,Memory=Recur(Jbar,new_indicators,Eg,Memory,dict_PC)\n",
    "            # print(f\"Done with {new_indicators} \\n\")\n",
    "                \n",
    "    return Eg,Memory # will also be an issue.\n",
    "def get_Eg(dict_PC,power_set_str,set_PC):\n",
    "    \n",
    "    Eg={frozenset(power_set_str)}.union(set_PC) #empty set is an issue. #Main set we wish to construct\n",
    "    Memory={sorted_frozenset((\"0\"))}    # contains indicators which are tuples of x \\in powerset for primitives sets.\n",
    "    \n",
    "    for x in power_set_str:\n",
    "        Ibar=dict_PC[x] #primitive closed set\n",
    "        power_complment=set(power_set_str)-Ibar\n",
    "        for y in power_complment: #exhaustively getting all closed sets.\n",
    "            indicators=(x,y)\n",
    "            set_indicators=set([sorted_frozenset(tuple(indicators)),])\n",
    "            Memory=Memory.union(set_indicators)            \n",
    "            Eg,Memory=Recur(Ibar,indicators,Eg,Memory,dict_PC)\n",
    "            \n",
    "    return Eg\n",
    "\n",
    "# Creating functions in $P_{n+1}$ from Eg for this f_dict.\n",
    "def add_n_p_1_to_string(string,n):\n",
    "    #Adds n+1 to strings subsets\n",
    "    np1string=string+(f\"{n+1}\")\n",
    "    return np1string\n",
    "\n",
    "def get_extension_of_f(f_dict,extender,power_set_str,n):\n",
    "    # Need to attatch n+1 to strings to build extension of function by epsilon.\n",
    "\n",
    "    # Build the extension of f by epsilon.\n",
    "    \n",
    "    # Add the (key,values) of f_dict\n",
    "    builder_ext_0=[(k, f_dict[k]) for k in f_dict]\n",
    "\n",
    "    #We add 1 to the function value of f_dict for x in extender\n",
    "    builder_ext_p1=[(add_n_p_1_to_string(x,n),f_dict[x]+1) for x in list(extender)]\n",
    "\n",
    "    #We add 0 to the function value of f_dict for x NOT in extender\n",
    "    compl_extender=set(power_set_str) - set(extender)\n",
    "    builder_ext_p2=[(add_n_p_1_to_string(x,n),f_dict[x]+0) for x in list(compl_extender)]\n",
    "    \n",
    "    # We include f({n+1})=0\n",
    "    builder_ext_np1=[(add_n_p_1_to_string(\"\",n),int(0)),]\n",
    "\n",
    "\n",
    "    #Compile together.\n",
    "    extension_f_dict={**dict(builder_ext_0),**dict(builder_ext_p1),**dict(builder_ext_p2),**dict(builder_ext_np1)}\n",
    "    \n",
    "    #Example\n",
    "    # get_extension_of_f(frozenset({'13', '23'}))\n",
    "\n",
    "    return extension_f_dict\n",
    "def empty_case_get_extension_of_f(f_dict,extender,power_set_str,n):\n",
    "    # Need to attatch n+1 to strings to build extension of function by epsilon.\n",
    "\n",
    "    # Build the extension of f by epsilon.\n",
    "    \n",
    "    # Add the (key,values) of f_dict\n",
    "    builder_ext_0=[(k, f_dict[k]) for k in f_dict]\n",
    "\n",
    "    builder_ext_p2=[(add_n_p_1_to_string(x,n),f_dict[x]+0) for x in list(power_set_str)]\n",
    "\n",
    "    builder_ext_np1=[(add_n_p_1_to_string(\"\",n),int(0)),]\n",
    "\n",
    "    \n",
    "    #Compile together.\n",
    "    extension_f_dict={**dict(builder_ext_0),**dict(builder_ext_p2),**dict(builder_ext_np1)}\n",
    "    \n",
    "    return extension_f_dict\n",
    "\n",
    "def Build_Pn_1(list_df_Pn,power_set_str,n):\n",
    "    # Returns:packets of extensions for each f_dict from Pn, can check those with max size of closed sets (later)\n",
    "    \n",
    "    # global power_set_str\n",
    "    \n",
    "    P_n1=[]\n",
    "    for f_dict in list_df_Pn:\n",
    "\n",
    "        # Store primitive closed sets \n",
    "\n",
    "        PC=[(x,get_barj({x},f_dict)) for x in power_set_str]\n",
    "        dict_PC=dict(PC)\n",
    "        set_PC=[sorted_frozenset(get_barj({x},f_dict)) for x in power_set_str]\n",
    "\n",
    "        #Get all closed sets for extensions for f_dict\n",
    "        Eg=list(get_Eg(dict_PC,power_set_str,set_PC))\n",
    "        Eg.append(\"empty\")\n",
    "\n",
    "        #Build extensions for f_dict\n",
    "        extension_of_f_dict=[]\n",
    "        for term in Eg:\n",
    "            if term ==\"empty\":\n",
    "                extension_of_f_dict.append(empty_case_get_extension_of_f(f_dict,term,power_set_str,n))\n",
    "            else:\n",
    "                extension_of_f_dict.append(get_extension_of_f(f_dict,term,power_set_str,n))\n",
    "\n",
    "        #We record packets of extensions where we take +1\n",
    "        P_n1.append(extension_of_f_dict)\n",
    "    \n",
    "    return P_n1\n",
    "\n",
    "def extend_sets_to_Pnp1(P_n1):\n",
    "    #Return the data frame of Pn+1\n",
    "    \n",
    "    final_P_n1=[]\n",
    "    for ls in P_n1:\n",
    "        final_P_n1=final_P_n1+ls # Adding all the extension packets together.\n",
    "    df=pd.DataFrame(final_P_n1)\n",
    "    # df = df.reindex(columns=power_set_str_np1)\n",
    "    \n",
    "    return df\n",
    "\n",
    "# This function returns Pn+1\n",
    "\n",
    "\n",
    "def get_Pnp1(Pn,n):\n",
    "\n",
    "    #inputs\n",
    "    # Pn and n.\n",
    "\n",
    "    #New data created\n",
    "    np1=n+1\n",
    "    power_set_str,power_set_str_np1=get_n_np1_powersets(n)\n",
    "\n",
    "    #Load Pn as a list of dictionaries.\n",
    "    list_df_Pn=Pn.to_dict(orient='records')\n",
    "\n",
    "    #Create Pn+1 and hold as dataframe.\n",
    "    P_n1=Build_Pn_1(list_df_Pn,power_set_str,n)\n",
    "    Pnp1=extend_sets_to_Pnp1(P_n1)\n",
    "\n",
    "    Pnp1 = Pnp1.reindex(columns=power_set_str_np1)\n",
    "\n",
    "    inspect_Pn1(Pnp1)\n",
    "\n",
    "    return Pnp1\n",
    "\n",
    "# Analysis of Pn+1\n",
    "\n",
    "def string_cols(df):\n",
    "    \n",
    "    #Changing them to string format!only\n",
    "    #reorders the columns of dataframe to powerset list order.\n",
    "    \n",
    "    cols=df.columns\n",
    "    rnamer=dict([(cols[i],str(cols[i])) for i in range(0,len(cols))])\n",
    "    df=df.rename(columns=rnamer)\n",
    "    \n",
    "    return df\n",
    "\n",
    "def inspect_Pn1(df):\n",
    "    print(\"Number of functions:\",len(df.index))\n",
    "    # df=df.unique()\n",
    "\n",
    "    duplicate_rows = df[df.duplicated()]\n",
    "    print(\"Are there duplicates?\",duplicate_rows.shape, \"if (0,-) then no common rows\")\n",
    "\n",
    "    # df.head()\n",
    "    return\n",
    "\n",
    "def check_df_duplicates(df1,df2,How,value):\n",
    "    #Compare df1,df2 if that common rows\n",
    "    \n",
    "    # how is either \"right\" or \"left\"\n",
    "    # for left gives those in df1 and says True or false if also in df2\n",
    "    \n",
    "    #     example\n",
    "    #     df1 = pd.DataFrame({'team' : ['A', 'B', 'C', 'D', 'E'], \n",
    "    #                     'points' : [12, 15, 22, 29, 24]}) \n",
    "    #  #create second DataFrame\n",
    "    # df2 = pd.DataFrame({'team' : ['A', 'D', 'F', 'G', 'H'],\n",
    "    #                     'points' : [12, 29, 15, 19, 10]})\n",
    "    \n",
    "    #merge two dataFrames and add indicator column\n",
    "    all_df = pd.merge(df1, df2, how=How, indicator='exists')\n",
    "\n",
    "    #add column to show if each row in first DataFrame exists in second\n",
    "    all_df['exists'] = np.where(all_df.exists == 'both', True, False)\n",
    "\n",
    "    #view updated DataFrame\n",
    "    # print (all_df)\n",
    "\n",
    "    m=all_df.loc[all_df['exists'] == True]\n",
    "    \n",
    "    #Number of terms common\n",
    "    num_items = m.loc[m['exists'] == value].shape[0]\n",
    "    print(f\"Number of those in both: {num_items} \\n\")\n",
    "\n",
    "    return m    \n",
    "\n",
    "def is_valid_function(f,power_set_str):\n",
    "    # define the condition to check for each function\n",
    "    Pn=[string_to_set(x) for x in power_set_str] #convert to sets\n",
    "    \n",
    "    # f(T) must equal 0 for elements T in Pn with size 1\n",
    "\n",
    "    for T in Pn:\n",
    "        if any(f[set_to_string(T)] != 0 for T in Pn if len(T) == 1):\n",
    "            return False\n",
    "    # for any I, J in Pn such that I and J have no common elements, 1 > f(I U J) - f(I) - f(J) or 0 = f(I U J) - f(I) - f(J)\n",
    "    for I in Pn:\n",
    "        for J in Pn:\n",
    "            if len(I.intersection(J)) == 0 and f[set_to_string(I.union(J))] - f[set_to_string(I)] - f[set_to_string(J)] < 0 or f[set_to_string(I.union(J))] - f[set_to_string(I)] - f[set_to_string(J)] > 1:\n",
    "            # for any I, J in Pn such that I and J have no common elements, 0 <= f(I U J) - f(I) - f(J) <= 1\n",
    "                return False\n",
    "    return True\n",
    "\n",
    "def check_functs_MSA(df,power_set_str):\n",
    "\n",
    "    num_rows=df.shape[0]\n",
    "\n",
    "    bad_f_in_df=[] #indices of functions fail to be MSA in df\n",
    "    for i in range(0,num_rows):\n",
    "        f=df.loc[i].to_dict()\n",
    "        if  is_valid_function(f,power_set_str)==False:\n",
    "            bad_f_in_df.append(f)\n",
    "    \n",
    "    bad_df=pd.DataFrame(bad_f_in_df)\n",
    "    \n",
    "    return bad_df #indices of functions that are not MSA"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f323cfe-d650-4f13-858a-26ba3ad73953",
   "metadata": {
    "tags": [],
    "toc-hr-collapsed": true
   },
   "source": [
    "# P2 extending to P3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a3b210af-29e7-49ec-b1cf-e6aa08813037",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of functions: 10\n",
      "Are there duplicates? (0, 7) if (0,-) then no common rows\n",
      "   1  2  3  12  13  23  123\n",
      "0  0  0  0   0   1   0    1\n",
      "1  0  0  0   0   0   0    1\n",
      "2  0  0  0   0   1   1    1\n",
      "3  0  0  0   0   0   1    1\n",
      "4  0  0  0   0   0   0    0\n",
      "5  0  0  0   1   0   1    1\n",
      "6  0  0  0   1   1   1    2\n",
      "7  0  0  0   1   1   0    1\n",
      "8  0  0  0   1   1   1    1\n",
      "9  0  0  0   1   0   0    1\n"
     ]
    }
   ],
   "source": [
    "#Easy to know $P_2$\n",
    "P2=pd.DataFrame([{'1': 0, '2': 0, '12': 0},{'1': 0, '2': 0, '12': 1}])\n",
    "\n",
    "n=2\n",
    "power_set_str,power_set_str_np1=get_n_np1_powersets(n)\n",
    "P3=get_Pnp1(P2,2)\n",
    "print(P3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c403d0a-8acc-481d-bc27-bc942375e9c4",
   "metadata": {
    "tags": [],
    "toc-hr-collapsed": true
   },
   "source": [
    "# P3 extending to P4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6c271174-ad92-4738-b34b-29234e062c8d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of functions: 154\n",
      "Are there duplicates? (0, 15) if (0,-) then no common rows\n"
     ]
    }
   ],
   "source": [
    "n=3\n",
    "power_set_str,power_set_str_np1=get_n_np1_powersets(n)\n",
    "P4=get_Pnp1(P3,3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58495063-b7c8-40bf-995e-5de3b4548245",
   "metadata": {
    "tags": [
     "p3"
    ],
    "toc-hr-collapsed": true
   },
   "source": [
    "All the functions in P4 are MSA? Yes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ad7f30ea-6127-4071-8e49-cbccd6a74066",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0, 0)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "power4=get_powerset_str(4) #powerset on 4 elements\n",
    "check_functs_MSA(P4,power4).shape\n",
    "#Yes (0, 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c99dd4c3-6ed0-40a1-85d6-5e1165e07cf4",
   "metadata": {
    "tags": []
   },
   "source": [
    "# P4 extending to P5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b75fd0fa-23f4-4ae6-893a-533692d9c75d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of functions: 10334\n",
      "Are there duplicates? (0, 31) if (0,-) then no common rows\n"
     ]
    }
   ],
   "source": [
    "n=4\n",
    "power_set_str,power_set_str_np1=get_n_np1_powersets(n) #gives powerset on 4 and 5 elements respectively\n",
    "P5=get_Pnp1(P4,4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "923a32b6-d5c9-4def-96cc-445206895206",
   "metadata": {},
   "source": [
    "We check if all functions are MSA: Yes they are"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6eba7f17-0273-4bbd-aaaf-4740da3d6331",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# bad_P5=check_functs_MSA(P5,power_set_str_np1) #.shape #(36232, 31) fail to be MSA apparently\n",
    "# bad_P5.shape # (0, 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ddb821f-bfa9-4484-85a3-3a2a76d5c3bd",
   "metadata": {
    "tags": [],
    "toc-hr-collapsed": true
   },
   "source": [
    "# P5 extending to P6: Too long"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97294214-f4de-4658-8f83-afd5aa675080",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# n=5\n",
    "# power_set_str,power_set_str_np1=get_n_np1_powersets(n) #gives powerset on 4 and 5 elements respectively\n",
    "\n",
    "# P6=get_Pnp1(P5,5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "140b70ac-d156-4ad8-9b95-d28921016c43",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Build P6 by extending all parts of P5 (Computationally easier)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5f3d98e-81ae-450e-a18b-2ac6954ffb1b",
   "metadata": {},
   "source": [
    "As P6=get_Pnp1(P5,5) takes too long we break the construction into parts.\n",
    "\n",
    "As a result we get the function get_Pnp1_for_single_funct that allows one to obtain functions for higher Pn (I have not written this but it is easy to do if wanted).\n",
    "\n",
    "During this process we record the number of extension each function in P5 has, this will be used to give the sum\n",
    "\n",
    "$$|P_6|=\\sum_{g \\in P_5} |\\rho^{-1}(g)|$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "44f4eac8-c5f2-4ca3-918f-7994358f0986",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_Eg_for_single_g(df,numb,power_set_str):\n",
    "    #Obj: puts Eg into correct format to check if topolgoy\n",
    "    #Inputs: df=Pn, numb used to get a specific function from df\n",
    "    \n",
    "    # Pick function in Pn\n",
    "    g=df.iloc[numb].to_dict()\n",
    "\n",
    "    #get primitive closed sets\n",
    "    PC=[(x,get_barj({x},g)) for x in power_set_str]\n",
    "    dict_PC=dict(PC)\n",
    "    set_PC=[sorted_frozenset(get_barj({x},g)) for x in power_set_str]\n",
    "\n",
    "    #Get all closed sets for extensions for g\n",
    "    Eg=list(get_Eg(dict_PC,power_set_str,set_PC))\n",
    "    Eg.append(frozenset()) #rembmer the empty set\n",
    "    return Eg\n",
    "\n",
    "def get_Eg_column_Pn(df):\n",
    "    df=df.copy()\n",
    "\n",
    "    # Create a new column called \"new_column\"\n",
    "    df['Eg_Size'] = None\n",
    "\n",
    "    # Insert data into the new column\n",
    "    for i, row in df.iterrows():    \n",
    "        Eg=get_Eg_for_single_g(df,i,power_set_str)\n",
    "        df.at[i, 'Eg_Size'] = int(len(Eg))\n",
    "\n",
    "    return df\n",
    "\n",
    "def eg_column_Build_Pn_1(list_df_Pn,power_set_str,n):\n",
    "    \n",
    "    \"\"\"\n",
    "    Obj:Use Build_Pn_1 and modify change to include Eg size column in functions\n",
    "    \"\"\"\n",
    "    \n",
    "    # Returns:packets of extensions for each f_dict from Pn, can check those with max size of closed sets (later)\n",
    "    \n",
    "    P_n1=[]\n",
    "    for index,f_dict in enumerate(list_df_Pn):\n",
    "\n",
    "        # Store primitive closed sets \n",
    "        PC=[(x,get_barj({x},f_dict)) for x in power_set_str]\n",
    "        dict_PC=dict(PC)\n",
    "        set_PC=[sorted_frozenset(get_barj({x},f_dict)) for x in power_set_str]\n",
    "\n",
    "        #Get all closed sets for extensions for f_dict\n",
    "        Eg=list(get_Eg(dict_PC,power_set_str,set_PC))\n",
    "        Eg.append(\"empty\")\n",
    "        \n",
    "        Eg_col_kvpair={\"Eg_Size\":len(Eg)}\n",
    "\n",
    "        #Build extensions for f_dict\n",
    "        extension_of_f_dict=[]\n",
    "        for term in Eg:\n",
    "            if term ==\"empty\":                    \n",
    "                funct=empty_case_get_extension_of_f(f_dict,term,power_set_str,n) #dictionary\n",
    "                funct.update(Eg_col_kvpair)\n",
    "                extension_of_f_dict.append(funct)\n",
    "            else:\n",
    "                funct=get_extension_of_f(f_dict,term,power_set_str,n)\n",
    "                funct.update(Eg_col_kvpair)\n",
    "                extension_of_f_dict.append(funct)\n",
    "\n",
    "        #We record packets of extensions where we take +1\n",
    "        P_n1.append(extension_of_f_dict)\n",
    "    \n",
    "    return P_n1\n",
    "\n",
    "def get_Pnp1_for_single_funct(Pn,n):\n",
    "    \n",
    "    #inputs\n",
    "    # Pn:dataframe and n.\n",
    "    \n",
    "    \"\"\"\n",
    "    Returns a dataframe of extensions in Pn+1 for dataframe of functions in Pn.\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    #Example\n",
    "    n=3\n",
    "    power_set_str,power_set_str_np1=get_n_np1_powersets(n)\n",
    "    # P3\n",
    "\n",
    "    funct_list=[{'1': 0,'2': 0,\"3\":0,\"12\":0,\"23\":0,\"13\":0,\"123\":0}]\n",
    "    Pn=pd.DataFrame(funct_list)\n",
    "\n",
    "    #The following should be functions in P4 that are extendions of functions in funct_list\n",
    "    f_P3=get_Pnp1_for_single_funct(Pn,n)\n",
    "    print(f_P3)\n",
    "    \"\"\"\n",
    "\n",
    "    #New data created\n",
    "    power_set_str,power_set_str_np1=get_n_np1_powersets(n)\n",
    "\n",
    "    #Load Pn as a list of dictionaries.\n",
    "    list_df_Pn=Pn.to_dict(orient='records')\n",
    "\n",
    "    #Create Pn+1 and hold as dataframe.\n",
    "    P_n1=eg_column_Build_Pn_1(list_df_Pn,power_set_str,n)\n",
    "    Pnp1=extend_sets_to_Pnp1(P_n1) #<---- ISSUE\n",
    "    \n",
    "    power_set_str_np1\n",
    "    \n",
    "    columns=list(power_set_str_np1)+[\"Eg_Size\"]\n",
    "    Pnp1 = Pnp1.reindex(columns=columns)\n",
    "\n",
    "\n",
    "    inspect_Pn1(Pnp1)\n",
    "\n",
    "    return Pnp1\n",
    "\n",
    "def Pnp1_from_Pn_part(df,n,calc_start,calc_end):\n",
    "    # df where we are extending from\n",
    "    # calc_start=0\n",
    "    # calc_end=5\n",
    "\n",
    "    rows=range(calc_start,calc_end) #What functions we are extending\n",
    "    Pn=df.iloc[rows] #takes database of rows 0,1..,calc_up_to\n",
    "\n",
    "    # print(\"The functions we are extending\")\n",
    "    # print(Pn,\"\\n\")\n",
    "\n",
    "    part_Pn=get_Pnp1_for_single_funct(Pn,n) #Extensions of Pn\n",
    "    # print(part_Pn.shape)\n",
    "    \n",
    "    strt=f\"P{n+1}_parts\\{calc_start}_{calc_end}_Part_fromP{n}.xlsx\"\n",
    "    part_Pn.to_excel(strt) #Stores in excel\n",
    "    return"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56b7c195-b947-4bd4-a0ed-3e754f5e6a32",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Example of functions used and process for P3:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e59ed4d4-bb26-4cae-aac9-bf7a3812e775",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Single function extension example"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8574aff-1105-4598-89cd-5e3963b3ac94",
   "metadata": {},
   "source": [
    "Here we see get_Pnp1_for_single_funct in action for P3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f4180022-ed73-47df-a563-6e3fd0f3f13a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The functions we are extending\n",
      "   1  2  3  12  23  13  123\n",
      "0  0  0  0   0   0   0    0\n",
      "Number of functions: 19\n",
      "Are there duplicates? (0, 16) if (0,-) then no common rows\n",
      "    1  2  3  4  12  13  14  23  24  34  123  124  134  234  1234  Eg_Size\n",
      "0   0  0  0  0   0   0   1   0   0   1    0    1    1    1     1       19\n",
      "1   0  0  0  0   0   0   1   0   0   0    0    1    1    1     1       19\n",
      "2   0  0  0  0   0   0   0   0   0   0    0    0    1    0     1       19\n",
      "3   0  0  0  0   0   0   1   0   1   0    0    1    1    1     1       19\n",
      "4   0  0  0  0   0   0   0   0   0   0    0    1    1    1     1       19\n",
      "5   0  0  0  0   0   0   1   0   0   0    0    1    1    0     1       19\n",
      "6   0  0  0  0   0   0   0   0   1   0    0    1    0    1     1       19\n",
      "7   0  0  0  0   0   0   0   0   1   1    0    1    1    1     1       19\n",
      "8   0  0  0  0   0   0   1   0   1   1    0    1    1    1     1       19\n",
      "9   0  0  0  0   0   0   0   0   0   0    0    1    0    1     1       19\n",
      "10  0  0  0  0   0   0   0   0   0   1    0    1    1    1     1       19\n",
      "11  0  0  0  0   0   0   0   0   0   0    0    0    1    1     1       19\n",
      "12  0  0  0  0   0   0   0   0   0   0    0    0    0    1     1       19\n",
      "13  0  0  0  0   0   0   0   0   0   0    0    0    0    0     1       19\n",
      "14  0  0  0  0   0   0   0   0   0   1    0    0    1    1     1       19\n",
      "15  0  0  0  0   0   0   0   0   0   0    0    1    1    0     1       19\n",
      "16  0  0  0  0   0   0   0   0   0   0    0    1    0    0     1       19\n",
      "17  0  0  0  0   0   0   0   0   1   0    0    1    1    1     1       19\n",
      "18  0  0  0  0   0   0   0   0   0   0    0    0    0    0     0       19\n"
     ]
    }
   ],
   "source": [
    "#Testing functions example\n",
    "n=3\n",
    "power_set_str,power_set_str_np1=get_n_np1_powersets(n)\n",
    "# P3\n",
    "\n",
    "# You can take subsets of rows as a database, then extend. Better than below\n",
    "funct_list=[{'1': 0,'2': 0,\"3\":0,\"12\":0,\"23\":0,\"13\":0,\"123\":0}]\n",
    "Pn=pd.DataFrame(funct_list)\n",
    "print(\"The functions we are extending\")\n",
    "print(Pn)\n",
    "\n",
    "#The following should be functions in P4 that are extendions of functions in funct_list\n",
    "f_P3=get_Pnp1_for_single_funct(Pn,n)\n",
    "print(f_P3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "422773f9-4214-4122-8645-613b20e09768",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "#### Building P4 from P3 example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f445619-ff65-49de-8783-2cd4aaef6ddf",
   "metadata": {},
   "outputs": [],
   "source": [
    "n=3\n",
    "power_set_str,power_set_str_np1=get_n_np1_powersets(n)\n",
    "# P3\n",
    "df=P3 #Total dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46372f5d-8e2e-405e-914d-359cd6e5dec5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "calc_start=0\n",
    "calc_end=5\n",
    "\n",
    "rows=range(calc_start,calc_end) #What functions we are extending\n",
    "Pn=df.iloc[rows] #takes database of rows 0,1..,calc_up_to\n",
    "\n",
    "print(\"The functions we are extending\")\n",
    "print(Pn,\"\\n\")\n",
    "\n",
    "part_Pn=get_Pnp1_for_single_funct(Pn,n) #Extensions of Pn\n",
    "print(part_Pn)\n",
    "strt=f\"P3_parts_example\\{calc_start}_{calc_end}_PartP4.xlsx\"\n",
    "part_Pn.to_excel(strt) #Stores in excel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85c946b1-0817-4d01-bca7-f7c383bfb59d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "calc_start=5\n",
    "calc_end=10\n",
    "\n",
    "rows=range(calc_start,calc_end) #What functions we are extending\n",
    "Pn=df.iloc[rows] #takes database of rows 0,1..,calc_up_to\n",
    "\n",
    "print(\"The functions we are extending\")\n",
    "print(Pn,\"\\n\")\n",
    "\n",
    "part_Pn=get_Pnp1_for_single_funct(Pn,n) #Extensions of Pn\n",
    "print(part_Pn)\n",
    "strt=f\"P3_parts_example\\{calc_start}_{calc_end}_PartP4.xlsx\"\n",
    "part_Pn.to_excel(strt) #Stores in excel"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd25f5a9-15b1-4136-a3ab-70aa6750ff38",
   "metadata": {},
   "source": [
    " We now merge the parts together."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ccf1533-81a9-447d-a6ab-ac28b9043b02",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "P4_part1=pd.read_excel('P3_parts_example/0_5_PartP4.xlsx', index_col=0) \n",
    "P4_part2=pd.read_excel('P3_parts_example/5_10_PartP4.xlsx', index_col=0) \n",
    "\n",
    "merged_df = pd.concat([P4_part1, P4_part2], ignore_index=True)\n",
    "\n",
    "merged_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "048f9c19-25d8-4d57-a0f7-309b5947810f",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Building P6 from P5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f2b06f0-7f06-4555-bdf9-cc0185db0d0e",
   "metadata": {},
   "source": [
    "You can run the following cell to get the data for P6 (take about 4hrs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "5e69e520-3348-4cd9-8b15-adc3f8fc81ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Part builder cell for P5 to P6 #At this rate it will take 1.43hrs<time<6hrs to run this calculation\n",
    "\n",
    "n=5\n",
    "power_set_str,power_set_str_np1=get_n_np1_powersets(n)\n",
    "df=P5 #Total dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86a5e17d-54a2-4ef2-bf98-d881ed65e971",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "#### Test case"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "0acd366b-7cfb-4a76-af17-294f1d7d9c27",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of functions: 47417\n",
      "Are there duplicates? (0, 64) if (0,-) then no common rows\n",
      "Perf timer 107.10247100000004\n",
      "Process timer 73.859375\n"
     ]
    }
   ],
   "source": [
    "start_perf,start_process = time.perf_counter(),time.process_time()\n",
    "Pnp1_from_Pn_part(df,n,0,100)\n",
    "end_perf,end_process = time.perf_counter(),time.process_time()\n",
    "\n",
    "print(f\"Perf timer {end_perf-start_perf}\") #This method returns the time in seconds.\n",
    "print(f\"Process timer {end_process-start_process}\") #measures the time the process takes, including time that the process is blocked"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8884b85e-0d12-45e4-ac4a-e616e46f900c",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "#### Running to construct P6"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47b3ca8d-e235-4c62-b663-1a0f8d59686d",
   "metadata": {},
   "source": [
    "Constructs a series of 101 xlxs files to store dataframe of parts of P6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "9fed5a9b-a41d-4272-98c0-1ec7180b9b32",
   "metadata": {},
   "outputs": [],
   "source": [
    "def runner(start,end): \n",
    "    start_perf,start_process = time.perf_counter(),time.process_time()\n",
    "    Pnp1_from_Pn_part(df,n,start,end)\n",
    "    end_perf,end_process = time.perf_counter(),time.process_time()\n",
    "\n",
    "    print(f\"Perf timer {end_perf-start_perf}\") #This method returns the time in seconds.\n",
    "    print(f\"Process timer {end_process-start_process}\") #measures the time the process takes, including time that the process is blocked\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad84ca6d-1e2b-47da-9c7f-7f92a452077a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "for i in range(0, 10300, 100):\n",
    "    start=i\n",
    "    end=start+100\n",
    "    runner(start,end)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "7791920b-6a0e-459e-9247-e51d24747d33",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of functions: 13087\n",
      "Are there duplicates? (0, 64) if (0,-) then no common rows\n"
     ]
    }
   ],
   "source": [
    "calc_start=10300\n",
    "calc_end=10334\n",
    "\n",
    "rows=range(calc_start,calc_end) #What functions we are extending\n",
    "Pn=df.iloc[rows] #takes database of rows 0,1..,calc_up_to\n",
    "\n",
    "part_Pn=get_Pnp1_for_single_funct(Pn,n) #Extensions of Pn\n",
    "strt=f\"P6_parts\\{calc_start}_{calc_end}_Part_fromP5.xlsx\"\n",
    "part_Pn.to_excel(strt) #Stores in excel"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac3ff57a-f012-4976-8d36-7ac6462bc07d",
   "metadata": {},
   "source": [
    "### Analysis P6"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccd1800c-72e0-4550-ae18-51dff2697142",
   "metadata": {},
   "source": [
    "Here we will finally get $|\\tilde{P}_{6}|$. By loading all files in P6_parts and taking the shape.\n",
    "As part of this calculation we can also decompose this number into the sum,\n",
    "\n",
    "$$|P_6|=\\sum_{g \\in P_5} |\\rho^{-1}(g)|\\cdot 1_{g}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0920dcbe-252e-4e06-8245-1df0731f1afb",
   "metadata": {},
   "source": [
    "Note $|Eg|=|\\rho^{-1}(g)|$ by construction."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfaff27b-e70d-476c-867a-f40c357d3058",
   "metadata": {},
   "source": [
    "The following loads the data for the parts of $P6$ (takes about 1.5hrs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "84fcbd31-ff13-4285-b070-fd0cca060f38",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "P6_parts/0_100_Part_fromP5.xlsx\n",
      "P6_parts/100_200_Part_fromP5.xlsx\n",
      "P6_parts/200_300_Part_fromP5.xlsx\n",
      "P6_parts/300_400_Part_fromP5.xlsx\n",
      "P6_parts/400_500_Part_fromP5.xlsx\n",
      "P6_parts/500_600_Part_fromP5.xlsx\n",
      "P6_parts/600_700_Part_fromP5.xlsx\n",
      "P6_parts/700_800_Part_fromP5.xlsx\n",
      "P6_parts/800_900_Part_fromP5.xlsx\n",
      "P6_parts/900_1000_Part_fromP5.xlsx\n",
      "P6_parts/1000_1100_Part_fromP5.xlsx\n",
      "P6_parts/1100_1200_Part_fromP5.xlsx\n",
      "P6_parts/1200_1300_Part_fromP5.xlsx\n",
      "P6_parts/1300_1400_Part_fromP5.xlsx\n",
      "P6_parts/1400_1500_Part_fromP5.xlsx\n",
      "P6_parts/1500_1600_Part_fromP5.xlsx\n",
      "P6_parts/1600_1700_Part_fromP5.xlsx\n",
      "P6_parts/1700_1800_Part_fromP5.xlsx\n",
      "P6_parts/1800_1900_Part_fromP5.xlsx\n",
      "P6_parts/1900_2000_Part_fromP5.xlsx\n",
      "P6_parts/2000_2100_Part_fromP5.xlsx\n",
      "P6_parts/2100_2200_Part_fromP5.xlsx\n",
      "P6_parts/2200_2300_Part_fromP5.xlsx\n",
      "P6_parts/2300_2400_Part_fromP5.xlsx\n",
      "P6_parts/2400_2500_Part_fromP5.xlsx\n",
      "P6_parts/2500_2600_Part_fromP5.xlsx\n",
      "P6_parts/2600_2700_Part_fromP5.xlsx\n",
      "P6_parts/2700_2800_Part_fromP5.xlsx\n",
      "P6_parts/2800_2900_Part_fromP5.xlsx\n",
      "P6_parts/2900_3000_Part_fromP5.xlsx\n",
      "P6_parts/3000_3100_Part_fromP5.xlsx\n",
      "P6_parts/3100_3200_Part_fromP5.xlsx\n",
      "P6_parts/3200_3300_Part_fromP5.xlsx\n",
      "P6_parts/3300_3400_Part_fromP5.xlsx\n",
      "P6_parts/3400_3500_Part_fromP5.xlsx\n",
      "P6_parts/3500_3600_Part_fromP5.xlsx\n",
      "P6_parts/3600_3700_Part_fromP5.xlsx\n",
      "P6_parts/3700_3800_Part_fromP5.xlsx\n",
      "P6_parts/3800_3900_Part_fromP5.xlsx\n",
      "P6_parts/3900_4000_Part_fromP5.xlsx\n",
      "P6_parts/4000_4100_Part_fromP5.xlsx\n",
      "P6_parts/4100_4200_Part_fromP5.xlsx\n",
      "P6_parts/4200_4300_Part_fromP5.xlsx\n",
      "P6_parts/4300_4400_Part_fromP5.xlsx\n",
      "P6_parts/4400_4500_Part_fromP5.xlsx\n",
      "P6_parts/4500_4600_Part_fromP5.xlsx\n",
      "P6_parts/4600_4700_Part_fromP5.xlsx\n",
      "P6_parts/4700_4800_Part_fromP5.xlsx\n",
      "P6_parts/4800_4900_Part_fromP5.xlsx\n",
      "P6_parts/4900_5000_Part_fromP5.xlsx\n",
      "P6_parts/5000_5100_Part_fromP5.xlsx\n",
      "P6_parts/5100_5200_Part_fromP5.xlsx\n",
      "P6_parts/5200_5300_Part_fromP5.xlsx\n",
      "P6_parts/5300_5400_Part_fromP5.xlsx\n",
      "P6_parts/5400_5500_Part_fromP5.xlsx\n",
      "P6_parts/5500_5600_Part_fromP5.xlsx\n",
      "P6_parts/5600_5700_Part_fromP5.xlsx\n",
      "P6_parts/5700_5800_Part_fromP5.xlsx\n",
      "P6_parts/5800_5900_Part_fromP5.xlsx\n",
      "P6_parts/5900_6000_Part_fromP5.xlsx\n",
      "P6_parts/6000_6100_Part_fromP5.xlsx\n",
      "P6_parts/6100_6200_Part_fromP5.xlsx\n",
      "P6_parts/6200_6300_Part_fromP5.xlsx\n",
      "P6_parts/6300_6400_Part_fromP5.xlsx\n",
      "P6_parts/6400_6500_Part_fromP5.xlsx\n",
      "P6_parts/6500_6600_Part_fromP5.xlsx\n",
      "P6_parts/6600_6700_Part_fromP5.xlsx\n",
      "P6_parts/6700_6800_Part_fromP5.xlsx\n",
      "P6_parts/6800_6900_Part_fromP5.xlsx\n",
      "P6_parts/6900_7000_Part_fromP5.xlsx\n",
      "P6_parts/7000_7100_Part_fromP5.xlsx\n",
      "P6_parts/7100_7200_Part_fromP5.xlsx\n",
      "P6_parts/7200_7300_Part_fromP5.xlsx\n",
      "P6_parts/7300_7400_Part_fromP5.xlsx\n",
      "P6_parts/7400_7500_Part_fromP5.xlsx\n",
      "P6_parts/7500_7600_Part_fromP5.xlsx\n",
      "P6_parts/7600_7700_Part_fromP5.xlsx\n",
      "P6_parts/7700_7800_Part_fromP5.xlsx\n",
      "P6_parts/7800_7900_Part_fromP5.xlsx\n",
      "P6_parts/7900_8000_Part_fromP5.xlsx\n",
      "P6_parts/8000_8100_Part_fromP5.xlsx\n",
      "P6_parts/8100_8200_Part_fromP5.xlsx\n",
      "P6_parts/8200_8300_Part_fromP5.xlsx\n",
      "P6_parts/8300_8400_Part_fromP5.xlsx\n",
      "P6_parts/8400_8500_Part_fromP5.xlsx\n",
      "P6_parts/8500_8600_Part_fromP5.xlsx\n",
      "P6_parts/8600_8700_Part_fromP5.xlsx\n",
      "P6_parts/8700_8800_Part_fromP5.xlsx\n",
      "P6_parts/8800_8900_Part_fromP5.xlsx\n",
      "P6_parts/8900_9000_Part_fromP5.xlsx\n",
      "P6_parts/9000_9100_Part_fromP5.xlsx\n",
      "P6_parts/9100_9200_Part_fromP5.xlsx\n",
      "P6_parts/9200_9300_Part_fromP5.xlsx\n",
      "P6_parts/9300_9400_Part_fromP5.xlsx\n",
      "P6_parts/9400_9500_Part_fromP5.xlsx\n",
      "P6_parts/9500_9600_Part_fromP5.xlsx\n",
      "P6_parts/9600_9700_Part_fromP5.xlsx\n",
      "P6_parts/9700_9800_Part_fromP5.xlsx\n",
      "P6_parts/9800_9900_Part_fromP5.xlsx\n",
      "P6_parts/9900_10000_Part_fromP5.xlsx\n",
      "P6_parts/10000_10100_Part_fromP5.xlsx\n",
      "P6_parts/10100_10200_Part_fromP5.xlsx\n",
      "P6_parts/10200_10300_Part_fromP5.xlsx\n",
      "P6_parts/10300_10334_Part_fromP5.xlsx\n",
      "There are 5399325 functions in P6 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Store number of rows for each data packet of P6\n",
    "P6_size_list=[]\n",
    "Eg_numb_functs_init_counter=[]\n",
    "\n",
    "#Main Body of calculations\n",
    "for i in range(0, 10300, 100):\n",
    "    #Stores number for this packet in P6_size_list\n",
    "    start=i\n",
    "    end=start+100\n",
    "    P6_part=pd.read_excel(f'P6_parts/{start}_{end}_Part_fromP5.xlsx', index_col=0)\n",
    "    print(f'P6_parts/{start}_{end}_Part_fromP5.xlsx')\n",
    "    row_numb=P6_part.shape[0]\n",
    "    P6_size_list.append(row_numb)\n",
    "\n",
    "    # In this packet gets Eg_Size and get counter for eg sizes and number of functions in Eg_numb_functs_init_counter\n",
    "    values = P6_part['Eg_Size'].value_counts(ascending=True).keys().tolist()\n",
    "    counts = P6_part['Eg_Size'].value_counts(ascending=True).tolist()\n",
    "    zipped=list(zip(values,counts))\n",
    "    Eg_numb_functs_init_counter.extend(zipped)\n",
    "    \n",
    "# It remains to do 10300-10334\n",
    "#Stores number for this packet in P6_size_list\n",
    "start,end=(10300,10334)\n",
    "P6_part=pd.read_excel(f'P6_parts/{start}_{end}_Part_fromP5.xlsx', index_col=0)\n",
    "print(f'P6_parts/{start}_{end}_Part_fromP5.xlsx')\n",
    "row_numb=P6_part.shape[0]\n",
    "P6_size_list.append(row_numb)\n",
    "\n",
    "# In this packet gets Eg_Size and get counter for eg sizes and number of functions in Eg_numb_functs_init_counter\n",
    "values = P6_part['Eg_Size'].value_counts(ascending=True).keys().tolist()\n",
    "counts = P6_part['Eg_Size'].value_counts(ascending=True).tolist()\n",
    "zipped=list(zip(values,counts))\n",
    "Eg_numb_functs_init_counter.extend(zipped)\n",
    "\n",
    "#output-------------------------------------------------------------------------\n",
    "\n",
    "#Final answer\n",
    "P6_size=sum(P6_size_list)\n",
    "print(f\"There are {P6_size} functions in P6 \\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b948efa-6cdc-4fa2-a7df-4988ca3e5a91",
   "metadata": {
    "tags": []
   },
   "source": [
    "We get Eg_numb_functs_init_counter as an output also this records the decomposition of the sum $|P6|$. Eg_numb_functs_init_counter consists of (x,y) where x=|Eg| and y= x \\cdot the number of times x appeared in that packet. Therefore it is neccsary to sum to get (x,Y) where Y isx \\cdot the number of times x appeared in all packets. To get the number of times x appears it is necessary to divide Y by x."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "538f4675-80f2-4c83-bd79-e8e74cd9597d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Which decomposes as follows:\n",
      " 5399325=114*125+121*65+126*237+138*112+158*248+162*247+165*132+169*117+173*132+178*122+182*106+183*132+187*24+192*134+194*123+208*144+209*22+216*234+217*126+218*61+221*125+227*22+229*307+230*250+232*120+233*119+236*122+241*30+242*102+247*61+248*53+250*114+251*59+260*184+268*106+269*64+271*186+273*118+274*56+278*52+279*41+281*136+284*112+289*67+293*133+294*37+300*133+305*67+313*123+324*123+325*132+326*101+329*181+338*120+343*59+345*113+356*19+381*121+390*44+397*107+419*20+420*46+421*23+422*39+429*64+433*35+436*128+437*23+455*64+477*53+496*48+502*16+509*38+518*12+520*107+524*20+531*36+538*76+539*62+553*36+555*66+566*144+578*61+605*62+607*66+613*11+640*28+659*30+759*48+765*21+781*108+802*119+814*42+881*42+938*8+974*135+977*45+983*135+1002*45+1026*126+1029*63+1033*63+1068*96+1121*108+1145*22+1178*39+1189*39+1209*44+1212*11+1224*66+1257*44+1312*11+1322*32+1377*36+1409*16+1421*10+1450*54+1511*20+1666*21+1753*24+2110*42+2193*95+2285*4+2290*54+3544*12+3673*42+3820*48+3983*18+6280*1+6474*6+6702*14+6960*16+7250*9+7580*4\n"
     ]
    }
   ],
   "source": [
    "#He now obtain Eg_numb_functs from Eg_numb_functs_init_counter\n",
    "\n",
    "# This is a List of tuples where have  joined e.g (13, 6),(13, 19) to (13, 25) from Eg_numb_functs_init_counter\n",
    "\n",
    "from collections import defaultdict\n",
    "\n",
    "sums = defaultdict(int)\n",
    "\n",
    "for first, second in Eg_numb_functs_init_counter:\n",
    "    sums[first] += second\n",
    "\n",
    "Eg_numb_functs = [(first, second) for first, second in sums.items()]\n",
    "Eg_numb_functs = sorted(Eg_numb_functs, key=lambda x: x[0])#order so in increasing with respect to eg size\n",
    "\n",
    "#Build a string using a for loop, using the tuples in Eg_numb_functs\n",
    "f1,f2=Eg_numb_functs[0]\n",
    "\n",
    "f2_div=int(f2/f1)\n",
    "\n",
    "stringer=f'{f1}*{f2_div}'\n",
    "\n",
    "for item in Eg_numb_functs[1:]: #first item in stringer already\n",
    "    eg,numb=item #eg,numb not all the same\n",
    "    num_div=int(numb/eg)\n",
    "    s=f'+{eg}*{num_div}'\n",
    "    stringer += s\n",
    "    \n",
    "#Output\n",
    "print(f\"Which decomposes as follows:\\n {P6_size}={stringer}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "4d9539b3-a3bb-4ff7-982e-82777d3694ed",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5399325"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Check output calculates 5399325 #Yes\n",
    "s=114*125+121*65+126*237+138*112+158*248+162*247+165*132+169*117+173*132+178*122+182*106+183*132+187*24+192*134+194*123+208*144+209*22+216*234+217*126+218*61+221*125+227*22+229*307+230*250+232*120+233*119+236*122+241*30+242*102+247*61+248*53+250*114+251*59+260*184+268*106+269*64+271*186+273*118+274*56+278*52+279*41+281*136+284*112+289*67+293*133+294*37+300*133+305*67+313*123+324*123+325*132+326*101+329*181+338*120+343*59+345*113+356*19+381*121+390*44+397*107+419*20+420*46+421*23+422*39+429*64+433*35+436*128+437*23+455*64+477*53+496*48+502*16+509*38+518*12+520*107+524*20+531*36+538*76+539*62+553*36+555*66+566*144+578*61+605*62+607*66+613*11+640*28+659*30+759*48+765*21+781*108+802*119+814*42+881*42+938*8+974*135+977*45+983*135+1002*45+1026*126+1029*63+1033*63+1068*96+1121*108+1145*22+1178*39+1189*39+1209*44+1212*11+1224*66+1257*44+1312*11+1322*32+1377*36+1409*16+1421*10+1450*54+1511*20+1666*21+1753*24+2110*42+2193*95+2285*4+2290*54+3544*12+3673*42+3820*48+3983*18+6280*1+6474*6+6702*14+6960*16+7250*9+7580*4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a569ffb3-9104-4903-8fa5-8282e768eac0",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2f68e18-305b-4280-8b30-0decda313fa9",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Functions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "e8b32f80-cb9a-4f25-baa1-ab28f589a190",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_Eg_for_single_g(df,numb,power_set_str):\n",
    "    # print(power_set_str)\n",
    "    #Obj: puts Eg into correct format to check if topolgoy\n",
    "    #Inputs: df=Pn, numb used to get a specific function from df\n",
    "    \n",
    "    # Pick function in Pn\n",
    "    g=df.iloc[numb].to_dict()\n",
    "\n",
    "    #get primitive closed sets\n",
    "    PC=[(x,get_barj({x},g)) for x in power_set_str]\n",
    "    dict_PC=dict(PC)\n",
    "    set_PC=[sorted_frozenset(get_barj({x},g)) for x in power_set_str]\n",
    "\n",
    "    #Get all closed sets for extensions for g\n",
    "    Eg=list(get_Eg(dict_PC,power_set_str,set_PC))\n",
    "    Eg.append(frozenset()) #rembmer the empty set\n",
    "    return Eg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "id": "fd166050-336f-44ef-b757-a6ff5934fbea",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_Eg_for_single_g_dfisfunct(funct,power_set_str):\n",
    "    # print(power_set_str)\n",
    "    #Obj: puts Eg into correct format to check if topolgoy\n",
    "    #Inputs: df=Pn, numb used to get a specific function from df\n",
    "    \n",
    "    # Pick function in Pn\n",
    "    g=funct\n",
    "    \n",
    "    print(power_set_str)\n",
    "\n",
    "    #get primitive closed sets\n",
    "    PC=[(x,get_barj({x},g)) for x in power_set_str]\n",
    "    dict_PC=dict(PC)\n",
    "    set_PC=[sorted_frozenset(get_barj({x},g)) for x in power_set_str]\n",
    "    # print(\"len set_PC\",len(set_PC))\n",
    "    \n",
    "    #Get all closed sets for extensions for g\n",
    "    Eg=list(get_Eg(dict_PC,power_set_str,set_PC))\n",
    "    Eg.append(frozenset()) #rembmer the empty set\n",
    "    return Eg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "3b8c2e15-7955-4b28-8cd0-e31ce2cd1c3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_primitive_for_single_g(df,numb):\n",
    "    #Inputs: df=Pn, numb used to get a specific function from df\n",
    "    \n",
    "    # Pick function in Pn\n",
    "    g=df.iloc[numb].to_dict()\n",
    "\n",
    "    #get primitive closed sets\n",
    "    PC=[(x,get_barj({x},g)) for x in power_set_str]\n",
    "    dict_PC=dict(PC)\n",
    "    set_PC=[sorted_frozenset(get_barj({x},g)) for x in power_set_str]\n",
    "\n",
    "\n",
    "    return dict_PC,set_PC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "e0367b60-7420-41b8-96fb-cd8ad309857a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_finite_topological_space(X, T):\n",
    "    \n",
    "    \"\"\"\n",
    "        # Does this account for unions? \n",
    "\n",
    "    # X = {'a', 'b', 'c'}\n",
    "    # T = [X, set(), {'a'}, { 'c'}, {'b'}]\n",
    "    # print(is_finite_topological_space(X, T)) # prints False\n",
    "\n",
    "    # This condition is not met because the union of {'a'} and {'c'} is {'a','c'} which is not in T.\n",
    "    \"\"\"\n",
    "    \n",
    "    if not all(isinstance(x, set) for x in T):\n",
    "        # T must be a collection of sets\n",
    "        print(x)\n",
    "        \n",
    "        return False\n",
    "    if not all(x.issubset(X) for x in T):\n",
    "        # Each element of T must be a subset of X\n",
    "        print(\"here1\")\n",
    "\n",
    "        return False\n",
    "    if not all(x.intersection(y) in T for x in T for y in T):\n",
    "        print(\"here2\")\n",
    "\n",
    "        # The intersection of any two elements of T must be in T\n",
    "        return False\n",
    "    if not all(x.union(y) in T for x in T for y in T):\n",
    "        # The union of any two elements of T must be in T\n",
    "        print(\"here3\")\n",
    "        return False\n",
    "    \n",
    "    if not X in T:\n",
    "        print(\"here4\")\n",
    "        print(x)\n",
    "\n",
    "        # X must be in T\n",
    "        return False\n",
    "    if not set() in T:\n",
    "        print(\"here5\")\n",
    "\n",
    "        # The empty set must be in T\n",
    "        return False\n",
    "    return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "d5720241-d67c-4708-a105-9d96d393b3a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_top_for_single_g(df,numb,power_set_str):\n",
    "    #Inputs:\n",
    "    # df=Pn\n",
    "    # numb=choice of function\n",
    "    # power_set_str for df\n",
    "    \n",
    "    # Output:True or False\n",
    "    \n",
    "    Eg=get_Eg_for_single_g(df,numb,power_set_str)\n",
    "    Top=list(set(x) for x in Eg)\n",
    "    Top.append(set())\n",
    "    \n",
    "    X_Eg=set(power_set_str)\n",
    "    \n",
    "    return is_finite_topological_space(X_Eg, Top)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "9365a459-5555-4a23-ab22-31e9e99c001e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_all_funct_topspace(df,power_set_str):\n",
    "    length=[]\n",
    "    for i,g in enumerate(df.to_dict(\"records\")): #list of dictionaries of functions\n",
    "        check_top=check_top_for_single_g(df,i,power_set_str)\n",
    "        if check_top == False:\n",
    "            print(f\"Function {i} does NOT give a topolgical space: {check_top}\")\n",
    "        else:\n",
    "            length.append(check_top)\n",
    "            \n",
    "    if len(df.to_dict(\"records\"))==len(length):\n",
    "        print(\"Every function produces a topological space\" )\n",
    "    else:\n",
    "        print(\"something is not a topological space\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "id": "3056b840-f5f1-4019-ae1c-d4d12a8000a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_Eg_column_Pn(df):\n",
    "    df=df.copy()\n",
    "    \n",
    "    #Get Eg for the row in df.\n",
    "\n",
    "    # Create a new column called \"new_column\"\n",
    "    #--------\n",
    "    power_set_str_n=get_powerset_str(n+1)\n",
    "    # print(\"Power\",power_set_str_n)\n",
    "    print(\"hii\",df)\n",
    "\n",
    "    # Insert data into the new column\n",
    "    for i, row in df.iterrows():   \n",
    "        funct=dict(row)\n",
    "        funct.pop('Eg_Size', None)\n",
    "        # print(funct)\n",
    "        Eg=get_Eg_for_single_g_dfisfunct(funct,power_set_str_n)\n",
    "        # print(Eg)\n",
    "        df.at[i, 'Eg_Size'] = int(len(Eg))\n",
    "        print(len(Eg))\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "2bfa801d-8f12-4685-b40e-05ccd53416d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def eg_column_Build_Pn_1(list_df_Pn,power_set_str,n):\n",
    "    \n",
    "    \"\"\"\n",
    "    Obj:Use Build_Pn_1 and modify change to include Eg size column in functions\n",
    "    \"\"\"\n",
    "    \n",
    "    # Returns:packets of extensions for each f_dict from Pn, can check those with max size of closed sets (later)\n",
    "    \n",
    "    P_n1=[]\n",
    "    for index,f_dict in enumerate(list_df_Pn):\n",
    "\n",
    "        # Store primitive closed sets \n",
    "        PC=[(x,get_barj({x},f_dict)) for x in power_set_str]\n",
    "        dict_PC=dict(PC)\n",
    "        set_PC=[sorted_frozenset(get_barj({x},f_dict)) for x in power_set_str]\n",
    "\n",
    "        #Get all closed sets for extensions for f_dict\n",
    "        Eg=list(get_Eg(dict_PC,power_set_str,set_PC))\n",
    "        Eg.append(\"empty\")\n",
    "        \n",
    "        # #Append eg_size for f_dict\n",
    "        # power_set_str_n=get_powerset_str(n+1)[0]\n",
    "        # funct_list=[f_dict] #eg is 5 \n",
    "        # print(funct_list)\n",
    "        # f_dict_pd=pd.DataFrame(funct_list)\n",
    "        # Eg_f_dict=get_Eg_for_single_g(f_dict_pd,0,power_set_str_n)\n",
    "        Eg_col_kvpair={\"Eg_Size\":len(Eg)}\n",
    "        # Eg_col_kvpair={\"Eg_Size\":len(Eg_f_dict)}\n",
    "\n",
    "#         print(len(Eg_f_dict))\n",
    "        \n",
    "        #Build extensions for f_dict\n",
    "        extension_of_f_dict=[]\n",
    "        for term in Eg:\n",
    "            if term ==\"empty\":                    \n",
    "                funct=empty_case_get_extension_of_f(f_dict,term,power_set_str,n) #dictionary\n",
    "                funct.update(Eg_col_kvpair)\n",
    "                extension_of_f_dict.append(funct)\n",
    "            else:\n",
    "                funct=get_extension_of_f(f_dict,term,power_set_str,n)\n",
    "                funct.update(Eg_col_kvpair)\n",
    "                extension_of_f_dict.append(funct)\n",
    "\n",
    "        #We record packets of extensions where we take +1\n",
    "        P_n1.append(extension_of_f_dict)\n",
    "    \n",
    "    return P_n1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "4fead0ff-37e7-4eba-940e-8d42397a70a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_Pnp1_for_single_funct(Pn,n):\n",
    "    \n",
    "    #inputs\n",
    "    # Pn:dataframe and n.\n",
    "    \n",
    "    \"\"\"\n",
    "    Returns a dataframe of extensions in Pn+1 for dataframe of functions in Pn.\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    #Example\n",
    "    n=3\n",
    "    power_set_str,power_set_str_np1=get_n_np1_powersets(n)\n",
    "    # P3\n",
    "\n",
    "    funct_list=[{'1': 0,'2': 0,\"3\":0,\"12\":0,\"23\":0,\"13\":0,\"123\":0}]\n",
    "    Pn=pd.DataFrame(funct_list)\n",
    "\n",
    "    #The following should be functions in P4 that are extendions of functions in funct_list\n",
    "    f_P3=get_Pnp1_for_single_funct(Pn,n)\n",
    "    print(f_P3)\n",
    "    \"\"\"\n",
    "\n",
    "    #New data created\n",
    "    power_set_str,power_set_str_np1=get_n_np1_powersets(n)\n",
    "\n",
    "    #Load Pn as a list of dictionaries.\n",
    "    list_df_Pn=Pn.to_dict(orient='records')\n",
    "\n",
    "    #Create Pn+1 and hold as dataframe.\n",
    "    P_n1=eg_column_Build_Pn_1(list_df_Pn,power_set_str,n)\n",
    "    #As previous was done in E(g) parts, joint together\n",
    "    Pnp1=extend_sets_to_Pnp1(P_n1)\n",
    "        \n",
    "    Pnp1=get_Eg_column_Pn(Pnp1)\n",
    "    \n",
    "    columns=list(power_set_str_np1)+[\"Eg_Size\"]\n",
    "    Pnp1 = Pnp1.reindex(columns=columns)\n",
    "    \n",
    "    # inspect_Pn1(Pnp1)\n",
    "\n",
    "    return Pnp1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "a1092b56-c5f2-4004-9c26-7788b4800f95",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extend_sets_to_Pnp1(P_n1):\n",
    "    #Return the data frame of Pn+1\n",
    "    \n",
    "    final_P_n1=[]\n",
    "    for ls in P_n1:\n",
    "        final_P_n1=final_P_n1+ls # Adding all the extension packets together.\n",
    "    df=pd.DataFrame(final_P_n1)\n",
    "    # df = df.reindex(columns=power_set_str_np1)\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fd7d3eb-1ea5-4e94-b22f-3bf98e315124",
   "metadata": {
    "tags": [],
    "toc-hr-collapsed": true
   },
   "source": [
    "## For $g \\in P_3$ get $\\mathbb{E}(g)$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d28d335-f87a-4244-9505-289e405b9490",
   "metadata": {},
   "source": [
    "The set $\\mathbb{E}(g)$ is the topology assoicated to the function $g$ and can be used to construct the extensions of $g$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58e72dd6-ed09-40a2-a040-ae2531abe6d2",
   "metadata": {},
   "source": [
    "We now have:\n",
    "- A function/method to get $\\mathbb{E}(g)$.\n",
    "- A function to check if $\\mathbb{E}(g)$ is a finite topology.\n",
    "- Check if all function in P3,P4,P5 generate a topology: Answer P3,P4 yes, examples of P5 yes\n",
    "\n",
    "What we could do next:\n",
    "- A way to determine the size of $\\mathbb{E}(g)$ (using basis).\n",
    "- Are all Top(g) for $g \\in P_n$ homeomorphic? (topologies are different sizes)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2aa69ff-94cc-4557-bd1a-ad84aa4e5c07",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Specific Example from P3 is topological space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "e0cbb07e-026b-4f98-bde2-34061b09c1c9",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'1': 0, '2': 0, '3': 0, '12': 0, '13': 0, '23': 0, '123': 0} \n",
      "\n",
      "{'12', '123', '23'}\n",
      "{'12', '1', '123', '13'}\n",
      "{'123', '3', '23', '13'}\n",
      "{'3', '2', '23', '12', '1', '123', '13'}\n",
      "{'123'}\n",
      "{'3', '23', '12', '123', '13'}\n",
      "{'2', '23', '12', '1', '123', '13'}\n",
      "{'12', '123', '23', '13'}\n",
      "{'12', '123', '13'}\n",
      "{'23', '12', '1', '123', '13'}\n",
      "{'12', '23', '123', '2'}\n",
      "{'12', '123'}\n",
      "{'123', '23'}\n",
      "{'3', '23', '2', '12', '123', '13'}\n",
      "{'123', '13'}\n",
      "{'23', '2', '12', '123', '13'}\n",
      "{'123', '23', '13'}\n",
      "{'3', '23', '12', '1', '123', '13'}\n",
      "set()\n",
      "\n",
      "  All closed sets\n"
     ]
    }
   ],
   "source": [
    "power_set_str=get_powerset_str(3)\n",
    "\n",
    "df=P3\n",
    "numb=4\n",
    "#Get data\n",
    "Eg=get_Eg_for_single_g(df,numb,power_set_str)\n",
    "# print(power_set_str)\n",
    "print(P3.iloc[numb].to_dict(),\"\\n\")\n",
    "\n",
    "for term in Eg:\n",
    "    print(set(term))\n",
    "    \n",
    "print(\"\\n  All closed sets\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92ba1299-aead-4f79-982c-bcf3c798df7d",
   "metadata": {},
   "source": [
    "Check if Eg is a finite topological space on the powerset for an example g in P3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28575e50-b390-4bc5-9a0a-759606995780",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_Eg= {'13', '23', '123', '12', '2', '3', '1'} #total space ie powerset\n",
    "Top=list(set(x) for x in Eg) #topology\n",
    "\n",
    "print(is_finite_topological_space(X_Eg, Top))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "266dc5da-57d0-42e5-95ae-2908bf7562f2",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Primitives for specific example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "8ddf753b-6f77-4228-bc66-64d9e0191a69",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'1': 0, '2': 0, '3': 0, '12': 0, '13': 0, '23': 0, '123': 0} \n",
      "\n",
      "{'12', '1', '123', '13'}\n",
      "{'12', '23', '123', '2'}\n",
      "{'123', '3', '23', '13'}\n",
      "{'12', '123'}\n",
      "{'123', '13'}\n",
      "{'123', '23'}\n",
      "{'123'}\n",
      "set()\n",
      "\n",
      " Primitive closed sets\n"
     ]
    }
   ],
   "source": [
    "dict_PC,set_PC=get_primitive_for_single_g(df,numb)\n",
    "\n",
    "print(P3.iloc[numb].to_dict(),\"\\n\")\n",
    "\n",
    "\n",
    "# print(dict_PC) #Remembers what the terms of powerset is\n",
    "set_PC.append(frozenset()) # add empty set: this is the set of primitive closed sets\n",
    "for i in set_PC:\n",
    "    print(set(i))\n",
    "    \n",
    "print(\"\\n Primitive closed sets\")\n",
    "    \n",
    "# X_Eg= {'13', '23', '123', '12', '2', '3', '1'} #total space ie powerset\n",
    "# Prim_Top= list(set(x) for x in set_PC) # primitive set as a topology? is it a basis?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "689dc7a6-f7e4-42bc-b74c-9ba350be68fa",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Consider Top(g) for two different g,g' in P3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bda02a9-abbd-46d2-8ab9-78580bafcace",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Setup data\n",
    "power_set_str=get_powerset_str(3)\n",
    "df=P3\n",
    "X_Eg= set(power_set_str) #total space ie powerset\n",
    "\n",
    "#Inputs\n",
    "numb1=0\n",
    "numb2=1\n",
    "\n",
    "#Top space 1\n",
    "Eg1=get_Eg_for_single_g(df,numb,power_set_str)\n",
    "Top1=list(set(x) for x in Eg1) #topology\n",
    "print(is_finite_topological_space(X_Eg, Top1))\n",
    "#------------------------------\n",
    "#Top space 2\n",
    "Eg2=get_Eg_for_single_g(df,numb2,power_set_str)\n",
    "Top2=list(set(x) for x in Eg2) #topology\n",
    "print(is_finite_topological_space(X_Eg, Top2))\n",
    "#------------------------------\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "619c0d5d-db86-4209-bd70-1d2cd5d3ad5f",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Get Pn with |Eg| size & Find g's with largest and smallest |Eg|"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c6e8399-9308-42a8-b921-876f29145111",
   "metadata": {
    "tags": []
   },
   "source": [
    "### P3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "id": "52d636f4-31dd-45fe-ab81-b6cea22aa8ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hii    1  2  3  12  13  23  123\n",
      "0  0  0  0   0   1   0    1\n",
      "1  0  0  0   0   0   0    1\n",
      "2  0  0  0   0   1   1    1\n",
      "3  0  0  0   0   0   1    1\n",
      "4  0  0  0   0   0   0    0\n",
      "5  0  0  0   1   0   1    1\n",
      "6  0  0  0   1   1   1    2\n",
      "7  0  0  0   1   1   0    1\n",
      "8  0  0  0   1   1   1    1\n",
      "9  0  0  0   1   0   0    1\n",
      "['1', '2', '3', '12', '13', '23', '123']\n",
      "13\n",
      "['1', '2', '3', '12', '13', '23', '123']\n",
      "19\n",
      "['1', '2', '3', '12', '13', '23', '123']\n",
      "13\n",
      "['1', '2', '3', '12', '13', '23', '123']\n",
      "13\n",
      "['1', '2', '3', '12', '13', '23', '123']\n",
      "19\n",
      "['1', '2', '3', '12', '13', '23', '123']\n",
      "13\n",
      "['1', '2', '3', '12', '13', '23', '123']\n",
      "19\n",
      "['1', '2', '3', '12', '13', '23', '123']\n",
      "13\n",
      "['1', '2', '3', '12', '13', '23', '123']\n",
      "19\n",
      "['1', '2', '3', '12', '13', '23', '123']\n",
      "13\n",
      "   1  2  3  12  13  23  123  Eg_Size\n",
      "0  0  0  0   0   1   0    1     13.0\n",
      "2  0  0  0   0   1   1    1     13.0\n",
      "3  0  0  0   0   0   1    1     13.0\n",
      "5  0  0  0   1   0   1    1     13.0\n",
      "7  0  0  0   1   1   0    1     13.0\n",
      "9  0  0  0   1   0   0    1     13.0\n",
      "1  0  0  0   0   0   0    1     19.0\n",
      "4  0  0  0   0   0   0    0     19.0\n",
      "6  0  0  0   1   1   1    2     19.0\n",
      "8  0  0  0   1   1   1    1     19.0 \n",
      "\n",
      "13.0    6\n",
      "19.0    4\n",
      "Name: Eg_Size, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "power_set_str=get_powerset_str(3)\n",
    "\n",
    "P3_eg=get_Eg_column_Pn(P3)\n",
    "P3_eg=P3_eg.sort_values('Eg_Size') # get those 13,then 19\n",
    "print(P3_eg,\"\\n\")\n",
    "# print(sum(P3_eg[\"Eg_Size\"]))# check 154 total\n",
    "\n",
    "df=P3_eg\n",
    "print(df['Eg_Size'].value_counts()) #count number of 13s and 19s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6876349f-73fa-4ad4-aa97-ee6e4993205d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get Eg for the function in P3\n",
    "#3  0  0  0   0   0   0    1      19\n",
    "numb =3\n",
    "df=P3\n",
    "# g=df.iloc[numb].to_dict()\n",
    "print(g,\"\\n\")\n",
    "\n",
    "EEg_example=get_Eg_for_single_g(df,numb,power_set_str)\n",
    "# for i in EEg_example:\n",
    "#     print(set(i))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "500b3eb8-c6b2-4988-81aa-71d9c3f4de90",
   "metadata": {
    "tags": []
   },
   "source": [
    "### P4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "13308fd2-fa7b-4cac-a4a9-5ad8e195bb1f",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     1  2  3  4  12  13  14  23  24  34  123  124  134  234  1234 Eg_Size\n",
      "0    0  0  0  0   0   1   0   0   0   1    1    0    1    1     1      42\n",
      "141  0  0  0  0   1   0   1   0   1   1    1    2    1    1     2      42\n",
      "143  0  0  0  0   1   0   1   0   0   0    1    1    1    0     1      42\n",
      "46   0  0  0  0   0   0   0   1   1   0    1    1    0    1     1      42\n",
      "119  0  0  0  0   1   1   1   0   0   1    1    1    2    1     2      42\n",
      "56   0  0  0  0   0   0   1   1   1   1    1    1    1    2     2      42\n",
      "144  0  0  0  0   1   0   0   0   1   0    1    1    0    1     1      42\n",
      "35   0  0  0  0   0   1   1   1   0   1    1    1    2    1     2      42\n",
      "45   0  0  0  0   0   0   0   1   0   1    1    0    1    1     1      42\n",
      "44   0  0  0  0   0   1   0   1   0   0    1    0    1    1     1      42\n",
      "67   0  0  0  0   0   0   1   0   0   1    0    1    1    1     1      42\n",
      "120  0  0  0  0   1   1   1   0   1   0    1    2    1    1     2      42\n",
      "68   0  0  0  0   0   0   0   0   1   1    0    1    1    1     1      42\n",
      "102  0  0  0  0   1   1   1   1   0   0    2    1    1    1     2      42\n",
      "101  0  0  0  0   1   1   0   1   1   0    2    1    1    1     2      42\n",
      "75   0  0  0  0   0   0   1   0   1   0    0    1    1    1     1      42\n",
      "97   0  0  0  0   1   1   0   1   0   1    2    1    1    1     2      42\n",
      "78   0  0  0  0   1   0   0   1   1   1    1    1    1    2     2      42\n",
      "89   0  0  0  0   1   0   0   1   0   0    1    1    0    1     1      42\n",
      "39   0  0  0  0   0   1   0   1   1   1    1    1    1    2     2      42\n",
      "83   0  0  0  0   1   0   1   1   1   0    1    2    1    1     2      42\n",
      "1    0  0  0  0   0   1   1   0   0   0    1    1    1    0     1      42\n",
      "11   0  0  0  0   0   1   1   0   1   1    1    1    2    1     2      42\n",
      "121  0  0  0  0   1   1   0   0   0   0    1    1    1    0     1      42\n",
      "42     24\n",
      "45     24\n",
      "47     24\n",
      "82     16\n",
      "46     12\n",
      "54     12\n",
      "111    12\n",
      "99     10\n",
      "69      8\n",
      "133     8\n",
      "167     4\n",
      "Name: Eg_Size, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "power_set_str=get_powerset_str(4)\n",
    "\n",
    "P4_eg=get_Eg_column_Pn(P4)\n",
    "P4_eg=P4_eg.sort_values('Eg_Size') # get those\n",
    "\n",
    "# print(P4_eg,\"\\n\")\n",
    "# print(P4_eg.head) \n",
    "print(P4_eg[P4_eg.Eg_Size == 42])\n",
    "\n",
    "#The sizes of |Eg|\n",
    "print(P4_eg['Eg_Size'].value_counts()) #count number of 13s and 19s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "38ab941a-d108-4678-8df5-589c2ce1286c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     1  2  3  4  12  13  14  23  24  34  123  124  134  234  1234 Eg_Size\n",
      "0    0  0  0  0   0   1   0   0   0   1    1    0    1    1     1      42\n",
      "141  0  0  0  0   1   0   1   0   1   1    1    2    1    1     2      42\n",
      "143  0  0  0  0   1   0   1   0   0   0    1    1    1    0     1      42\n",
      "46   0  0  0  0   0   0   0   1   1   0    1    1    0    1     1      42\n",
      "119  0  0  0  0   1   1   1   0   0   1    1    1    2    1     2      42\n",
      "56   0  0  0  0   0   0   1   1   1   1    1    1    1    2     2      42\n",
      "144  0  0  0  0   1   0   0   0   1   0    1    1    0    1     1      42\n",
      "35   0  0  0  0   0   1   1   1   0   1    1    1    2    1     2      42\n",
      "45   0  0  0  0   0   0   0   1   0   1    1    0    1    1     1      42\n",
      "44   0  0  0  0   0   1   0   1   0   0    1    0    1    1     1      42\n",
      "67   0  0  0  0   0   0   1   0   0   1    0    1    1    1     1      42\n",
      "120  0  0  0  0   1   1   1   0   1   0    1    2    1    1     2      42\n",
      "68   0  0  0  0   0   0   0   0   1   1    0    1    1    1     1      42\n",
      "102  0  0  0  0   1   1   1   1   0   0    2    1    1    1     2      42\n",
      "101  0  0  0  0   1   1   0   1   1   0    2    1    1    1     2      42\n",
      "75   0  0  0  0   0   0   1   0   1   0    0    1    1    1     1      42\n",
      "97   0  0  0  0   1   1   0   1   0   1    2    1    1    1     2      42\n",
      "78   0  0  0  0   1   0   0   1   1   1    1    1    1    2     2      42\n",
      "89   0  0  0  0   1   0   0   1   0   0    1    1    0    1     1      42\n",
      "39   0  0  0  0   0   1   0   1   1   1    1    1    1    2     2      42\n",
      "83   0  0  0  0   1   0   1   1   1   0    1    2    1    1     2      42\n",
      "1    0  0  0  0   0   1   1   0   0   0    1    1    1    0     1      42\n",
      "11   0  0  0  0   0   1   1   0   1   1    1    1    2    1     2      42\n",
      "121  0  0  0  0   1   1   0   0   0   0    1    1    1    0     1      42\n",
      "(24, 16)\n"
     ]
    }
   ],
   "source": [
    "y=P4_eg[P4_eg.Eg_Size == 42]\n",
    "print(y)\n",
    "\n",
    "print(y.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04f2fbeb-00ba-45bc-97e7-28b5771ab5b8",
   "metadata": {},
   "source": [
    "### single function P2,single P3,single P4 what are |Eg| for each list"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56b6e4db-7242-4b55-9de5-c114f6cadccd",
   "metadata": {},
   "source": [
    "what is Eg for choose functions that have minimum eg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "id": "4e7794f5-1688-4bb4-b68d-fb4da7948d71",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The functions we are extending\n",
      "   1  2  12\n",
      "0  0  0   0\n",
      "hii    1  2  12  123  13  23  3  Eg_Size\n",
      "0  0  0   0    1   1   0  0        5\n",
      "1  0  0   0    1   0   0  0        5\n",
      "2  0  0   0    1   1   1  0        5\n",
      "3  0  0   0    1   0   1  0        5\n",
      "4  0  0   0    0   0   0  0        5\n",
      "['1', '2', '3', '12', '13', '23', '123']\n",
      "27\n",
      "['1', '2', '3', '12', '13', '23', '123']\n",
      "27\n",
      "['1', '2', '3', '12', '13', '23', '123']\n",
      "21\n",
      "['1', '2', '3', '12', '13', '23', '123']\n",
      "27\n",
      "['1', '2', '3', '12', '13', '23', '123']\n",
      "37\n",
      "   1  2  3  12  13  23  123  Eg_Size\n",
      "0  0  0  0   0   1   0    1       27\n",
      "1  0  0  0   0   0   0    1       27\n",
      "2  0  0  0   0   1   1    1       21\n",
      "3  0  0  0   0   0   1    1       27\n",
      "4  0  0  0   0   0   0    0       37\n"
     ]
    }
   ],
   "source": [
    "#Testing functions example\n",
    "n=2\n",
    "power_set_str,power_set_str_np1=get_n_np1_powersets(n)\n",
    "# P2\n",
    "\n",
    "funct_list=[{'1': 0,'2': 0,\"12\":0}] #eg is 5 \n",
    "Pn=pd.DataFrame(funct_list)\n",
    "print(\"The functions we are extending\")\n",
    "print(Pn)\n",
    "\n",
    "#The following should be functions in P4 that are extendions of functions in funct_list\n",
    "f_P2=get_Pnp1_for_single_funct(Pn,n)\n",
    "print(f_P2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "33b9e85b-5e54-4ea4-802b-6a657754461f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The functions we are extending\n",
      "   1  2  3  12  23  13  123\n",
      "0  0  0  0   0   0   1    1\n",
      "Number of functions: 13\n",
      "Are there duplicates? (0, 16) if (0,-) then no common rows\n",
      "    1  2  3  4  12  13  14  23  24  34  123  124  134  234  1234  Eg_Size\n",
      "0   0  0  0  0   0   1   0   0   0   1    1    0    1    1     1       13\n",
      "1   0  0  0  0   0   1   1   0   0   0    1    1    1    0     1       13\n",
      "2   0  0  0  0   0   1   1   0   0   1    1    1    2    1     2       13\n",
      "3   0  0  0  0   0   1   0   0   0   0    1    1    1    0     1       13\n",
      "4   0  0  0  0   0   1   1   0   0   1    1    1    1    1     2       13\n",
      "5   0  0  0  0   0   1   0   0   0   0    1    1    1    1     1       13\n",
      "6   0  0  0  0   0   1   1   0   0   1    1    1    1    1     1       13\n",
      "7   0  0  0  0   0   1   1   0   0   0    1    1    1    1     1       13\n",
      "8   0  0  0  0   0   1   0   0   0   0    1    0    1    1     1       13\n",
      "9   0  0  0  0   0   1   0   0   0   1    1    1    1    1     1       13\n",
      "10  0  0  0  0   0   1   1   0   1   1    1    1    1    1     2       13\n",
      "11  0  0  0  0   0   1   1   0   1   1    1    1    2    1     2       13\n",
      "12  0  0  0  0   0   1   0   0   0   0    1    0    1    0     1       13\n"
     ]
    }
   ],
   "source": [
    "#Testing functions example\n",
    "n=3\n",
    "power_set_str,power_set_str_np1=get_n_np1_powersets(n)\n",
    "# P3\n",
    "\n",
    "# funct_list=[{'1': 0,'2': 0,\"3\":0,\"12\":0,\"23\":0,\"13\":0,\"123\":0}]\n",
    "\n",
    "funct_list=[{'1': 0,'2': 0,\"3\":0,\"12\":0,\"23\":0,\"13\":1,\"123\":1}]\n",
    "\n",
    "# funct_list=[{'1': 0,'2': 0,\"3\":0,\"12\":0,\"23\":0,\"13\":0,\"123\":0}]\n",
    "\n",
    "Pn=pd.DataFrame(funct_list)\n",
    "print(\"The functions we are extending\")\n",
    "print(Pn)\n",
    "\n",
    "#The following should be functions in P4 that are extendions of functions in funct_list\n",
    "f_P3=get_Pnp1_for_single_funct(Pn,n)\n",
    "print(f_P3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22580ec0-355f-4dd8-96f7-f741e9993969",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Assuming the zero function gives max Eg what is Eg\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7aea050e-5df9-4d8f-b068-7cacb69fab38",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "power_set_str=get_powerset_str(5)\n",
    "\n",
    "\n",
    "#Find index of zero function\n",
    "\n",
    "# Take a g in P5 and calculate Eg\n",
    "# P5.iloc[1].to_dict()\n",
    "all_zero_funct={'1': 0,'2': 0,'3': 0,'4': 0,'5': 0,'12': 0,'13': 0,'14': 0,'15': 0,'23': 0,'24': 0,'25': 0,'34': 0,'35': 0,'45': 0,'123': 0,'124': 0,'125': 0,'134': 0,'135': 0,'145': 0,'234': 0,'235': 0,'245': 0,'345': 0,'1234': 0,'1235': 0,'1245': 0,'1345': 0,'2345': 0,'12345': 0}\n",
    "df=P5\n",
    "my_dict=all_zero_funct\n",
    "index = df.loc[df.eq(my_dict).all(1)].index[0]\n",
    "print(index)#5166 #works\n",
    "# df.loc[5166]#\n",
    "# Get Eg for this. #Should be maximum\n",
    "# Eg_zeroP5=get_Eg_for_single_g(df,index,power_set_str)\n",
    "# print(len(Eg_zeroP5)) #7580\n",
    "\n",
    "\"\"\"Assuming the \"Something\" function gives min Eg, what is Eg\n",
    "not the min or max function, something else.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4f2db93-ca23-46b3-a4f9-76e4c132804a",
   "metadata": {
    "tags": [],
    "toc-hr-collapsed": true
   },
   "source": [
    "## Check if all function in P3,P4,P5 generate a topological space: Yes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14c8e0f6-a8e7-4bea-9f36-324fb2e37861",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### P3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3492c0c7-9d9b-4f70-b8f2-d88967224c22",
   "metadata": {},
   "outputs": [],
   "source": [
    "#inputs\n",
    "df=P3\n",
    "power_set_str=get_powerset_str(3)\n",
    "check_all_funct_topspace(df,power_set_str)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2524d1a-961d-416b-b30f-59476b66e14a",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### P4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "473e1069-5d02-4497-acee-2729441e1377",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#inputs\n",
    "df=P4\n",
    "power_set_str=get_powerset_str(4)\n",
    "check_all_funct_topspace(df,power_set_str) #False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb5149c1-f805-426f-8719-0b5914af8c3b",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### P5 (where Eg for a function allow for calc of part of P6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0f69c5a-613c-4e92-ba29-3f5a151aa6dd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Take long: \n",
    "\n",
    "#inputs\n",
    "df=P5\n",
    "power_set_str=get_powerset_str(5)\n",
    "check_all_funct_topspace(df,power_set_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc848042-cdd3-4222-8a95-95d5e9a1ddb6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Works for some choosen example.\n",
    "\n",
    "df=P5\n",
    "power_set_str=get_powerset_str(5)\n",
    "numb=0 # function indexed at 0\n",
    "\n",
    "Eg=get_Eg_for_single_g(df,numb,power_set_str) # set of closed sets\n",
    "X_Eg= set(power_set_str) #total space ie powerset\n",
    "Top=list(set(x) for x in Eg) #topology from closed sets Eg #Size: 1178\n",
    "\n",
    "print(len(Top))\n",
    "\n",
    "#Check whether topological space:\n",
    "print(is_finite_topological_space(X_Eg, Top)) #The total powerset on 4 elements is not included.\n",
    "\n",
    "#Result:\n",
    "#numb=0\n",
    "#1178 #Top size\n",
    "#True"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "120dfdfe-b0fd-4983-a766-92126853e218",
   "metadata": {
    "tags": [],
    "toc-hr-collapsed": true
   },
   "source": [
    "# Leftover code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37e75e82-e545-4632-b91e-87ac2e76500c",
   "metadata": {},
   "outputs": [],
   "source": [
    "Question chatgpt:\n",
    "    \n",
    "let X be the set {'13', '23', '123', '12', '2', '3', '1'},\n",
    "\n",
    "and the topology be:\n",
    "{\n",
    "{'13', '23', '123', '1', '2'},\n",
    "{'13', '23', '123', '12', '2', '3', '1'},\n",
    "{'13', '1'},\n",
    "{'13', '2', '23', '1'},\n",
    "{'13', '2', '23'},\n",
    "{'13', '23'},\n",
    "{'13', '23', '123', '12', '2', '1'},\n",
    "{'23'},\n",
    "{'13'},\n",
    "{'13', '23', '123', '1', '2', '3'},\n",
    "{'13', '23', '1'},\n",
    "{'23', '2'},\n",
    "set() }\n",
    "\n",
    "what properties does this topological space have?\n",
    "\n",
    "This topological space \"is\":\n",
    "- Hausdorff,Compact,Second Countable,T0,T1,Regular,Normal\n",
    "\n",
    "This topological space \"is not\":\n",
    "- Discrete: not all subsets of X are open, for example the subsets {'12'} and {'3'} are not open in the topology T."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82245cc3-6aef-4d47-9238-309d07fdd4ea",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Examples for understanding code"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "812f3d77-e016-4f04-a3ce-3f15d6394da2",
   "metadata": {},
   "source": [
    "Are primitive closed sets a basis for the topology? (Maybe not relevant)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc939b0b-3234-4592-8b67-cadea14c5757",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "Asides:\n",
    "- Is Eg a matriod? No does not satisfy downward closure. In our sepcific example f(12)=1,f(123)=1 else 0 Take {1} in the closure of 1 i.e {1,13}, {1} is not a closed set.\n",
    "- Are primite closed sets a basis for the topology? Is there a way to construct $\\mathbb{E}(g)$ using primitive (using minimal basis)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6ae0abb-2438-4c3d-b8e3-9b22e1c8d0ad",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# dict_PC,set_PC=get_primitive_for_single_g(df,numb)\n",
    "\n",
    "# # print(dict_PC) #Remembers what the terms of powerset is\n",
    "# set_PC.append(frozenset()) # add empty set: this is the set of primitive closed sets\n",
    "# # for i in set_PC:\n",
    "# #     print(set(i))\n",
    "    \n",
    "# X_Eg= {'13', '23', '123', '12', '2', '3', '1'} #total space ie powerset\n",
    "# Prim_Top= list(set(x) for x in set_PC) # primitive set as a topology? is it a basis?\n",
    "\n",
    "# def is_basis(X, T, B):\n",
    "#     if not all(isinstance(x, set) for x in B):\n",
    "#         # B must be a collection of sets\n",
    "#         return False\n",
    "#     if not all(x.issubset(X) for x in B):\n",
    "#         # Each element of B must be a subset of X\n",
    "#         return False\n",
    "    \n",
    "    \n",
    "#     if not all(x.intersection(y) == set() for x in B for y in B if x != y):\n",
    "#         # The intersection of any two distinct elements of B must be empty\n",
    "#         return False\n",
    "    \n",
    "#     if not all(x.union(y) in T for x in B for y in B):\n",
    "#         # The union of any two elements of T must be in T\n",
    "#         return False\n",
    "    \n",
    "#     if not all(x.issubset(y) or y.issubset(x) for x in B for y in T):\n",
    "#         # Each element of B must be a subset of some element of T or conversely\n",
    "#         return False\n",
    "#     return True\n",
    "\n",
    "# # Example usage\n",
    "# X = {'a', 'b', 'c'}\n",
    "# T = [X, set(), {'a', 'b'}, {'b', 'c'}, {'b'}]\n",
    "# B = [{'a', 'b'}, {'b', 'c'}]\n",
    "# print(is_basis(X, T, B)) # prints True\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e67f6d6-6df5-45ad-9f97-3a91381e76c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get all functions in P3 stored a data frame\n",
    "\n",
    "#Construct P3\n",
    "pre_functs=[[\"12\",\"123\"],[\"13\",\"123\"],[\"23\",\"123\"],[\"12\",\"13\",\"123\"],[\"12\",\"23\",\"123\"],[\"13\",\"23\",\"123\"],[\"12\",\"13\",\"23\",\"123\"],[],[\"123\"]]\n",
    "ds=[]\n",
    "for vals in pre_functs:\n",
    "    builder1=[(x,1) for x in vals]\n",
    "    builder0=[(x,0) for x in power3 if x not in vals]\n",
    "    f_dict={**dict(builder1),**dict(builder0)}\n",
    "    ds.append(f_dict)\n",
    "big={'123': 2, '1': 0, '2': 0, '3': 0, '12': 1, '13': 1, '23': 1} #remaining case, with 2 value\n",
    "dp=ds+[big,]\n",
    "\n",
    "#Build dataframe\n",
    "P3 = pd.DataFrame(dp)\n",
    "\n",
    "#order columns of P3\n",
    "power3=get_powerset_str(3) #powerset on 3 elements #for \n",
    "column_order=power3 #as strings\n",
    "P3 = P3.reindex(columns=column_order)\n",
    "\n",
    "print(P3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50638e8e-56f6-42ea-a952-78841485fa15",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Checking if the calculations for P5, are the same as our old calculations for P5 in xlsx.\n",
    "\n",
    "# #Previous data P5 in correct format\n",
    "# dataframe2 = pd.read_excel('P4_P5_xlsx\\P_5.xlsx')\n",
    "\n",
    "# #Make sure columns are in the same order as P5\n",
    "\n",
    "# #Reordering columns\n",
    "# column_order=[int(x) for x in power_set_str_np1]\n",
    "# dataframe2 = dataframe2.reindex(columns=column_order)\n",
    "\n",
    "# #Making sure columns are the same, in string format\n",
    "# dataframe2=string_cols(dataframe2)\n",
    "\n",
    "# # print(dataframe2.head)\n",
    "# print(dataframe2.shape) #(10334, 31)\n",
    "\n",
    "# # n=4\n",
    "# # power_set_str,alt_power_set_str_np1=get_n_np1_powersets(n)\n",
    "# # check_functs_MSA(dataframe2,power_set_str_np1).shape #(0,0) i.e all are msa\n",
    "\n",
    "# Comparision of P5 and dataframe 2. We see they are the same.\n",
    "\n",
    "# compar=check_df_duplicates(P5,dataframe2,\"left\",True) #True:10334 , False:0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8bcb6fb-157f-48ad-a4f5-02b4fee4c27d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # The calculations for P4, are the same as our old calculations for P4 in xlsx.\n",
    "\n",
    "# dataframe1 = pd.read_excel('P4_P5_xlsx\\P_4.xlsx')\n",
    "# #Making sure columns are the same, in string format and order as P4 above\n",
    "# dataframe1=string_cols(dataframe1)\n",
    "\n",
    "# # #Comparision of P4 and xlsx version\n",
    "# compar=check_df_duplicates(P4,dataframe1,\"left\",True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b88dc3c7-0fb0-44e6-8253-15e1f937bbb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# n=3\n",
    "# # Create a set with 3 elements\n",
    "# # s = {1, 2, 3}\n",
    "# s=set(range(1,n+1))\n",
    "\n",
    "# # Generate all possible subsets of the set\n",
    "# power_set = [set(x)  for r in range(len(s) + 1) for x in itertools.combinations(s, r)]\n",
    "\n",
    "# # Print the power set\n",
    "# print(power_set)\n",
    "\n",
    "# power_set_str=[set_to_string(x) for x in power_set]\n",
    "# power_set_str=[set_to_string(x) for x in power_set if len(x)>0]\n",
    "# power_set_str\n",
    "\n",
    "# #Get a function in P3\n",
    "\n",
    "# val1_loc=[\"12\",\"123\"] #where it take 1\n",
    "# val0_loc= [x for x in power_set_str if x not in val1_loc] #where it takes 0\n",
    "# # print(val0_loc)\n",
    "\n",
    "# builder1=[(x,1) for x in val1_loc]\n",
    "# builder0=[(x,0) for x in val0_loc]\n",
    "# builder0\n",
    "# dict(builder0)\n",
    "# dict(builder1)\n",
    "# f_dict={**dict(builder1),**dict(builder0)}\n",
    "# f_dict\n",
    "\n",
    "# A,B=\"1\",\"123\"\n",
    "# print(f\"Is {A} subset of {B}, f minimal? : {fmin(A,B,f_dict)}\")\n",
    "# print(f\"Is {A} subset of {B}, f maximal? : {fmax(A,B,f_dict)}\")\n",
    "\n",
    "#Testing for obtaining primitive sets:\n",
    "# #Example\n",
    "# for x in power_set_str:\n",
    "#     print(x,get_barj({x},f_dict))\n",
    "\n",
    "#As all the closed sets, checked by hand:\n",
    "# 3 T term {'13', '123', '1', '3', '23', '2'}\n",
    "# 1 {'1', '13'}\n",
    "# 2 {'23', '2'}\n",
    "# 12 {'1', '12', '123', '13', '2', '23'} \n",
    "# 13 {'13'} # should be just 13\n",
    "# 23 {'23'} # should be 23\n",
    "# 123 {'13', '123', '1', '23', '2'} # should not have 3 or 12\n",
    "\n",
    "# Store primitive closed sets \n",
    "\n",
    "# PC=[(x,get_barj({x},f_dict)) for x in power_set_str]\n",
    "# dict_PC=dict(PC)\n",
    "# set_PC=[sorted_frozenset(get_barj({x},f_dict)) for x in power_set_str]\n",
    "# # set(set_PC)\n",
    "# Eg={sorted_frozenset(power_set_str)}.union(set_PC)\n",
    "\n",
    "# Using main functions:\n",
    "# Eg=list(get_Eg(dict_PC,power_set_str,set_PC))\n",
    "# # print(len(x)) # I get like 13 so this is an issue.\n",
    "# Eg\n",
    "# Eg.append(\"empty\")\n",
    "# Eg\n",
    "\n",
    "\n",
    "#Rows are not in the same order:\n",
    "# # print(P4.equals(dataframe1))\n",
    "\n",
    "#Checking if duplicates, #https://stackoverflow.com/questions/48647534/find-difference-between-two-data-frames?noredirect=1&lq=1\n",
    "# df = P4.merge(dataframe1, how = 'inner' ,indicator=False)\n",
    "\"\"\" \n",
    "# first dataframe\n",
    "df1 = pd.DataFrame({\n",
    "    'Age': ['20', '14', '56', '28', '10'],\n",
    "    'Weight': [59, 29, 73, 56, 48]})\n",
    "display(df1)\n",
    "\n",
    "# second dataframe\n",
    "df2 = pd.DataFrame({\n",
    "    'Age': ['16', '20', '24', '40', '22'],\n",
    "    'Weight': [55, 59, 73, 85, 56]})\n",
    "display(df2)\n",
    "\n",
    "#Common rows:\n",
    "df = df1.merge(df2, how = 'inner' ,indicator=False)\n",
    "print(df)\n",
    "\"\"\"\n",
    "\n",
    "\"The above method only works for those data frames that don't already have duplicates themselves. For example:\"\n",
    "no_dups=pd.concat([P4,dataframe1]).drop_duplicates(keep=False)\n",
    "no_dups.head()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "toc-autonumbering": false,
  "toc-showcode": false,
  "toc-showmarkdowntxt": false,
  "toc-showtags": false
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
