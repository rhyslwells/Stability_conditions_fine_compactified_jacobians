{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "99f39ef1-d2d8-45cd-ba95-b4f7045192cc",
   "metadata": {},
   "source": [
    "Note: I have forgotten to check for f3 from f2, that f3(\"empty\")+f(\"1...n\")=-2.\n",
    "\n",
    "I have not corrected it here but have in\n",
    "\n",
    "Desktop/For github/Stability_Conditions/MSA/Dimension_Analysis/empty_poly_selector.ipynb\n",
    "\n",
    "where i have outputed to txt files the list of [f2,f3,dim_poly]."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e5fc415-1f30-409e-b802-f051fb4f9ed8",
   "metadata": {
    "tags": [],
    "toc-hr-collapsed": true
   },
   "source": [
    "# Overview"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "174c4f90-96d6-4c38-aacb-b3cf8b774d3b",
   "metadata": {},
   "source": [
    "Following the documentation we generate P2,P3,P4,P5 so we can investigate the genus 2 case and find a counterexample akin to the genus 1 case."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cf1f875-e215-40c3-8dc1-8c85b052db87",
   "metadata": {},
   "source": [
    "$$P_n=\\bigsqcup_{g \\in P_{n-1}} \\rho^{-1}(g)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d507465-16e6-4eba-b5b1-45fc45be4e88",
   "metadata": {},
   "source": [
    "We take extensions of $f2 \\in P_{n-1}$, which are our $f3$ functions once we apply the complement condition."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29bb5443-f712-4340-893f-af8f31f18949",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "# Table of contents\n",
    "1. [Building P2-P5](#s1)\n",
    "    1. [Functions](#s11)\n",
    "    2. [Cases](#s12)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "431c2a14-6e49-47ec-a3d9-668b7ed92fbc",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Building P2-P5 <a name=\"s1\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be280e86-74e7-4066-bb2b-466dd64f7522",
   "metadata": {
    "tags": [],
    "toc-hr-collapsed": true
   },
   "source": [
    "## Functions <a name=\"s11\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "1ac3ed41-821e-47aa-90b1-1fc5beb70c64",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#General use functions:\n",
    "def set_to_string(s):\n",
    "    s = list(s)\n",
    "    s.sort()\n",
    "    # Convert the list of numbers to a list of strings\n",
    "    my_list=s\n",
    "    my_list = [str(i) for i in my_list]\n",
    "    # Use the join() function to convert the list of strings to a single string\n",
    "    result = ''.join(my_list)\n",
    "    return result\n",
    "def string_to_set(s):\n",
    "    my_list = list(s)\n",
    "    # Convert the list of strings to a list of integers\n",
    "    my_list = list(map(int,my_list))\n",
    "    # Convert the list to a set\n",
    "    my_set = set(my_list)\n",
    "    return my_set\n",
    "def get_powerset_str(n):\n",
    "    s=set(range(1,n+1))\n",
    "    power_set = [set(x)  for r in range(len(s) + 1) for x in itertools.combinations(s, r)]\n",
    "    power_set_str=[set_to_string(x) for x in power_set if len(x)>0]\n",
    "\n",
    "    return power_set_str\n",
    "def get_n_np1_powersets(n):\n",
    "    s=set(range(1,n+1))\n",
    "    power_set = [set(x)  for r in range(len(s) + 1) for x in itertools.combinations(s, r)]\n",
    "    power_set_str=[set_to_string(x) for x in power_set if len(x)>0]\n",
    "\n",
    "    np1=n+1\n",
    "    sp1=set(range(1,n+2))\n",
    "    power_set_np1 = [set(x)  for r in range(np1+1 + 1) for x in itertools.combinations(sp1, r)]\n",
    "    power_set_str_np1=[set_to_string(x) for x in power_set_np1 if len(x)>0]\n",
    "\n",
    "    return power_set_str,power_set_str_np1\n",
    "def sorted_frozenset(s):\n",
    "    t=sorted(s)\n",
    "    r=frozenset(t)\n",
    "    return r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "e78ba416-3f57-4a1c-9dba-85adac3a2388",
   "metadata": {},
   "outputs": [],
   "source": [
    "# x=[set_to_string,string_to_set,get_n_np1_powersets,get_powerset_str,sorted_frozenset]\n",
    "# find_functions_used_l(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "89b3a80d-8f2a-47d0-8e46-4bdddfdd0a6f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Functions for primitive closed sets:\n",
    "def fmin(A,B,f_dict):\n",
    "    #A included in B\n",
    "    # global f_dict\n",
    "    \n",
    "    valA=f_dict[A]\n",
    "    valB=f_dict[B]\n",
    "    compBA=set_to_string(string_to_set(B)-string_to_set(A)) # B minus A\n",
    "    if len(compBA)==0: # to avoide when taking the complement gives empty set\n",
    "        return False\n",
    "    valBA=f_dict[compBA]\n",
    "    if valB==valA+valBA:\n",
    "        return True\n",
    "    else:\n",
    "        return False\n",
    "def fmax(A,B,f_dict):\n",
    "    #A included in B\n",
    "    # global f_dict\n",
    "    \n",
    "    valA=f_dict[A]\n",
    "    valB=f_dict[B]\n",
    "    compBA=set_to_string(string_to_set(B)-string_to_set(A)) # B minus A\n",
    "    \n",
    "    #A has to be a subset of B first\n",
    "    if len(compBA)==0: # to avoide when taking the complement gives empty set\n",
    "        return False\n",
    "    valBA=f_dict[compBA]\n",
    "    if valB==valA+valBA+1:\n",
    "        return True\n",
    "    else:\n",
    "        return False\n",
    "def next_j(ji,f_dict):\n",
    "    #ji is the ith set on constructing the closure. \n",
    "    # print(\"ji\",ji)\n",
    "    # global f\n",
    "    my_list=[]    \n",
    "    for x in power_set_str:\n",
    "        for y in ji:\n",
    "            if fmin(y,x,f_dict) and string_to_set(y).issubset(string_to_set(x)):\n",
    "                # print(\"y is\",y)\n",
    "                my_list.append(x)\n",
    "                # print(f\"{x} containing {y} f min \")\n",
    "    for x in power_set_str:\n",
    "        for y in ji:\n",
    "            if fmax(x,y,f_dict) and string_to_set(x).issubset(string_to_set(y)):\n",
    "                my_list.append(x)\n",
    "    jip1=set(my_list).union(ji)\n",
    "    return jip1    \n",
    "def rec_j(T,S,f_dict):\n",
    "    # T=Ji and S=Ji-1, |T|\\ge |S|.\n",
    "    C=string_to_set(T)-string_to_set(S) #T \\S\n",
    "    C={str(x) for x in C}    \n",
    "    # print(\"C\",C,f\"S term {S}\",f\"T term {T}\")\n",
    "    if len(C)==0: #Check if closed set\n",
    "        return T # return closed set\n",
    "    if len(C)>0:\n",
    "        U=next_j(T,f_dict) #next_j(T)\n",
    "        Z=rec_j(U,T,f_dict)\n",
    "    # print(T)\n",
    "    return Z\n",
    "def get_barj(J,f_dict):\n",
    "    j0=J\n",
    "    j1=next_j(j0,f_dict)\n",
    "    T=rec_j(j1,j0,f_dict)\n",
    "    return T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "264254c0-e0c7-4a9f-890a-e102bc24d399",
   "metadata": {},
   "outputs": [],
   "source": [
    "# x=[fmin,rec_j,next_j,fmax,get_barj]\n",
    "# find_functions_used_l(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "b4d75469-4f1f-4201-bb53-2620e4a4016c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Main: making use of primitives to get all closed sets:\n",
    "def Recur(Ibar,indicators,Eg,Memory,dict_PC):\n",
    "    \"\"\"\n",
    "    inputs:\n",
    "    returns:\n",
    "    \"\"\"\n",
    "    # global dict_PC\n",
    "        \n",
    "    # To save recalulating we uses this series of checks, if fails checks end recursion turn.\n",
    "    \n",
    "    if indicators in Memory:# Memortisation for preventing repeated calculations.\n",
    "        # print(\"Fails Memory check\")\n",
    "        # print(f\"\\n At this failure we have \\n Eg: {len(Eg)} \\n Memory: {len(Memory)} \\n\")\n",
    "        return Eg,Memory\n",
    "    \n",
    "    y=indicators[-1]\n",
    "    # print(f\"term taken for primitive closed set we are unioning {y}\")\n",
    "    P_y=dict_PC[y] # the primitive closed set wrt y\n",
    "    Jbar= Ibar.union(P_y)\n",
    "    \n",
    "    if len(Jbar)==len(power_set_str): # We always have the power set in Eg\n",
    "        # print(f\"Fails as {Jbar} == poweset\")\n",
    "        # print(f\"\\n At this failure we have \\n Eg: {len(Eg)} \\n Memory: {len(Memory)} \\n\")\n",
    "        return Eg,Memory\n",
    "              \n",
    "    if Jbar in Eg:\n",
    "        # print(f\"Fails as {Jbar} in Eg\")\n",
    "        # print(f\"\\n At this failure we have \\n Eg: {len(Eg)} \\n Memory: {len(Memory)} \\n\")\n",
    "        return Eg,Memory\n",
    "    else:\n",
    "        # print(\"Passes Checks \\n\")\n",
    "        #Add the new data to memory\n",
    "        set_indicators=set([sorted_frozenset(tuple(indicators)),])\n",
    "        Memory=Memory.union(set_indicators)\n",
    "        \n",
    "        Jbar_union=set([sorted_frozenset(Jbar),])#Correct format for union\n",
    "        #Add new data to Eg\n",
    "        Eg=Eg.union(Jbar_union) # will ge an issue with where defines and globals.\n",
    "\n",
    "        #Move onto the next recursion.\n",
    "        power_complment=set(power_set_str)-Jbar\n",
    "        for w in power_complment: # pick one in the complement to to continus exhaustive method\n",
    "            new_indicators=indicators+(w,) #want to add it at end\n",
    "            Eg,Memory=Recur(Jbar,new_indicators,Eg,Memory,dict_PC)\n",
    "            # print(f\"Done with {new_indicators} \\n\")\n",
    "                \n",
    "    return Eg,Memory # will also be an issue.\n",
    "def get_Eg(dict_PC,power_set_str,set_PC):\n",
    "    \n",
    "    Eg={frozenset(power_set_str)}.union(set_PC) #empty set is an issue. #Main set we wish to construct\n",
    "    Memory={sorted_frozenset((\"0\"))}    # contains indicators which are tuples of x \\in powerset for primitives sets.\n",
    "    \n",
    "    for x in power_set_str:\n",
    "        Ibar=dict_PC[x] #primitive closed set\n",
    "        power_complment=set(power_set_str)-Ibar\n",
    "        for y in power_complment: #exhaustively getting all closed sets.\n",
    "            indicators=(x,y)\n",
    "            set_indicators=set([sorted_frozenset(tuple(indicators)),])\n",
    "            Memory=Memory.union(set_indicators)            \n",
    "            Eg,Memory=Recur(Ibar,indicators,Eg,Memory,dict_PC)\n",
    "            \n",
    "    return Eg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "f470f1e8-1998-4a6f-b942-f7b4cec73089",
   "metadata": {},
   "outputs": [],
   "source": [
    "# x=[Recur,get_Eg]\n",
    "# find_functions_used_l(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "0c9fd8d6-e510-4a0e-bc2c-5078b05744a1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Creating functions in $P_{n+1}$ from Eg for this f_dict.\n",
    "def add_n_p_1_to_string(string,n):\n",
    "    #Adds n+1 to strings subsets\n",
    "    np1string=string+(f\"{n+1}\")\n",
    "    return np1string\n",
    "def get_extension_of_f(f_dict,extender,power_set_str,n):\n",
    "    # Need to attatch n+1 to strings to build extension of function by epsilon.\n",
    "\n",
    "    # Build the extension of f by epsilon.\n",
    "    \n",
    "    # Add the (key,values) of f_dict\n",
    "    builder_ext_0=[(k, f_dict[k]) for k in f_dict]\n",
    "\n",
    "    #We add 1 to the function value of f_dict for x in extender\n",
    "    builder_ext_p1=[(add_n_p_1_to_string(x,n),f_dict[x]+1) for x in list(extender)]\n",
    "\n",
    "    #We add 0 to the function value of f_dict for x NOT in extender\n",
    "    compl_extender=set(power_set_str) - set(extender)\n",
    "    builder_ext_p2=[(add_n_p_1_to_string(x,n),f_dict[x]+0) for x in list(compl_extender)]\n",
    "    \n",
    "    # We include f({n+1})=0\n",
    "    builder_ext_np1=[(add_n_p_1_to_string(\"\",n),int(0)),]\n",
    "\n",
    "\n",
    "    #Compile together.\n",
    "    extension_f_dict={**dict(builder_ext_0),**dict(builder_ext_p1),**dict(builder_ext_p2),**dict(builder_ext_np1)}\n",
    "    \n",
    "    #Example\n",
    "    # get_extension_of_f(frozenset({'13', '23'}))\n",
    "\n",
    "    return extension_f_dict\n",
    "def empty_case_get_extension_of_f(f_dict,extender,power_set_str,n):\n",
    "    # Need to attatch n+1 to strings to build extension of function by epsilon.\n",
    "\n",
    "    # Build the extension of f by epsilon.\n",
    "    \n",
    "    # Add the (key,values) of f_dict\n",
    "    builder_ext_0=[(k, f_dict[k]) for k in f_dict]\n",
    "\n",
    "    builder_ext_p2=[(add_n_p_1_to_string(x,n),f_dict[x]+0) for x in list(power_set_str)]\n",
    "\n",
    "    builder_ext_np1=[(add_n_p_1_to_string(\"\",n),int(0)),]\n",
    "\n",
    "    \n",
    "    #Compile together.\n",
    "    extension_f_dict={**dict(builder_ext_0),**dict(builder_ext_p2),**dict(builder_ext_np1)}\n",
    "    \n",
    "    return extension_f_dict\n",
    "def Build_Pn_1(list_df_Pn,power_set_str,n):\n",
    "    # Returns:packets of extensions for each f_dict from Pn, can check those with max size of closed sets (later)\n",
    "    \n",
    "    # global power_set_str\n",
    "    \n",
    "    P_n1=[]\n",
    "    for f_dict in list_df_Pn:\n",
    "\n",
    "        # Store primitive closed sets \n",
    "\n",
    "        PC=[(x,get_barj({x},f_dict)) for x in power_set_str]\n",
    "        dict_PC=dict(PC)\n",
    "        set_PC=[sorted_frozenset(get_barj({x},f_dict)) for x in power_set_str]\n",
    "\n",
    "        #Get all closed sets for extensions for f_dict\n",
    "        Eg=list(get_Eg(dict_PC,power_set_str,set_PC))\n",
    "        Eg.append(\"empty\")\n",
    "\n",
    "        #Build extensions for f_dict\n",
    "        extension_of_f_dict=[]\n",
    "        for term in Eg:\n",
    "            if term ==\"empty\":\n",
    "                extension_of_f_dict.append(empty_case_get_extension_of_f(f_dict,term,power_set_str,n))\n",
    "            else:\n",
    "                extension_of_f_dict.append(get_extension_of_f(f_dict,term,power_set_str,n))\n",
    "\n",
    "        #We record packets of extensions where we take +1\n",
    "        P_n1.append(extension_of_f_dict)\n",
    "    \n",
    "    return P_n1\n",
    "\n",
    "def extend_sets_to_Pnp1(P_n1):\n",
    "    #Return the data frame of Pn+1\n",
    "    \n",
    "    final_P_n1=[]\n",
    "    for ls in P_n1:\n",
    "        final_P_n1=final_P_n1+ls # Adding all the extension packets together.\n",
    "    df=pd.DataFrame(final_P_n1)\n",
    "    # df = df.reindex(columns=power_set_str_np1)\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "4b4cd4a6-2145-4b93-a373-89d570b21fff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# x=[add_n_p_1_to_string,Build_Pn_1,empty_case_get_extension_of_f,get_extension_of_f,extend_sets_to_Pnp1]\n",
    "# find_functions_used_l(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "61fe8504-7cbc-4214-b55c-f80ecf31e9e1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# This function returns Pn+1\n",
    "def get_Pnp1(Pn,n):\n",
    "\n",
    "    #inputs\n",
    "    # Pn and n.\n",
    "\n",
    "    #New data created\n",
    "    np1=n+1\n",
    "    power_set_str,power_set_str_np1=get_n_np1_powersets(n)\n",
    "\n",
    "    #Load Pn as a list of dictionaries.\n",
    "    list_df_Pn=Pn.to_dict(orient='records')\n",
    "\n",
    "    #Create Pn+1 and hold as dataframe.\n",
    "    P_n1=Build_Pn_1(list_df_Pn,power_set_str,n)\n",
    "    Pnp1=extend_sets_to_Pnp1(P_n1)\n",
    "\n",
    "    Pnp1 = Pnp1.reindex(columns=power_set_str_np1)\n",
    "\n",
    "    inspect_Pn1(Pnp1)\n",
    "\n",
    "    return Pnp1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "d5d68a7d-9816-47ab-b6ed-6ab8a69ec15f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# x=[get_Pnp1]\n",
    "# find_functions_used_l(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "80ef21c0-1241-4e24-89c7-7b9bf46187e5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Analysis of Pn+1\n",
    "def string_cols(df):\n",
    "    \n",
    "    #Changing them to string format!only\n",
    "    #reorders the columns of dataframe to powerset list order.\n",
    "    \n",
    "    cols=df.columns\n",
    "    rnamer=dict([(cols[i],str(cols[i])) for i in range(0,len(cols))])\n",
    "    df=df.rename(columns=rnamer)\n",
    "    \n",
    "    return df\n",
    "def inspect_Pn1(df):\n",
    "    print(\"Number of functions:\",len(df.index))\n",
    "    # df=df.unique()\n",
    "\n",
    "    duplicate_rows = df[df.duplicated()]\n",
    "    print(\"Are there duplicates?\",duplicate_rows.shape, \"if (0,-) then no common rows\")\n",
    "\n",
    "    # df.head()\n",
    "    return\n",
    "def check_df_duplicates(df1,df2,How,value):\n",
    "    #Compare df1,df2 if that common rows\n",
    "    \n",
    "    # how is either \"right\" or \"left\"\n",
    "    # for left gives those in df1 and says True or false if also in df2\n",
    "    \n",
    "    #     example\n",
    "    #     df1 = pd.DataFrame({'team' : ['A', 'B', 'C', 'D', 'E'], \n",
    "    #                     'points' : [12, 15, 22, 29, 24]}) \n",
    "    #  #create second DataFrame\n",
    "    # df2 = pd.DataFrame({'team' : ['A', 'D', 'F', 'G', 'H'],\n",
    "    #                     'points' : [12, 29, 15, 19, 10]})\n",
    "    \n",
    "    #merge two dataFrames and add indicator column\n",
    "    all_df = pd.merge(df1, df2, how=How, indicator='exists')\n",
    "\n",
    "    #add column to show if each row in first DataFrame exists in second\n",
    "    all_df['exists'] = np.where(all_df.exists == 'both', True, False)\n",
    "\n",
    "    #view updated DataFrame\n",
    "    # print (all_df)\n",
    "\n",
    "    m=all_df.loc[all_df['exists'] == True]\n",
    "    \n",
    "    #Number of terms common\n",
    "    num_items = m.loc[m['exists'] == value].shape[0]\n",
    "    print(f\"Number of those in both: {num_items} \\n\")\n",
    "\n",
    "    return m    \n",
    "def is_valid_function(f,power_set_str):\n",
    "    # define the condition to check for each function\n",
    "    Pn=[string_to_set(x) for x in power_set_str] #convert to sets\n",
    "    \n",
    "    # f(T) must equal 0 for elements T in Pn with size 1\n",
    "\n",
    "    for T in Pn:\n",
    "        if any(f[set_to_string(T)] != 0 for T in Pn if len(T) == 1):\n",
    "            return False\n",
    "    # for any I, J in Pn such that I and J have no common elements, 1 > f(I U J) - f(I) - f(J) or 0 = f(I U J) - f(I) - f(J)\n",
    "    for I in Pn:\n",
    "        for J in Pn:\n",
    "            if len(I.intersection(J)) == 0 and f[set_to_string(I.union(J))] - f[set_to_string(I)] - f[set_to_string(J)] < 0 or f[set_to_string(I.union(J))] - f[set_to_string(I)] - f[set_to_string(J)] > 1:\n",
    "            # for any I, J in Pn such that I and J have no common elements, 0 <= f(I U J) - f(I) - f(J) <= 1\n",
    "                return False\n",
    "    return True\n",
    "def check_functs_MSA(df,power_set_str):\n",
    "\n",
    "    num_rows=df.shape[0]\n",
    "\n",
    "    bad_f_in_df=[] #indices of functions fail to be MSA in df\n",
    "    for i in range(0,num_rows):\n",
    "        f=df.loc[i].to_dict()\n",
    "        if  is_valid_function(f,power_set_str)==False:\n",
    "            bad_f_in_df.append(f)\n",
    "    \n",
    "    bad_df=pd.DataFrame(bad_f_in_df)\n",
    "    \n",
    "    return bad_df #indices of functions that are not MSA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "5b6971e3-8782-40ee-9dba-b53d10d607f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# x=[string_cols,check_functs_MSA,is_valid_function,check_df_duplicates,inspect_Pn1]\n",
    "# find_functions_used_l(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da8462b5-db47-49c6-a71d-5aa7e5c8a633",
   "metadata": {
    "tags": [],
    "toc-hr-collapsed": true
   },
   "source": [
    "## Cases"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f72d33f5-d489-4f1f-9e37-4675f9d551c3",
   "metadata": {
    "tags": []
   },
   "source": [
    "### P2 store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "id": "99243a6f-6b94-4de5-abd7-0423ca4e2c80",
   "metadata": {},
   "outputs": [],
   "source": [
    "P2=pd.DataFrame([{'1': 0, '2': 0, '12': 0},{'1': 0, '2': 0, '12': 1}])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "id": "7fbd7474-edcd-4ab5-841e-196d45907c27",
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open('results\\P2.pkl', 'wb') as file:\n",
    "#     pickle.dump(P2, file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f323cfe-d650-4f13-858a-26ba3ad73953",
   "metadata": {
    "tags": [],
    "toc-hr-collapsed": true
   },
   "source": [
    "### P2 extending to P3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "id": "a3b210af-29e7-49ec-b1cf-e6aa08813037",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of functions: 10\n",
      "Are there duplicates? (0, 7) if (0,-) then no common rows\n",
      "   1  2  3  12  13  23  123\n",
      "0  0  0  0   0   1   1    1\n",
      "1  0  0  0   0   1   0    1\n",
      "2  0  0  0   0   0   1    1\n",
      "3  0  0  0   0   0   0    1\n",
      "4  0  0  0   0   0   0    0\n",
      "5  0  0  0   1   0   1    1\n",
      "6  0  0  0   1   1   1    2\n",
      "7  0  0  0   1   1   1    1\n",
      "8  0  0  0   1   1   0    1\n",
      "9  0  0  0   1   0   0    1\n"
     ]
    }
   ],
   "source": [
    "#Easy to know $P_2$\n",
    "P2=pd.DataFrame([{'1': 0, '2': 0, '12': 0},{'1': 0, '2': 0, '12': 1}])\n",
    "\n",
    "n=2\n",
    "power_set_str,power_set_str_np1=get_n_np1_powersets(n)\n",
    "P3=get_Pnp1(P2,2)\n",
    "print(P3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "id": "1fb90373-3155-46af-bdd8-bdb88d928e6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_list_of_dicts = P3.to_dict(orient='records')\n",
    "# df_list_of_dicts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "9bf839f7-edcb-46a4-a1dc-f75b069de2a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open('results\\P3.pkl', 'wb') as file:\n",
    "#     pickle.dump(P3, file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c403d0a-8acc-481d-bc27-bc942375e9c4",
   "metadata": {
    "tags": [],
    "toc-hr-collapsed": true
   },
   "source": [
    "### P3 extending to P4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "6c271174-ad92-4738-b34b-29234e062c8d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of functions: 154\n",
      "Are there duplicates? (0, 15) if (0,-) then no common rows\n"
     ]
    }
   ],
   "source": [
    "n=3\n",
    "power_set_str,power_set_str_np1=get_n_np1_powersets(n)\n",
    "P4=get_Pnp1(P3,3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a95c6b01-3351-4aef-a04f-9851f5ac1487",
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open('results\\P4.pkl', 'wb') as file:\n",
    "#     pickle.dump(P4, file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c99dd4c3-6ed0-40a1-85d6-5e1165e07cf4",
   "metadata": {
    "tags": []
   },
   "source": [
    "### P4 extending to P5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b75fd0fa-23f4-4ae6-893a-533692d9c75d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of functions: 10334\n",
      "Are there duplicates? (0, 31) if (0,-) then no common rows\n"
     ]
    }
   ],
   "source": [
    "n=4\n",
    "power_set_str,power_set_str_np1=get_n_np1_powersets(n) #gives powerset on 4 and 5 elements respectively\n",
    "P5=get_Pnp1(P4,4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "7fe1e094-5308-48d9-8910-ed6b3212166e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open('results\\P5.pkl', 'wb') as file:\n",
    "#     pickle.dump(P5, file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "caac0991-f2cf-4874-bd01-51b1fa475d43",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Cases Translated $f(\\{1\\})=1$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ffdce7a-de8d-45bc-8f24-a0a64dae4dab",
   "metadata": {},
   "source": [
    "Here we take the translate case where f({1})=1 and store as 'results\\P3_trans.pkl'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9e4486c-5d48-44cd-bc45-420b9db1f149",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### P2 store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "f648b7ea-6e19-411a-ae3b-87758707bdcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "P2=pd.DataFrame([{'1': 1, '2': 0, '12': 1},{'1': 1, '2': 0, '12': 2}])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "60104c86-b0c2-492b-9b93-bb46a5428351",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('results\\P2_trans.pkl', 'wb') as file:\n",
    "    pickle.dump(P2, file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ce07010-4db0-4ea1-b481-1bcb5c9a3468",
   "metadata": {
    "tags": [],
    "toc-hr-collapsed": true
   },
   "source": [
    "### P2 extending to P3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "7e603905-57d4-474b-8953-f233ae44fc21",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of functions: 10\n",
      "Are there duplicates? (0, 7) if (0,-) then no common rows\n",
      "   1  2  3  12  13  23  123\n",
      "0  1  0  0   1   1   0    2\n",
      "1  1  0  0   1   2   1    2\n",
      "2  1  0  0   1   1   1    2\n",
      "3  1  0  0   1   2   0    2\n",
      "4  1  0  0   1   1   0    1\n",
      "5  1  0  0   2   2   0    2\n",
      "6  1  0  0   2   2   1    3\n",
      "7  1  0  0   2   1   1    2\n",
      "8  1  0  0   2   2   1    2\n",
      "9  1  0  0   2   1   0    2\n"
     ]
    }
   ],
   "source": [
    "#Easy to know $P_2$\n",
    "P2=pd.DataFrame([{'1': 1, '2': 0, '12': 1},{'1': 1, '2': 0, '12': 2}])\n",
    "\n",
    "n=2\n",
    "power_set_str,power_set_str_np1=get_n_np1_powersets(n)\n",
    "P3=get_Pnp1(P2,2)\n",
    "print(P3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "id": "225fc6c5-4aef-4ff5-bb16-709a1a1fea33",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_list_of_dicts = P3.to_dict(orient='records')\n",
    "# df_list_of_dicts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "a38b0b4f-481c-4546-b734-998f2231ff5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open('results\\P3_trans.pkl', 'wb') as file:\n",
    "#     pickle.dump(P3, file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf01c1ea-7b4a-4a0e-b40d-0a9c74e26399",
   "metadata": {
    "tags": [],
    "toc-hr-collapsed": true
   },
   "source": [
    "### P3 extending to P4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "f08f5756-b345-496f-bf4c-77b06af72e4e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of functions: 154\n",
      "Are there duplicates? (0, 15) if (0,-) then no common rows\n"
     ]
    }
   ],
   "source": [
    "n=3\n",
    "power_set_str,power_set_str_np1=get_n_np1_powersets(n)\n",
    "P4=get_Pnp1(P3,3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "4d63b409-402a-4eaf-8e4b-b688787d062c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# P4.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "74eb067e-7d04-4eea-bbbf-6d872f4f060e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open('results\\P4_trans.pkl', 'wb') as file:\n",
    "#     pickle.dump(P4, file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8799ffb-acf8-40d9-b0f5-8bf84d189f17",
   "metadata": {
    "tags": []
   },
   "source": [
    "### P4 extending to P5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "ef7438fb-e869-4bb9-90a4-845ba9e8b537",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of functions: 10334\n",
      "Are there duplicates? (0, 31) if (0,-) then no common rows\n"
     ]
    }
   ],
   "source": [
    "n=4\n",
    "power_set_str,power_set_str_np1=get_n_np1_powersets(n) #gives powerset on 4 and 5 elements respectively\n",
    "P5=get_Pnp1(P4,4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "da2ec9ea-2186-4e22-88e7-f5f7a89c0e88",
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open('results\\P5_trans.pkl', 'wb') as file:\n",
    "#     pickle.dump(P5, file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "140b70ac-d156-4ad8-9b95-d28921016c43",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Obtaining f3 from f2 in Pn <a name=\"s2\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cb003ce-bc3e-4194-8290-e41ca74556d6",
   "metadata": {},
   "source": [
    "Following the documentation we aim to get the list of f3 for a given f2 in Pn. \n",
    "\n",
    "We will store the results in \\results and then move to empty_poly_selector.ipynb to check if the polytopes are empty for a given f2,f3 pair."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56b7c195-b947-4bd4-a0ed-3e754f5e6a32",
   "metadata": {
    "tags": [],
    "toc-hr-collapsed": true
   },
   "source": [
    "## Extensions of f2 in P3 to ext(f2) in P4 <a name=\"s22\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9712713-e66b-4388-aed9-8d94a1e70f1d",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### Functions: to extend f2 <a name=\"s21\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "755b7792-8b87-4bd9-855f-9b17b748840d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For get_Pnp1_for_single_funct\n",
    "\n",
    "def extend_sets_to_Pnp1(P_n1):\n",
    "    #Return the data frame of Pn+1\n",
    "    \n",
    "    final_P_n1=[]\n",
    "    for ls in P_n1:\n",
    "        final_P_n1=final_P_n1+ls # Adding all the extension packets together.\n",
    "    df=pd.DataFrame(final_P_n1)\n",
    "    # df = df.reindex(columns=power_set_str_np1)\n",
    "    \n",
    "    return df\n",
    "\n",
    "def eg_column_Build_Pn_1(list_df_Pn,power_set_str,n):\n",
    "    \n",
    "    \"\"\"\n",
    "    Obj:Use Build_Pn_1 and modify change to include Eg size column in functions\n",
    "    \"\"\"\n",
    "    \n",
    "    # Returns:packets of extensions for each f_dict from Pn, can check those with max size of closed sets (later)\n",
    "    \n",
    "    P_n1=[]\n",
    "    for index,f_dict in enumerate(list_df_Pn):\n",
    "\n",
    "        # Store primitive closed sets \n",
    "        PC=[(x,get_barj({x},f_dict)) for x in power_set_str]\n",
    "        dict_PC=dict(PC)\n",
    "        set_PC=[sorted_frozenset(get_barj({x},f_dict)) for x in power_set_str]\n",
    "\n",
    "        #Get all closed sets for extensions for f_dict\n",
    "        Eg=list(get_Eg(dict_PC,power_set_str,set_PC))\n",
    "        Eg.append(\"empty\")\n",
    "        \n",
    "        Eg_col_kvpair={\"Eg_Size\":len(Eg)}\n",
    "\n",
    "        #Build extensions for f_dict\n",
    "        extension_of_f_dict=[]\n",
    "        for term in Eg:\n",
    "            if term ==\"empty\":                    \n",
    "                funct=empty_case_get_extension_of_f(f_dict,term,power_set_str,n) #dictionary\n",
    "                funct.update(Eg_col_kvpair)\n",
    "                extension_of_f_dict.append(funct)\n",
    "            else:\n",
    "                funct=get_extension_of_f(f_dict,term,power_set_str,n)\n",
    "                funct.update(Eg_col_kvpair)\n",
    "                extension_of_f_dict.append(funct)\n",
    "\n",
    "        #We record packets of extensions where we take +1\n",
    "        P_n1.append(extension_of_f_dict)\n",
    "    \n",
    "    return P_n1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "c21c5df5-7e82-4e6f-83dc-35cd7206633a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_Pnp1_for_single_funct(Pn,n):\n",
    "    \n",
    "    #inputs\n",
    "    # Pn:dataframe and n.\n",
    "    \n",
    "    \"\"\"\n",
    "    Returns a dataframe of extensions in Pn+1 for dataframe of functions in Pn.\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    #Example\n",
    "    n=3\n",
    "    power_set_str,power_set_str_np1=get_n_np1_powersets(n)\n",
    "    # P3\n",
    "\n",
    "    funct_list=[{'1': 0,'2': 0,\"3\":0,\"12\":0,\"23\":0,\"13\":0,\"123\":0}]\n",
    "    Pn=pd.DataFrame(funct_list)\n",
    "\n",
    "    #The following should be functions in P4 that are extendions of functions in funct_list\n",
    "    f_P3=get_Pnp1_for_single_funct(Pn,n)\n",
    "    print(f_P3)\n",
    "    \"\"\"\n",
    "\n",
    "    #New data created\n",
    "    power_set_str,power_set_str_np1=get_n_np1_powersets(n)\n",
    "\n",
    "    #Load Pn as a list of dictionaries.\n",
    "    list_df_Pn=Pn.to_dict(orient='records')\n",
    "\n",
    "    #Create Pn+1 and hold as dataframe.\n",
    "    P_n1=eg_column_Build_Pn_1(list_df_Pn,power_set_str,n)\n",
    "    #As previous was done in E(g) parts, joint together\n",
    "    Pnp1=extend_sets_to_Pnp1(P_n1)\n",
    "    \n",
    "        \n",
    "    # Pnp1=get_Eg_column_Pn(Pnp1)\n",
    "    # columns=list(power_set_str_np1)+[\"Eg_Size\"]\n",
    "    \n",
    "    columns=list(power_set_str_np1)\n",
    "\n",
    "    \n",
    "    Pnp1 = Pnp1.reindex(columns=columns)\n",
    "    \n",
    "    # inspect_Pn1(Pnp1)\n",
    "\n",
    "    return Pnp1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf34cc7c-738d-4d8e-ad92-3dabf929e5c3",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### Example for f2 in P3 to determine ext(f2) in Pn+1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4c3defb-f662-4927-81cb-c3d6f9781c33",
   "metadata": {},
   "source": [
    "Here construct for $f_{2} \\in P_{n}$ the set $\\rho^{-1}(f_{2})$ in order to determine the set of possible $f_{3}$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8574aff-1105-4598-89cd-5e3963b3ac94",
   "metadata": {},
   "source": [
    "Here we see get_Pnp1_for_single_funct in action for f2 in P3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "7da2ea42-e47b-44cb-8bfb-9f41ff77f7af",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The function we are extending\n",
      "   1  2  3  12  23  13  123\n",
      "0  0  0  0   0   0   0    0\n",
      "    1  2  3  4  12  13  14  23  24  34  123  124  134  234  1234\n",
      "0   0  0  0  0   0   0   0   0   0   0    0    1    0    1     1\n",
      "1   0  0  0  0   0   0   1   0   1   1    0    1    1    1     1\n",
      "2   0  0  0  0   0   0   1   0   0   1    0    1    1    1     1\n",
      "3   0  0  0  0   0   0   1   0   0   0    0    1    1    1     1\n",
      "4   0  0  0  0   0   0   0   0   0   0    0    0    1    0     1\n",
      "5   0  0  0  0   0   0   0   0   0   0    0    0    0    0     1\n",
      "6   0  0  0  0   0   0   0   0   0   0    0    0    0    1     1\n",
      "7   0  0  0  0   0   0   0   0   1   0    0    1    1    1     1\n",
      "8   0  0  0  0   0   0   0   0   1   1    0    1    1    1     1\n",
      "9   0  0  0  0   0   0   0   0   0   1    0    0    1    1     1\n",
      "10  0  0  0  0   0   0   0   0   1   0    0    1    0    1     1\n",
      "11  0  0  0  0   0   0   1   0   0   0    0    1    1    0     1\n",
      "12  0  0  0  0   0   0   0   0   0   0    0    0    1    1     1\n",
      "13  0  0  0  0   0   0   0   0   0   0    0    1    1    0     1\n",
      "14  0  0  0  0   0   0   0   0   0   0    0    1    1    1     1\n",
      "15  0  0  0  0   0   0   1   0   1   0    0    1    1    1     1\n",
      "16  0  0  0  0   0   0   0   0   0   1    0    1    1    1     1\n",
      "17  0  0  0  0   0   0   0   0   0   0    0    1    0    0     1\n",
      "18  0  0  0  0   0   0   0   0   0   0    0    0    0    0     0\n"
     ]
    }
   ],
   "source": [
    "#Testing functions example\n",
    "n=3\n",
    "power_set_str,power_set_str_np1=get_n_np1_powersets(n)\n",
    "\n",
    "# You can take subsets of rows as a database, then extend. Better than below\n",
    "funct_list=[{'1': 0,'2': 0,\"3\":0,\"12\":0,\"23\":0,\"13\":0,\"123\":0}]\n",
    "Pn=pd.DataFrame(funct_list)\n",
    "print(\"The function we are extending\")\n",
    "print(Pn)\n",
    "\n",
    "#The following should be functions in P4 that are extendions of functions in funct_list\n",
    "ext_f2=get_Pnp1_for_single_funct(Pn,n)\n",
    "print(ext_f2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13451048-c319-427a-a4ee-b74d7f9ae66b",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Check which ext_f2 are f_3 format."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5b0b85e-c135-49dc-86de-87178d041c8c",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Functions to obtain f3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "0180d8ad-d497-4676-bbbd-7f1a16abb633",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Check that trans_pre_f3 satistfies the symmetry condition.\n",
    "def get_comp(k,max_set):#for symmetry_con_check\n",
    "    #get complement \n",
    "    # number_string = \"12345\"\n",
    "    # y = \"134\"\n",
    "    z = \"\".join(char for char in max_set if char not in k)\n",
    "    return z #\"25\"\n",
    "\n",
    "\n",
    "#translating by empty set value and then adding the emptyset value.\n",
    "def add_empty_and_translate(pre_f3,max_set,n):\n",
    "    \n",
    "    \"\"\"\n",
    "    Obj: Given the extension of a f2 in Pn we need to prepare it in order to check the sym condition.\n",
    "    \n",
    "    1) we find the empty set value\n",
    "    2)we translate all terms by it\n",
    "    3)we remove all terms with \"n+1\" from the function to get a f3 in powerset of n\n",
    "    \"\"\"\n",
    "\n",
    "    #determine value of empty set\n",
    "    \n",
    "    m_val=pre_f3[max_set]\n",
    "    \n",
    "    # print(max_set)\n",
    "    \n",
    "    ceiling_result = math.ceil(m_val/2)\n",
    "    empt_value=  -1-ceiling_result\n",
    "    \n",
    "    \n",
    "    #Translate all terms\n",
    "    trans_pre_f3_done = {key: value + empt_value for key, value in pre_f3.items()}\n",
    "\n",
    "    #Add empty set value\n",
    "    trans_pre_f3_done[\"empty\"]=empt_value\n",
    "    \n",
    "    #REstrict : Remove key-value pairs where the key contains \"n+1\"\n",
    "    np1=str(n+1)\n",
    "    filtered_dict = {key: value for key, value in trans_pre_f3_done.items() if np1  in key}\n",
    "    \n",
    "    #Remove the term \"n+1\" from the keys\n",
    "    \n",
    "    prepped_f3_pos = {key.replace(np1, ''): value for key, value in filtered_dict.items()}\n",
    "\n",
    "    del prepped_f3_pos[\"\"]\n",
    "    \n",
    "    #Add empty set value\n",
    "    prepped_f3_pos[\"empty\"]=empt_value\n",
    "\n",
    "    return prepped_f3_pos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "7a6a5260-edae-491f-82c1-f2e804f3a510",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Main \n",
    "\n",
    "def get_f3_l_from_f2(f2,n):#main\n",
    "    \n",
    "    \"\"\"\n",
    "    in:\n",
    "    f2 in Pn i.e. form {'1': 0,'2': 0,\"3\":0,\"12\":0,\"23\":0,\"13\":0,\"123\":0}\n",
    "    n: for f2 in Pn\n",
    "    \n",
    "    out: list of f3 i.e [f31,f32] where each f3 is of the form {'empty':6,'1': 0,'2': 0,\"3\":0,\"12\":0,\"23\":0,\"13\":0,\"123\":0} \\in Pn\n",
    "    \n",
    "    Obj:return the list of f3 (satisfying the conditions) from a f2\n",
    "    \"\"\"\n",
    "    \n",
    "    #init\n",
    "\n",
    "    #get basis extensions of f2 (which we need to change into true f3).\n",
    "    \n",
    "    power_set_str,power_set_str_np1=get_n_np1_powersets(n)\n",
    "    funct_list=[f2]\n",
    "    df_f2=pd.DataFrame(funct_list)\n",
    "    \n",
    "    # functions in Pn=1 that are extendions of functions in funct_list\n",
    "    ext_f2=get_Pnp1_for_single_funct(df_f2,n)\n",
    "    \n",
    "    #list of pre_f3 to which we need to check conditions to be a f3\n",
    "    pre_f3_l= ext_f2.to_dict(orient='records')\n",
    "    \n",
    "    #get the key of the largest term key of f2 \"1...n\"\n",
    "    number_list_f2 = [str(i) for i in range(1, n + 1)] #if n=3 want 123 so can tail ceiling nad det f3(empty) value\n",
    "    max_set_f2  = \"\".join(number_list_f2)\n",
    "    \n",
    "    # number_list_f3 = [str(i) for i in range(1, n + 1)] #if n=3 want 123 so can do condition with empty\n",
    "    # max_set_f3  = \"\".join(number_list_f3)\n",
    "    \n",
    "    # print(f\"max_set_f3{max_set_f3},max_set_f2{max_set_f2}\")\n",
    "    \n",
    "    # print(\"max_set||||||||\",max_set)\n",
    "\n",
    "    f3_l=[] #the set of pre_f3 that have been translated by the empty, inc the empty value and sats the symm condition.\n",
    "                  \n",
    "    for pre_f3 in pre_f3_l:##!!\n",
    "\n",
    "        #translate by empty value and add empty\n",
    "        trans_pre_f3=add_empty_and_translate(pre_f3,max_set_f2,n)\n",
    "        \n",
    "        # print(trans_pre_f3)\n",
    "        \n",
    "        #check sym cond\n",
    "        f3=symmetry_con_check(trans_pre_f3,max_set_f2)\n",
    "\n",
    "        #discard those that fail symm\n",
    "        if f3==None:\n",
    "            continue\n",
    "        else:    \n",
    "            f3_l.append(f3)\n",
    "            \n",
    "    # print(\"f2 done\")\n",
    "    return f3_l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "23834253-d101-4c04-a5ab-3129376da7bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Collate f2_f3_data to send to poly_size_checker Set so runs through all Pn.\n",
    "def get_f2f3_for_df(path,n): #for a given df of f2 msa functions\n",
    "\n",
    "    \"\"\"\n",
    "    In:\n",
    "    #init\n",
    "    # path='results\\subset_P5_w_degenRf.pkl'\n",
    "    # n=5 int of Pn\n",
    "\n",
    "    out: list of [f2,f3_l]\n",
    "\n",
    "    Obj:for any df of msa functions take the extensions of those to get f3 and then save a list of outputs.\n",
    "    #pack data to list to send to poly size checker\n",
    "    # get a list of lists such that item is [f2,[f31,f32,...]]\n",
    "\n",
    "    Want to generate to get polytope so can check dimension.\n",
    "    \"\"\"\n",
    "\n",
    "    #load \n",
    "    with open(path, 'rb') as file:\n",
    "        loaded_df = pickle.load(file)\n",
    "    # loaded_df.head()\n",
    "\n",
    "    df_list_of_dicts = loaded_df.to_dict(orient='records')\n",
    "\n",
    "    # #Building columns for Pn+1\n",
    "    # power_set_str,power_set_str_np1=get_n_np1_powersets(n)\n",
    "\n",
    "    # get f3 for f2 in df\n",
    "    \n",
    "    df_f2f3_l=[]\n",
    "    for index,f2 in enumerate(df_list_of_dicts):\n",
    "        print(index+1) #progress tracker\n",
    "        f3_l=get_f3_l_from_f2(f2,n)\n",
    "        \n",
    "        #We may have duplicates.\n",
    "    \n",
    "        # Using a loop to remove duplicates\n",
    "        unique_dicts = []\n",
    "        seen_dicts = set()\n",
    "\n",
    "        for dct in f3_l:\n",
    "            # Convert the dictionary to a tuple of key-value pairs for hashing\n",
    "            dct_tuple = tuple(sorted(dct.items()))\n",
    "\n",
    "            if dct_tuple not in seen_dicts:\n",
    "                seen_dicts.add(dct_tuple)\n",
    "                unique_dicts.append(dct)\n",
    "\n",
    "        f3_l_unique_dicts=unique_dicts\n",
    "        \n",
    "        df_f2f3_l.append([f2,f3_l_unique_dicts])\n",
    "    \n",
    "    return df_f2f3_l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "bc38d624-3dbd-4907-91f5-2b7c50eb0279",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def symmetry_con_check(trans_pre_f3,max_set):#for get_f3_l_from_f2\n",
    "    \n",
    "    #starting conditions\n",
    "    \n",
    "    # print(\"max_set\",max_set)\n",
    "    \n",
    "    # print(\"trans_pre_f3:\",trans_pre_f3)\n",
    "    \n",
    "    #Now check that sym condition satisfied, check false if not.\n",
    "\n",
    "    # print(trans_pre_f3)\n",
    "    rl=[\"empty\",max_set]\n",
    "    \n",
    "    terms = [item for item in list(trans_pre_f3.keys()) if item not in rl]\n",
    "    \n",
    "    for k in terms:\n",
    "\n",
    "        # if k==\"empty\":\n",
    "        #     if trans_pre_f3[max_set]!= -2-trans_pre_f3[\"empty\"]:\n",
    "        #         # print(\"None\")\n",
    "        #         return None\n",
    "        #     continue\n",
    "        # if k== max_set:\n",
    "        #     # print(k)\n",
    "        #     if trans_pre_f3[\"empty\"]!= -2-trans_pre_f3[max_set]:\n",
    "        #         # print(\"None\")\n",
    "        #         return None\n",
    "        #     continue\n",
    "            \n",
    "        #get complemnt to k        \n",
    "        k_comp=get_comp(k,max_set)\n",
    "        \n",
    "        if trans_pre_f3[k_comp]+trans_pre_f3[k]!= -2:\n",
    "#             print(f\"trans_pre_f3[k_comp]+trans_pre_f3[k]={trans_pre_f3[k_comp]}+{trans_pre_f3[k]}={-2}?\")\n",
    "#             print(f\"Breaks here:  k_comp:{k_comp},k :{k} \\n\")\n",
    "            return None\n",
    "\n",
    "    f3=trans_pre_f3\n",
    "    return f3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "579b6eff-589f-4ab9-a21f-c869d29acc22",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Example get f3's for each f2 in P3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "6ad964e7-07e6-4fc6-ab1d-a681e2770aca",
   "metadata": {},
   "outputs": [],
   "source": [
    "path=\"results\\P3.pkl\"\n",
    "n=3\n",
    "power_set_str,power_set_str_np1=get_n_np1_powersets(n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "2c0ad9c2-f1cd-4433-bf83-9243892f9a98",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n"
     ]
    }
   ],
   "source": [
    "#this is all for P3\n",
    "example_f2_f3_l=get_f2f3_for_df(path,n)\n",
    "# #number of f3s per f2.\n",
    "# for i in range(10):\n",
    "#     print(len(example_f2_f3_l[i][1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "3fd4a1b5-b038-4ad0-95bd-744b24617cde",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Save this\n",
    "# with open('results\\P3_f2_f3l.pkl', 'wb') as file:\n",
    "#     pickle.dump(example_f2_f3_l, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "17de7302-a614-4b85-b059-8b3cce79f7ed",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "27"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "total=0\n",
    "for i in range(len(example_f2_f3_l)):\n",
    "    # print(len(example_f2_f3_l[i][1]))\n",
    "    \n",
    "    total+=len(example_f2_f3_l[i][1])\n",
    "        \n",
    "    # if len(example_f2_f3_l[i][1])==0:\n",
    "    #     print(example_f2_f3_l[i][0],\"\\n\",example_f2_f3_l[i][1],\"\\n\\n\")\n",
    "print(f\"total:{total}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "aa160943-d021-4e01-a695-dffaea890964",
   "metadata": {},
   "outputs": [],
   "source": [
    "#outputing examples to text file.\n",
    "data = example_f2_f3_l\n",
    "\n",
    "# Open a text file in write mode\n",
    "with open('P3_f2_f3l.txt', 'w') as file:\n",
    "    file.write(\"Given f2 in P3 we record for each f2 the set of f3\\n\\n\")\n",
    "\n",
    "    for i, sub_list in enumerate(data):\n",
    "        f2 = sub_list[0]\n",
    "        file.write(f'f2_{i+1} = {f2}\\n')\n",
    "\n",
    "        f3_list = sub_list[1]\n",
    "        for j, f3 in enumerate(f3_list):\n",
    "            file.write(f'f3{i+1}{j+1} = {f3}\\n')\n",
    "        file.write(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "id": "b1080238-b364-4ff1-b26d-5a77adebe625",
   "metadata": {},
   "outputs": [],
   "source": [
    "# #Check with marco's example\n",
    "\n",
    "# data = [\n",
    "#     #f2\n",
    "#     {'1': 0, '2': 0, '3': 0, '12': 1, '13': 1, '23': 0, '123': 1},\n",
    "#     [\n",
    "#         #f3\n",
    "#         {'1': -1, '2': -2, '3': -1, '12': -1, '13': 0, '23': -1, '123': 0, 'empty': -2},\n",
    "\n",
    "#         {'1': -1, '2': -1, '3': -2, '12': 0, '13': -1, '23': -1, '123': 0, 'empty': -2}\n",
    "        \n",
    "#         {'1': -1, '2': -1, '3': -1, '12': -1, '13': -1, '23': -1, '123': 0, 'empty': -2},\n",
    "#     ]\n",
    "# ]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "660bcd61-92d9-4979-b531-767adc746b1d",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Example get f3's for each f2 in P4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "f575a365-f401-49d3-b510-2353d96c7708",
   "metadata": {},
   "outputs": [],
   "source": [
    "path=\"results\\P4.pkl\"\n",
    "n=4\n",
    "power_set_str,power_set_str_np1=get_n_np1_powersets(n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "2989a93e-df77-4381-a843-016c431c5344",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "25\n",
      "26\n",
      "27\n",
      "28\n",
      "29\n",
      "30\n",
      "31\n",
      "32\n",
      "33\n",
      "34\n",
      "35\n",
      "36\n",
      "37\n",
      "38\n",
      "39\n",
      "40\n",
      "41\n",
      "42\n",
      "43\n",
      "44\n",
      "45\n",
      "46\n",
      "47\n",
      "48\n",
      "49\n",
      "50\n",
      "51\n",
      "52\n",
      "53\n",
      "54\n",
      "55\n",
      "56\n",
      "57\n",
      "58\n",
      "59\n",
      "60\n",
      "61\n",
      "62\n",
      "63\n",
      "64\n",
      "65\n",
      "66\n",
      "67\n",
      "68\n",
      "69\n",
      "70\n",
      "71\n",
      "72\n",
      "73\n",
      "74\n",
      "75\n",
      "76\n",
      "77\n",
      "78\n",
      "79\n",
      "80\n",
      "81\n",
      "82\n",
      "83\n",
      "84\n",
      "85\n",
      "86\n",
      "87\n",
      "88\n",
      "89\n",
      "90\n",
      "91\n",
      "92\n",
      "93\n",
      "94\n",
      "95\n",
      "96\n",
      "97\n",
      "98\n",
      "99\n",
      "100\n",
      "101\n",
      "102\n",
      "103\n",
      "104\n",
      "105\n",
      "106\n",
      "107\n",
      "108\n",
      "109\n",
      "110\n",
      "111\n",
      "112\n",
      "113\n",
      "114\n",
      "115\n",
      "116\n",
      "117\n",
      "118\n",
      "119\n",
      "120\n",
      "121\n",
      "122\n",
      "123\n",
      "124\n",
      "125\n",
      "126\n",
      "127\n",
      "128\n",
      "129\n",
      "130\n",
      "131\n",
      "132\n",
      "133\n",
      "134\n",
      "135\n",
      "136\n",
      "137\n",
      "138\n",
      "139\n",
      "140\n",
      "141\n",
      "142\n",
      "143\n",
      "144\n",
      "145\n",
      "146\n",
      "147\n",
      "148\n",
      "149\n",
      "150\n",
      "151\n",
      "152\n",
      "153\n",
      "154\n"
     ]
    }
   ],
   "source": [
    "#this is all for P4\n",
    "example_f2_f3_l=get_f2f3_for_df(path,n)\n",
    "\n",
    "# #number of f3s per f2.\n",
    "# for i in range(154):\n",
    "#     print(len(example_f2_f3_l[i][1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "ea150493-1337-4b77-9496-84e0d902e7d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Save this\n",
    "with open('results\\P4_f2_f3l.pkl', 'wb') as file:\n",
    "    pickle.dump(example_f2_f3_l, file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47074942-d5c1-4210-b035-aaa2676b40da",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### Example: get f3's for each f2 in P5 for f2 degenerate poly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce4c49c6-e797-4fd2-93c8-613193b02525",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load df from results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "f8886b8e-16ee-4c9a-87b6-237203c16290",
   "metadata": {},
   "outputs": [],
   "source": [
    "path='results\\subset_P5_w_degenRf.pkl'\n",
    "n=5\n",
    "power_set_str,power_set_str_np1=get_n_np1_powersets(n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "dc4d90a6-cd17-4265-bb56-3562a1e5e090",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "25\n",
      "26\n",
      "27\n",
      "28\n",
      "29\n",
      "30\n",
      "31\n",
      "32\n",
      "33\n",
      "34\n",
      "35\n",
      "36\n",
      "37\n",
      "38\n",
      "39\n",
      "40\n",
      "41\n",
      "42\n",
      "43\n",
      "44\n",
      "45\n",
      "46\n",
      "47\n",
      "48\n",
      "49\n",
      "50\n",
      "51\n",
      "52\n",
      "53\n",
      "54\n",
      "55\n",
      "56\n",
      "57\n",
      "58\n",
      "59\n",
      "60\n",
      "61\n",
      "62\n",
      "63\n",
      "64\n",
      "65\n",
      "66\n",
      "67\n",
      "68\n",
      "69\n",
      "70\n",
      "71\n",
      "72\n",
      "73\n",
      "74\n",
      "75\n",
      "76\n",
      "77\n",
      "78\n",
      "79\n",
      "80\n",
      "81\n",
      "82\n",
      "83\n",
      "84\n",
      "85\n",
      "86\n",
      "87\n",
      "88\n",
      "89\n",
      "90\n",
      "91\n",
      "92\n",
      "93\n",
      "94\n",
      "95\n",
      "96\n",
      "97\n",
      "98\n",
      "99\n",
      "100\n",
      "101\n",
      "102\n",
      "103\n",
      "104\n",
      "105\n",
      "106\n",
      "107\n",
      "108\n",
      "109\n",
      "110\n",
      "111\n",
      "112\n",
      "113\n",
      "114\n",
      "115\n",
      "116\n",
      "117\n",
      "118\n",
      "119\n",
      "120\n",
      "121\n",
      "122\n",
      "123\n",
      "124\n",
      "125\n",
      "126\n",
      "127\n",
      "128\n",
      "129\n",
      "130\n",
      "131\n",
      "132\n",
      "133\n",
      "134\n",
      "135\n",
      "136\n",
      "137\n",
      "138\n",
      "139\n",
      "140\n",
      "141\n",
      "142\n",
      "143\n",
      "144\n",
      "145\n",
      "146\n",
      "147\n",
      "148\n",
      "149\n",
      "150\n",
      "151\n",
      "152\n",
      "153\n",
      "154\n",
      "155\n",
      "156\n",
      "157\n",
      "158\n",
      "159\n",
      "160\n",
      "161\n",
      "162\n",
      "163\n",
      "164\n",
      "165\n",
      "166\n",
      "167\n",
      "168\n",
      "169\n",
      "170\n",
      "171\n",
      "172\n",
      "173\n",
      "174\n",
      "175\n",
      "176\n",
      "177\n",
      "178\n",
      "179\n",
      "180\n",
      "181\n",
      "182\n",
      "183\n",
      "184\n",
      "185\n",
      "186\n",
      "187\n",
      "188\n",
      "189\n",
      "190\n",
      "191\n",
      "192\n",
      "193\n",
      "194\n",
      "195\n",
      "196\n",
      "197\n",
      "198\n",
      "199\n",
      "200\n",
      "201\n",
      "202\n",
      "203\n",
      "204\n",
      "205\n",
      "206\n",
      "207\n",
      "208\n",
      "209\n",
      "210\n",
      "211\n",
      "212\n",
      "213\n",
      "214\n",
      "215\n",
      "216\n",
      "217\n",
      "218\n",
      "219\n",
      "220\n",
      "221\n",
      "222\n",
      "223\n",
      "224\n",
      "225\n",
      "226\n",
      "227\n",
      "228\n",
      "229\n",
      "230\n",
      "231\n",
      "232\n",
      "233\n",
      "234\n",
      "235\n",
      "236\n",
      "237\n",
      "238\n",
      "239\n",
      "240\n",
      "241\n",
      "242\n",
      "243\n",
      "244\n",
      "245\n",
      "246\n",
      "247\n",
      "248\n",
      "249\n",
      "250\n",
      "251\n",
      "252\n",
      "253\n",
      "254\n",
      "255\n",
      "256\n",
      "257\n",
      "258\n",
      "259\n",
      "260\n",
      "261\n",
      "262\n",
      "263\n",
      "264\n",
      "265\n",
      "266\n",
      "267\n",
      "268\n",
      "269\n",
      "270\n",
      "271\n",
      "272\n",
      "273\n",
      "274\n",
      "275\n",
      "276\n",
      "277\n",
      "278\n",
      "279\n",
      "280\n",
      "281\n",
      "282\n",
      "283\n",
      "284\n",
      "285\n",
      "286\n",
      "287\n",
      "288\n",
      "289\n",
      "290\n",
      "291\n",
      "292\n",
      "293\n",
      "294\n",
      "295\n",
      "296\n",
      "297\n",
      "298\n",
      "299\n",
      "300\n",
      "301\n",
      "302\n",
      "303\n",
      "304\n",
      "305\n",
      "306\n",
      "307\n",
      "308\n",
      "309\n",
      "310\n",
      "311\n",
      "312\n",
      "313\n",
      "314\n",
      "315\n",
      "316\n",
      "317\n",
      "318\n",
      "319\n",
      "320\n",
      "321\n",
      "322\n",
      "323\n",
      "324\n",
      "325\n",
      "326\n",
      "327\n",
      "328\n",
      "329\n",
      "330\n",
      "331\n",
      "332\n",
      "333\n",
      "334\n",
      "335\n",
      "336\n",
      "337\n",
      "338\n",
      "339\n",
      "340\n",
      "341\n",
      "342\n",
      "343\n",
      "344\n",
      "345\n",
      "346\n",
      "347\n",
      "348\n",
      "349\n",
      "350\n",
      "351\n",
      "352\n",
      "353\n",
      "354\n",
      "355\n",
      "356\n",
      "357\n",
      "358\n",
      "359\n",
      "360\n",
      "361\n",
      "362\n",
      "363\n",
      "364\n",
      "365\n",
      "366\n",
      "367\n",
      "368\n",
      "369\n",
      "370\n",
      "371\n",
      "372\n",
      "373\n",
      "374\n",
      "375\n",
      "376\n",
      "377\n",
      "378\n",
      "379\n",
      "380\n",
      "381\n",
      "382\n",
      "383\n",
      "384\n",
      "385\n",
      "386\n",
      "387\n",
      "388\n",
      "389\n",
      "390\n",
      "391\n",
      "392\n",
      "393\n",
      "394\n",
      "395\n",
      "396\n",
      "397\n",
      "398\n",
      "399\n",
      "400\n",
      "401\n",
      "402\n",
      "403\n",
      "404\n",
      "405\n",
      "406\n",
      "407\n",
      "408\n",
      "409\n",
      "410\n",
      "411\n",
      "412\n",
      "413\n",
      "414\n",
      "415\n",
      "416\n",
      "417\n",
      "418\n",
      "419\n",
      "420\n",
      "421\n",
      "422\n",
      "423\n",
      "424\n",
      "425\n",
      "426\n",
      "427\n",
      "428\n",
      "429\n",
      "430\n",
      "431\n",
      "432\n",
      "433\n",
      "434\n",
      "435\n",
      "436\n",
      "437\n",
      "438\n",
      "439\n",
      "440\n",
      "441\n",
      "442\n",
      "443\n",
      "444\n",
      "445\n",
      "446\n",
      "447\n",
      "448\n",
      "449\n",
      "450\n",
      "451\n",
      "452\n",
      "453\n",
      "454\n",
      "455\n",
      "456\n",
      "457\n",
      "458\n",
      "459\n",
      "460\n",
      "461\n",
      "462\n",
      "463\n",
      "464\n",
      "465\n",
      "466\n",
      "467\n",
      "468\n",
      "469\n",
      "470\n",
      "471\n",
      "472\n",
      "473\n",
      "474\n",
      "475\n",
      "476\n",
      "477\n",
      "478\n",
      "479\n",
      "480\n",
      "481\n",
      "482\n",
      "483\n",
      "484\n",
      "485\n",
      "486\n",
      "487\n",
      "488\n",
      "489\n",
      "490\n",
      "491\n",
      "492\n",
      "493\n",
      "494\n",
      "495\n",
      "496\n",
      "497\n",
      "498\n",
      "499\n",
      "500\n",
      "501\n",
      "502\n",
      "503\n",
      "504\n",
      "505\n",
      "506\n",
      "507\n",
      "508\n",
      "509\n",
      "510\n",
      "511\n",
      "512\n",
      "513\n",
      "514\n",
      "515\n",
      "516\n",
      "517\n",
      "518\n",
      "519\n",
      "520\n",
      "521\n",
      "522\n",
      "523\n",
      "524\n",
      "525\n",
      "526\n",
      "527\n",
      "528\n",
      "529\n",
      "530\n",
      "531\n",
      "532\n",
      "533\n",
      "534\n",
      "535\n",
      "536\n",
      "537\n",
      "538\n",
      "539\n",
      "540\n",
      "541\n",
      "542\n",
      "543\n",
      "544\n",
      "545\n",
      "546\n",
      "547\n",
      "548\n",
      "549\n",
      "550\n",
      "551\n",
      "552\n",
      "553\n",
      "554\n",
      "555\n",
      "556\n",
      "557\n",
      "558\n",
      "559\n",
      "560\n",
      "561\n",
      "562\n",
      "563\n",
      "564\n",
      "565\n",
      "566\n",
      "567\n",
      "568\n",
      "569\n",
      "570\n",
      "571\n",
      "572\n",
      "573\n",
      "574\n",
      "575\n",
      "576\n",
      "577\n",
      "578\n",
      "579\n",
      "580\n",
      "581\n",
      "582\n",
      "583\n",
      "584\n",
      "585\n",
      "586\n",
      "587\n",
      "588\n",
      "589\n",
      "590\n",
      "591\n",
      "592\n",
      "593\n",
      "594\n",
      "595\n",
      "596\n",
      "597\n",
      "598\n",
      "599\n",
      "600\n",
      "601\n",
      "602\n",
      "603\n",
      "604\n",
      "605\n",
      "606\n",
      "607\n",
      "608\n",
      "609\n",
      "610\n",
      "611\n",
      "612\n",
      "613\n",
      "614\n",
      "615\n",
      "616\n",
      "617\n",
      "618\n",
      "619\n",
      "620\n",
      "621\n",
      "622\n",
      "623\n",
      "624\n",
      "625\n",
      "626\n",
      "627\n",
      "628\n",
      "629\n",
      "630\n",
      "631\n",
      "632\n",
      "633\n",
      "634\n",
      "635\n",
      "636\n",
      "637\n",
      "638\n",
      "639\n",
      "640\n",
      "641\n",
      "642\n",
      "643\n",
      "644\n",
      "645\n",
      "646\n",
      "647\n",
      "648\n",
      "649\n",
      "650\n",
      "651\n",
      "652\n",
      "653\n",
      "654\n",
      "655\n",
      "656\n",
      "657\n",
      "658\n",
      "659\n",
      "660\n",
      "661\n",
      "662\n",
      "663\n",
      "664\n",
      "665\n",
      "666\n",
      "667\n",
      "668\n",
      "669\n",
      "670\n",
      "671\n",
      "672\n",
      "673\n",
      "674\n",
      "675\n",
      "676\n",
      "677\n",
      "678\n",
      "679\n",
      "680\n",
      "681\n",
      "682\n",
      "683\n",
      "684\n",
      "685\n",
      "686\n",
      "687\n",
      "688\n",
      "689\n",
      "690\n",
      "691\n",
      "692\n",
      "693\n",
      "694\n",
      "695\n",
      "696\n",
      "697\n",
      "698\n",
      "699\n",
      "700\n",
      "701\n",
      "702\n",
      "703\n",
      "704\n",
      "705\n",
      "706\n",
      "707\n",
      "708\n",
      "709\n",
      "710\n",
      "711\n",
      "712\n",
      "713\n",
      "714\n",
      "715\n",
      "716\n",
      "717\n",
      "718\n",
      "719\n",
      "720\n",
      "721\n",
      "722\n",
      "723\n",
      "724\n",
      "725\n",
      "726\n",
      "727\n",
      "728\n",
      "729\n",
      "730\n",
      "731\n",
      "732\n",
      "733\n",
      "734\n",
      "735\n",
      "736\n",
      "737\n",
      "738\n",
      "739\n",
      "740\n",
      "741\n",
      "742\n",
      "743\n",
      "744\n",
      "745\n",
      "746\n",
      "747\n",
      "748\n",
      "749\n",
      "750\n",
      "751\n",
      "752\n",
      "753\n",
      "754\n",
      "755\n",
      "756\n",
      "757\n",
      "758\n",
      "759\n",
      "760\n",
      "761\n",
      "762\n",
      "763\n",
      "764\n",
      "765\n",
      "766\n",
      "767\n",
      "768\n",
      "769\n",
      "770\n",
      "771\n",
      "772\n",
      "773\n",
      "774\n",
      "775\n",
      "776\n",
      "777\n",
      "778\n",
      "779\n",
      "780\n",
      "781\n",
      "782\n",
      "783\n",
      "784\n",
      "785\n",
      "786\n",
      "787\n",
      "788\n",
      "789\n",
      "790\n",
      "791\n",
      "792\n",
      "793\n",
      "794\n",
      "795\n",
      "796\n",
      "797\n",
      "798\n",
      "799\n",
      "800\n",
      "801\n",
      "802\n",
      "803\n",
      "804\n",
      "805\n",
      "806\n",
      "807\n",
      "808\n",
      "809\n",
      "810\n",
      "811\n",
      "812\n",
      "813\n",
      "814\n",
      "815\n",
      "816\n",
      "817\n",
      "818\n",
      "819\n",
      "820\n",
      "821\n",
      "822\n",
      "823\n",
      "824\n",
      "825\n",
      "826\n",
      "827\n",
      "828\n",
      "829\n",
      "830\n",
      "831\n",
      "832\n",
      "833\n",
      "834\n",
      "835\n",
      "836\n",
      "837\n",
      "838\n",
      "839\n",
      "840\n",
      "841\n",
      "842\n",
      "843\n",
      "844\n",
      "845\n",
      "846\n",
      "847\n",
      "848\n",
      "849\n",
      "850\n",
      "851\n",
      "852\n",
      "853\n",
      "854\n",
      "855\n",
      "856\n",
      "857\n",
      "858\n",
      "859\n",
      "860\n",
      "861\n",
      "862\n",
      "863\n",
      "864\n",
      "865\n",
      "866\n",
      "867\n",
      "868\n",
      "869\n",
      "870\n",
      "871\n",
      "872\n",
      "873\n",
      "874\n",
      "875\n",
      "876\n",
      "877\n",
      "878\n",
      "879\n",
      "880\n",
      "881\n",
      "882\n",
      "883\n",
      "884\n",
      "885\n",
      "886\n",
      "887\n",
      "888\n",
      "889\n",
      "890\n",
      "891\n",
      "892\n",
      "893\n",
      "894\n",
      "895\n",
      "896\n",
      "897\n",
      "898\n",
      "899\n",
      "900\n",
      "901\n",
      "902\n",
      "903\n",
      "904\n",
      "905\n",
      "906\n",
      "907\n",
      "908\n",
      "909\n",
      "910\n",
      "911\n",
      "912\n",
      "913\n",
      "914\n",
      "915\n",
      "916\n",
      "917\n",
      "918\n",
      "919\n",
      "920\n",
      "921\n",
      "922\n",
      "923\n",
      "924\n",
      "925\n",
      "926\n",
      "927\n",
      "928\n",
      "929\n",
      "930\n",
      "931\n",
      "932\n",
      "933\n",
      "934\n",
      "935\n",
      "936\n",
      "937\n",
      "938\n",
      "939\n",
      "940\n",
      "941\n",
      "942\n",
      "943\n",
      "944\n",
      "945\n",
      "946\n",
      "947\n",
      "948\n",
      "949\n",
      "950\n",
      "951\n",
      "952\n",
      "953\n",
      "954\n",
      "955\n",
      "956\n",
      "957\n",
      "958\n",
      "959\n",
      "960\n",
      "961\n",
      "962\n",
      "963\n",
      "964\n",
      "965\n",
      "966\n",
      "967\n",
      "968\n",
      "969\n",
      "970\n",
      "971\n",
      "972\n",
      "973\n",
      "974\n",
      "975\n",
      "976\n",
      "977\n",
      "978\n",
      "979\n",
      "980\n",
      "981\n",
      "982\n",
      "983\n",
      "984\n",
      "985\n",
      "986\n",
      "987\n",
      "988\n",
      "989\n",
      "990\n",
      "991\n",
      "992\n",
      "993\n",
      "994\n",
      "995\n",
      "996\n",
      "997\n",
      "998\n",
      "999\n",
      "1000\n",
      "1001\n",
      "1002\n",
      "1003\n",
      "1004\n",
      "1005\n",
      "1006\n",
      "1007\n",
      "1008\n",
      "1009\n",
      "1010\n",
      "1011\n",
      "1012\n",
      "1013\n",
      "1014\n",
      "1015\n",
      "1016\n",
      "1017\n",
      "1018\n",
      "1019\n",
      "1020\n",
      "1021\n",
      "1022\n",
      "1023\n",
      "1024\n",
      "1025\n",
      "1026\n",
      "1027\n",
      "1028\n",
      "1029\n",
      "1030\n",
      "1031\n",
      "1032\n",
      "1033\n",
      "1034\n",
      "1035\n",
      "1036\n",
      "1037\n",
      "1038\n",
      "1039\n",
      "1040\n",
      "1041\n",
      "1042\n",
      "1043\n",
      "1044\n",
      "1045\n",
      "1046\n",
      "1047\n",
      "1048\n",
      "1049\n",
      "1050\n",
      "1051\n",
      "1052\n",
      "1053\n",
      "1054\n",
      "1055\n",
      "1056\n",
      "1057\n",
      "1058\n",
      "1059\n",
      "1060\n",
      "1061\n",
      "1062\n",
      "1063\n",
      "1064\n",
      "1065\n",
      "1066\n",
      "1067\n",
      "1068\n",
      "1069\n",
      "1070\n",
      "1071\n",
      "1072\n",
      "1073\n",
      "1074\n",
      "1075\n",
      "1076\n",
      "1077\n",
      "1078\n",
      "1079\n",
      "1080\n",
      "1081\n",
      "1082\n",
      "1083\n",
      "1084\n",
      "1085\n",
      "1086\n",
      "1087\n",
      "1088\n",
      "1089\n",
      "1090\n",
      "1091\n",
      "1092\n",
      "1093\n",
      "1094\n",
      "1095\n",
      "1096\n",
      "1097\n",
      "1098\n",
      "1099\n",
      "1100\n",
      "1101\n",
      "1102\n",
      "1103\n",
      "1104\n",
      "1105\n",
      "1106\n",
      "1107\n",
      "1108\n",
      "1109\n",
      "1110\n",
      "1111\n",
      "1112\n",
      "1113\n",
      "1114\n",
      "1115\n",
      "1116\n",
      "1117\n",
      "1118\n",
      "1119\n",
      "1120\n",
      "1121\n",
      "1122\n",
      "1123\n",
      "1124\n",
      "1125\n",
      "1126\n",
      "1127\n",
      "1128\n",
      "1129\n",
      "1130\n",
      "1131\n",
      "1132\n",
      "1133\n",
      "1134\n",
      "1135\n",
      "1136\n",
      "1137\n",
      "1138\n",
      "1139\n",
      "1140\n",
      "1141\n",
      "1142\n",
      "1143\n",
      "1144\n",
      "1145\n",
      "1146\n",
      "1147\n",
      "1148\n",
      "1149\n",
      "1150\n",
      "1151\n",
      "1152\n",
      "1153\n",
      "1154\n",
      "1155\n",
      "1156\n",
      "1157\n",
      "1158\n",
      "1159\n",
      "1160\n",
      "1161\n",
      "1162\n",
      "1163\n",
      "1164\n",
      "1165\n",
      "1166\n",
      "1167\n",
      "1168\n",
      "1169\n",
      "1170\n",
      "1171\n",
      "1172\n",
      "1173\n",
      "1174\n",
      "1175\n",
      "1176\n",
      "1177\n",
      "1178\n",
      "1179\n",
      "1180\n",
      "1181\n",
      "1182\n",
      "1183\n",
      "1184\n",
      "1185\n",
      "1186\n",
      "1187\n",
      "1188\n",
      "1189\n",
      "1190\n",
      "1191\n",
      "1192\n",
      "1193\n",
      "1194\n",
      "1195\n",
      "1196\n",
      "1197\n",
      "1198\n",
      "1199\n",
      "1200\n",
      "1201\n",
      "1202\n",
      "1203\n",
      "1204\n",
      "1205\n",
      "1206\n",
      "1207\n",
      "1208\n",
      "1209\n",
      "1210\n",
      "1211\n",
      "1212\n",
      "1213\n",
      "1214\n",
      "1215\n",
      "1216\n",
      "1217\n",
      "1218\n",
      "1219\n",
      "1220\n",
      "1221\n",
      "1222\n",
      "1223\n",
      "1224\n",
      "1225\n",
      "1226\n",
      "1227\n",
      "1228\n",
      "1229\n",
      "1230\n",
      "1231\n",
      "1232\n",
      "1233\n",
      "1234\n",
      "1235\n",
      "1236\n",
      "1237\n",
      "1238\n",
      "1239\n",
      "1240\n",
      "1241\n",
      "1242\n",
      "1243\n",
      "1244\n",
      "1245\n",
      "1246\n",
      "1247\n",
      "1248\n",
      "1249\n",
      "1250\n",
      "1251\n",
      "1252\n",
      "1253\n",
      "1254\n",
      "1255\n",
      "1256\n",
      "1257\n",
      "1258\n",
      "1259\n",
      "1260\n",
      "1261\n",
      "1262\n",
      "1263\n",
      "1264\n",
      "1265\n",
      "1266\n",
      "1267\n",
      "1268\n",
      "1269\n",
      "1270\n",
      "1271\n",
      "1272\n",
      "1273\n",
      "1274\n",
      "1275\n",
      "1276\n",
      "1277\n",
      "1278\n",
      "1279\n",
      "1280\n",
      "1281\n",
      "1282\n",
      "1283\n",
      "1284\n",
      "1285\n",
      "1286\n",
      "1287\n",
      "1288\n",
      "1289\n",
      "1290\n",
      "1291\n",
      "1292\n",
      "1293\n",
      "1294\n",
      "1295\n",
      "1296\n",
      "1297\n",
      "1298\n",
      "1299\n",
      "1300\n",
      "1301\n",
      "1302\n",
      "1303\n",
      "1304\n",
      "1305\n",
      "1306\n",
      "1307\n",
      "1308\n",
      "1309\n",
      "1310\n",
      "1311\n",
      "1312\n",
      "1313\n",
      "1314\n",
      "1315\n",
      "1316\n",
      "1317\n",
      "1318\n",
      "1319\n",
      "1320\n",
      "1321\n",
      "1322\n",
      "1323\n",
      "1324\n",
      "1325\n",
      "1326\n",
      "1327\n",
      "1328\n",
      "1329\n",
      "1330\n",
      "1331\n",
      "1332\n",
      "1333\n",
      "1334\n",
      "1335\n",
      "1336\n",
      "1337\n",
      "1338\n",
      "1339\n",
      "1340\n",
      "1341\n",
      "1342\n",
      "1343\n",
      "1344\n",
      "1345\n",
      "1346\n",
      "1347\n",
      "1348\n",
      "1349\n",
      "1350\n",
      "1351\n",
      "1352\n",
      "1353\n",
      "1354\n",
      "1355\n",
      "1356\n",
      "1357\n",
      "1358\n",
      "1359\n",
      "1360\n",
      "1361\n",
      "1362\n",
      "1363\n",
      "1364\n",
      "1365\n",
      "1366\n",
      "1367\n",
      "1368\n",
      "1369\n",
      "1370\n",
      "1371\n",
      "1372\n",
      "1373\n",
      "1374\n",
      "1375\n",
      "1376\n",
      "1377\n",
      "1378\n",
      "1379\n",
      "1380\n",
      "1381\n",
      "1382\n",
      "1383\n",
      "1384\n",
      "1385\n",
      "1386\n",
      "1387\n",
      "1388\n",
      "1389\n",
      "1390\n",
      "1391\n",
      "1392\n",
      "1393\n",
      "1394\n",
      "1395\n",
      "1396\n",
      "1397\n",
      "1398\n",
      "1399\n",
      "1400\n",
      "1401\n",
      "1402\n",
      "1403\n",
      "1404\n",
      "1405\n",
      "1406\n",
      "1407\n",
      "1408\n",
      "1409\n",
      "1410\n",
      "1411\n",
      "1412\n",
      "1413\n",
      "1414\n",
      "1415\n",
      "1416\n",
      "1417\n",
      "1418\n",
      "1419\n",
      "1420\n",
      "1421\n",
      "1422\n",
      "1423\n",
      "1424\n",
      "1425\n",
      "1426\n",
      "1427\n",
      "1428\n",
      "1429\n",
      "1430\n",
      "1431\n",
      "1432\n",
      "1433\n",
      "1434\n",
      "1435\n",
      "1436\n",
      "1437\n",
      "1438\n",
      "1439\n",
      "1440\n",
      "1441\n",
      "1442\n",
      "1443\n",
      "1444\n",
      "1445\n",
      "1446\n",
      "1447\n",
      "1448\n",
      "1449\n",
      "1450\n",
      "1451\n",
      "1452\n",
      "1453\n",
      "1454\n",
      "1455\n",
      "1456\n",
      "1457\n",
      "1458\n",
      "1459\n",
      "1460\n",
      "1461\n",
      "1462\n",
      "1463\n",
      "1464\n",
      "1465\n",
      "1466\n",
      "1467\n",
      "1468\n",
      "1469\n",
      "1470\n",
      "1471\n",
      "1472\n",
      "1473\n",
      "1474\n",
      "1475\n",
      "1476\n",
      "1477\n",
      "1478\n",
      "1479\n",
      "1480\n",
      "1481\n",
      "1482\n",
      "1483\n",
      "1484\n",
      "1485\n",
      "1486\n",
      "1487\n",
      "1488\n",
      "1489\n",
      "1490\n",
      "1491\n",
      "1492\n",
      "1493\n",
      "1494\n",
      "1495\n",
      "1496\n",
      "1497\n",
      "1498\n",
      "1499\n",
      "1500\n",
      "1501\n",
      "1502\n",
      "1503\n",
      "1504\n",
      "1505\n",
      "1506\n",
      "1507\n",
      "1508\n",
      "1509\n",
      "1510\n",
      "1511\n",
      "1512\n",
      "1513\n",
      "1514\n",
      "1515\n",
      "1516\n",
      "1517\n",
      "1518\n",
      "1519\n",
      "1520\n",
      "1521\n",
      "1522\n",
      "1523\n",
      "1524\n",
      "1525\n",
      "1526\n",
      "1527\n",
      "1528\n",
      "1529\n",
      "1530\n",
      "1531\n",
      "1532\n",
      "1533\n",
      "1534\n",
      "1535\n",
      "1536\n",
      "1537\n",
      "1538\n",
      "1539\n",
      "1540\n",
      "1541\n",
      "1542\n",
      "1543\n",
      "1544\n",
      "1545\n",
      "1546\n",
      "1547\n",
      "1548\n",
      "1549\n",
      "1550\n",
      "1551\n",
      "1552\n",
      "1553\n",
      "1554\n",
      "1555\n",
      "1556\n",
      "1557\n",
      "1558\n",
      "1559\n",
      "1560\n",
      "1561\n",
      "1562\n",
      "1563\n",
      "1564\n",
      "1565\n",
      "1566\n",
      "1567\n",
      "1568\n",
      "1569\n",
      "1570\n",
      "1571\n",
      "1572\n",
      "1573\n",
      "1574\n",
      "1575\n",
      "1576\n",
      "1577\n",
      "1578\n",
      "1579\n",
      "1580\n",
      "1581\n",
      "1582\n",
      "1583\n",
      "1584\n",
      "1585\n",
      "1586\n",
      "1587\n",
      "1588\n",
      "1589\n",
      "1590\n",
      "1591\n",
      "1592\n",
      "1593\n",
      "1594\n",
      "1595\n",
      "1596\n",
      "1597\n",
      "1598\n",
      "1599\n",
      "1600\n",
      "1601\n",
      "1602\n",
      "1603\n",
      "1604\n",
      "1605\n",
      "1606\n",
      "1607\n",
      "1608\n",
      "1609\n",
      "1610\n",
      "1611\n",
      "1612\n",
      "1613\n",
      "1614\n",
      "1615\n",
      "1616\n",
      "1617\n",
      "1618\n",
      "1619\n",
      "1620\n",
      "1621\n",
      "1622\n",
      "1623\n",
      "1624\n",
      "1625\n",
      "1626\n",
      "1627\n",
      "1628\n",
      "1629\n",
      "1630\n",
      "1631\n",
      "1632\n",
      "1633\n",
      "1634\n",
      "1635\n",
      "1636\n",
      "1637\n",
      "1638\n",
      "1639\n",
      "1640\n",
      "1641\n",
      "1642\n",
      "1643\n",
      "1644\n",
      "1645\n",
      "1646\n",
      "1647\n",
      "1648\n",
      "1649\n",
      "1650\n",
      "1651\n",
      "1652\n",
      "1653\n",
      "1654\n",
      "1655\n",
      "1656\n",
      "1657\n",
      "1658\n",
      "1659\n",
      "1660\n",
      "1661\n",
      "1662\n",
      "1663\n",
      "1664\n",
      "1665\n",
      "1666\n",
      "1667\n",
      "1668\n",
      "1669\n",
      "1670\n",
      "1671\n",
      "1672\n",
      "1673\n",
      "1674\n",
      "1675\n",
      "1676\n",
      "1677\n",
      "1678\n",
      "1679\n",
      "1680\n",
      "1681\n",
      "1682\n",
      "1683\n",
      "1684\n",
      "1685\n",
      "1686\n",
      "1687\n",
      "1688\n",
      "1689\n",
      "1690\n",
      "1691\n",
      "1692\n",
      "1693\n",
      "1694\n",
      "1695\n",
      "1696\n",
      "1697\n",
      "1698\n",
      "1699\n",
      "1700\n",
      "1701\n",
      "1702\n",
      "1703\n",
      "1704\n",
      "1705\n",
      "1706\n",
      "1707\n",
      "1708\n",
      "1709\n",
      "1710\n",
      "1711\n",
      "1712\n",
      "1713\n",
      "1714\n",
      "1715\n",
      "1716\n",
      "1717\n",
      "1718\n",
      "1719\n",
      "1720\n",
      "1721\n",
      "1722\n",
      "1723\n",
      "1724\n",
      "1725\n",
      "1726\n",
      "1727\n",
      "1728\n",
      "1729\n",
      "1730\n",
      "1731\n",
      "1732\n",
      "1733\n",
      "1734\n",
      "1735\n",
      "1736\n",
      "1737\n",
      "1738\n",
      "1739\n",
      "1740\n",
      "1741\n",
      "1742\n",
      "1743\n",
      "1744\n",
      "1745\n",
      "1746\n",
      "1747\n",
      "1748\n",
      "1749\n",
      "1750\n",
      "1751\n",
      "1752\n",
      "1753\n",
      "1754\n",
      "1755\n",
      "1756\n",
      "1757\n",
      "1758\n",
      "1759\n",
      "1760\n",
      "1761\n",
      "1762\n",
      "1763\n",
      "1764\n",
      "1765\n",
      "1766\n",
      "1767\n",
      "1768\n",
      "1769\n",
      "1770\n",
      "1771\n",
      "1772\n",
      "1773\n",
      "1774\n",
      "1775\n",
      "1776\n",
      "1777\n",
      "1778\n",
      "1779\n",
      "1780\n",
      "1781\n",
      "1782\n",
      "1783\n",
      "1784\n",
      "1785\n",
      "1786\n",
      "1787\n",
      "1788\n",
      "1789\n",
      "1790\n",
      "1791\n",
      "1792\n",
      "1793\n",
      "1794\n",
      "1795\n",
      "1796\n",
      "1797\n",
      "1798\n",
      "1799\n",
      "1800\n",
      "1801\n",
      "1802\n",
      "1803\n",
      "1804\n",
      "1805\n",
      "1806\n",
      "1807\n",
      "1808\n",
      "1809\n",
      "1810\n",
      "1811\n",
      "1812\n",
      "1813\n",
      "1814\n",
      "1815\n",
      "1816\n",
      "1817\n",
      "1818\n",
      "1819\n",
      "1820\n",
      "1821\n",
      "1822\n",
      "1823\n",
      "1824\n",
      "1825\n",
      "1826\n",
      "1827\n",
      "1828\n",
      "1829\n",
      "1830\n",
      "1831\n",
      "1832\n",
      "1833\n",
      "1834\n",
      "1835\n",
      "1836\n",
      "1837\n",
      "1838\n",
      "1839\n",
      "1840\n",
      "1841\n",
      "1842\n",
      "1843\n",
      "1844\n",
      "1845\n",
      "1846\n",
      "1847\n",
      "1848\n",
      "1849\n",
      "1850\n",
      "1851\n",
      "1852\n",
      "1853\n",
      "1854\n",
      "1855\n",
      "1856\n",
      "1857\n",
      "1858\n",
      "1859\n",
      "1860\n",
      "1861\n",
      "1862\n",
      "1863\n",
      "1864\n",
      "1865\n",
      "1866\n",
      "1867\n",
      "1868\n",
      "1869\n",
      "1870\n",
      "1871\n",
      "1872\n",
      "1873\n",
      "1874\n",
      "1875\n",
      "1876\n",
      "1877\n",
      "1878\n",
      "1879\n",
      "1880\n",
      "1881\n",
      "1882\n",
      "1883\n",
      "1884\n",
      "1885\n",
      "1886\n",
      "1887\n",
      "1888\n",
      "1889\n",
      "1890\n",
      "1891\n",
      "1892\n",
      "1893\n",
      "1894\n",
      "1895\n",
      "1896\n",
      "1897\n",
      "1898\n",
      "1899\n",
      "1900\n",
      "1901\n",
      "1902\n",
      "1903\n",
      "1904\n",
      "1905\n",
      "1906\n",
      "1907\n",
      "1908\n",
      "1909\n",
      "1910\n",
      "1911\n",
      "1912\n",
      "1913\n",
      "1914\n",
      "1915\n",
      "1916\n",
      "1917\n",
      "1918\n",
      "1919\n",
      "1920\n",
      "1921\n",
      "1922\n",
      "1923\n",
      "1924\n"
     ]
    }
   ],
   "source": [
    "example_f2_f3_l=get_f2f3_for_df(path,n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "dbff19b3-13d8-4e8d-b09d-f60b59d93ae2",
   "metadata": {},
   "outputs": [],
   "source": [
    "p5_deg=example_f2_f3_l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "00697924-5a96-4ef0-8c57-d2025a47934b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'1': 0, '2': 0, '3': 0, '4': 0, '5': 0, '12': 0, '13': 0, '14': 0, '15': 0, '23': 0, '24': 0, '25': 0, '34': 0, '35': 0, '45': 0, '123': 0, '124': 0, '125': 0, '134': 0, '135': 1, '145': 0, '234': 0, '235': 0, '245': 1, '345': 0, '1234': 0, '1235': 1, '1245': 1, '1345': 1, '2345': 1, '12345': 1} \n",
      " [] \n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# #number of f3 per f2\n",
    "# nic_ex={'1': 0, '2': 0, '3': 0, '4': 0, '5': 0, '12': 0, '13': 0, '14': 0, '15': 0, '23': 0, '24': 0, '25': 0, '34': 0, '35': 0, '45': 0, '123': 0, '124': 0, '125': 0, '134': 0, '135': 1, '145': 0, '234': 0, '235': 0, '245': 1, '345': 0, '1234': 0, '1235': 1, '1245': 1, '1345': 1, '2345': 1, '12345': 1}\n",
    "\n",
    "# for i in range(len(p5_deg)):\n",
    "#     # print(len(example_f2_f3_l[i][1]))\n",
    "    \n",
    "#     #checking nuber of f3 for nicolas exampe\n",
    "    \n",
    "#     # if p5_deg[i][0]==nic_ex:\n",
    "#     #     print(p5_deg[i][0],\"\\n\",p5_deg[i][1],\"\\n\\n\")\n",
    "        \n",
    "#     # if len(p5_deg[i][1])==0:\n",
    "#     #     print(p5_deg[i][0],\"\\n\",p5_deg[i][1],\"\\n\\n\")\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "2442d90a-c9ef-4831-9256-4fa4ff545b38",
   "metadata": {},
   "outputs": [],
   "source": [
    "# #Save this\n",
    "# with open('results\\P5_degen_f2_f3l.pkl', 'wb') as file:\n",
    "#     pickle.dump(example_f2_f3_l, file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c33d5f2d-41fe-4212-b5f4-a6b935df9177",
   "metadata": {},
   "source": [
    "Given this list of [f2,f3_l] i wish to present those pairs-where the list of f3_l is non-empty in a text file to send to marco."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "344661ae-9e86-4036-95a3-8a8ff538251f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# #outputing examples to text file.\n",
    "# data = example_f2_f3_l\n",
    "\n",
    "# # Open a text file in write mode\n",
    "# with open('output.txt', 'w') as file:\n",
    "#     file.write(\"Given those f2 in P5 which empty polytopes (1924 many) we record for each f2 the set of f3\\n\\n\")\n",
    "\n",
    "#     for i, sub_list in enumerate(data):\n",
    "#         f2 = sub_list[0]\n",
    "#         file.write(f'f2_{i+1} = {f2}\\n')\n",
    "\n",
    "#         f3_list = sub_list[1]\n",
    "#         for j, f3 in enumerate(f3_list):\n",
    "#             file.write(f'f3{i+1}{j+1} = {f3}\\n')\n",
    "#         file.write(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac622d4a-1fb2-4378-888e-a0719369cb02",
   "metadata": {},
   "source": [
    "### Example get f3's for each f2 in P5 (not yet done, not necessary yet)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8aeb66c8-29f4-4daf-9853-c5a37d4c427b",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### Example get f3's for each f2 in P3 Translated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "ae8d7b82-fc70-42f5-ba3f-c512d00fcc5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "path=\"results\\P3_trans.pkl\"\n",
    "n=3\n",
    "power_set_str,power_set_str_np1=get_n_np1_powersets(n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "8e3a4cca-5773-4acf-a409-29bf33eeb2d1",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n"
     ]
    }
   ],
   "source": [
    "#this is all for P3\n",
    "example_f2_f3_l=get_f2f3_for_df(path,n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "37a85a57-9219-47fb-96ef-6555a0797f13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Save this\n",
    "# with open('results\\P3_trans_f2_f3l.pkl', 'wb') as file:\n",
    "#     pickle.dump(example_f2_f3_l, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "c581a4fa-e463-418e-8fff-b42a1f3171e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total:27\n"
     ]
    }
   ],
   "source": [
    "total=0\n",
    "for i in range(len(example_f2_f3_l)):\n",
    "    # print(len(example_f2_f3_l[i][1]))\n",
    "    total+=len(example_f2_f3_l[i][1])\n",
    "    # if len(example_f2_f3_l[i][1])==0:\n",
    "    #     print(example_f2_f3_l[i][0],\"\\n\",example_f2_f3_l[i][1],\"\\n\\n\")\n",
    "print(f\"total:{total}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "3dd5437e-734c-4d8c-a125-a6c925a4413e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# #outputing examples to text file.\n",
    "# data = example_f2_f3_l\n",
    "\n",
    "# # Open a text file in write mode\n",
    "# with open('P3_trans_f2_f3l.txt', 'w') as file:\n",
    "#     file.write(\"Given f2 in P3 we record for each f2 the set of f3\\n\\n\")\n",
    "\n",
    "#     for i, sub_list in enumerate(data):\n",
    "#         f2 = sub_list[0]\n",
    "#         file.write(f'f2_{i+1} = {f2}\\n')\n",
    "\n",
    "#         f3_list = sub_list[1]\n",
    "#         for j, f3 in enumerate(f3_list):\n",
    "#             file.write(f'f3{i+1}{j+1} = {f3}\\n')\n",
    "#         file.write(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e133e20b-cfc5-45cc-b0aa-ea2a5e1b7696",
   "metadata": {},
   "source": [
    "### Example get f3's for each f2 in P4 Translated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "aec4b531-a39c-41cd-9bfa-7c09030b84c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "path=\"results\\P4_trans.pkl\"\n",
    "n=4\n",
    "power_set_str,power_set_str_np1=get_n_np1_powersets(n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "5dd8d75f-a374-4940-908b-ed75f67c43e2",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "25\n",
      "26\n",
      "27\n",
      "28\n",
      "29\n",
      "30\n",
      "31\n",
      "32\n",
      "33\n",
      "34\n",
      "35\n",
      "36\n",
      "37\n",
      "38\n",
      "39\n",
      "40\n",
      "41\n",
      "42\n",
      "43\n",
      "44\n",
      "45\n",
      "46\n",
      "47\n",
      "48\n",
      "49\n",
      "50\n",
      "51\n",
      "52\n",
      "53\n",
      "54\n",
      "55\n",
      "56\n",
      "57\n",
      "58\n",
      "59\n",
      "60\n",
      "61\n",
      "62\n",
      "63\n",
      "64\n",
      "65\n",
      "66\n",
      "67\n",
      "68\n",
      "69\n",
      "70\n",
      "71\n",
      "72\n",
      "73\n",
      "74\n",
      "75\n",
      "76\n",
      "77\n",
      "78\n",
      "79\n",
      "80\n",
      "81\n",
      "82\n",
      "83\n",
      "84\n",
      "85\n",
      "86\n",
      "87\n",
      "88\n",
      "89\n",
      "90\n",
      "91\n",
      "92\n",
      "93\n",
      "94\n",
      "95\n",
      "96\n",
      "97\n",
      "98\n",
      "99\n",
      "100\n",
      "101\n",
      "102\n",
      "103\n",
      "104\n",
      "105\n",
      "106\n",
      "107\n",
      "108\n",
      "109\n",
      "110\n",
      "111\n",
      "112\n",
      "113\n",
      "114\n",
      "115\n",
      "116\n",
      "117\n",
      "118\n",
      "119\n",
      "120\n",
      "121\n",
      "122\n",
      "123\n",
      "124\n",
      "125\n",
      "126\n",
      "127\n",
      "128\n",
      "129\n",
      "130\n",
      "131\n",
      "132\n",
      "133\n",
      "134\n",
      "135\n",
      "136\n",
      "137\n",
      "138\n",
      "139\n",
      "140\n",
      "141\n",
      "142\n",
      "143\n",
      "144\n",
      "145\n",
      "146\n",
      "147\n",
      "148\n",
      "149\n",
      "150\n",
      "151\n",
      "152\n",
      "153\n",
      "154\n"
     ]
    }
   ],
   "source": [
    "#this is all for P4\n",
    "example_f2_f3_l=get_f2f3_for_df(path,n)\n",
    "\n",
    "# #number of f3s per f2.\n",
    "# for i in range(154):\n",
    "#     print(len(example_f2_f3_l[i][1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "eb71d838-8a60-46a7-ae29-d8e72029afed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# #Save this\n",
    "# with open('results\\P4_trans_f2_f3l.pkl', 'wb') as file:\n",
    "#     pickle.dump(example_f2_f3_l, file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "188a5eee-24e5-4369-adcf-08f82ab317cd",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Used Throughout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "48b0bfce-c6fe-4ad1-88fd-7a04bf82fcd1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import itertools\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "import time\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "764b73c6-e6fa-4081-892c-898554640dcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ast\n",
    "import inspect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d9fb37b4-5458-4578-8613-c43fe5a4d78c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_functions_used(func):\n",
    "    \n",
    "    \"\"\"Add print Objective.\"\"\"\n",
    "    \n",
    "    functions_used = set()\n",
    "    tree = ast.parse(inspect.getsource(func))\n",
    "    for node in ast.walk(tree):\n",
    "        if isinstance(node, ast.Call) and isinstance(node.func, ast.Name):\n",
    "            functions_used.add(node.func.id)\n",
    "    return functions_used\n",
    "\n",
    "def extract_objective(func):\n",
    "    \"\"\"\n",
    "    Extracts the string of the line containing \"Objective\" from the docstring of a given function.\n",
    "\n",
    "    Args:\n",
    "        func: The function to extract the objective from.\n",
    "\n",
    "    Returns:\n",
    "        A string containing the objective.\n",
    "    \"\"\"\n",
    "    \n",
    "    \"\"\"\n",
    "        def f():\n",
    "\n",
    "        \n",
    "            Objective: This is a test\n",
    "            Input:\n",
    "            Return\n",
    "        \n",
    "\n",
    "        x=33\n",
    "\n",
    "        return\n",
    "        extract_objective(f)\n",
    "    \"\"\"\n",
    "    docstring = inspect.getdoc(func)\n",
    "    if docstring is None:\n",
    "        return None\n",
    "    lines = docstring.split('\\n')\n",
    "    for line in lines:\n",
    "        if 'Objective' in line:\n",
    "            return line.strip()\n",
    "    return None\n",
    "\n",
    "def find_functions_used_l(l,flag=False):\n",
    "    for func in l:\n",
    "        functions_used = find_functions_used(func)\n",
    "        if flag==True:\n",
    "            print(f\"{func.__name__}:\\n{extract_objective(func)}\\n{functions_used} \\n\")\n",
    "        if flag==False:\n",
    "            print(f\"{func.__name__}:\\n{functions_used} \\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "030edd01-f96d-4cdc-b014-3da94dd3eefa",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "toc-autonumbering": false,
  "toc-showcode": false,
  "toc-showmarkdowntxt": false,
  "toc-showtags": false
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
