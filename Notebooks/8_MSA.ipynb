{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8e5fc415-1f30-409e-b802-f051fb4f9ed8",
   "metadata": {
    "tags": [],
    "toc-hr-collapsed": true
   },
   "source": [
    "# Overview"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "174c4f90-96d6-4c38-aacb-b3cf8b774d3b",
   "metadata": {},
   "source": [
    "We consider Chapter $5$ of the thesis. In particular we implement the Algorithm of Section $5.1$ and generate $\\tilde{P}_{n}$ for $1 \\le n\\le 6$  from $\\tilde{P}_{n-1}$. We then give a decomposition of $p_n:=|\\tilde{P}_n|$ in terms of $|\\tilde{P}_{n-1}|$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fd53259-e079-407a-a7cf-b5a0b8887c7d",
   "metadata": {},
   "source": [
    "Recall that,\n",
    "\n",
    "$$P_n=\\bigsqcup_{g \\in P_{n-1}} \\rho^{-1}(g)= \\bigsqcup_{g \\in \\tilde{P}_{n}}\\{g+\\epsilon_{I} | I \\in \\mathbb{E}(g)\\}.$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "140edc3c-2562-498d-9dec-14a5378e59ce",
   "metadata": {},
   "source": [
    "The **procedure** to recursively obtain $\\tilde{P}_{n}$ from $\\tilde{P}_{n-1}$ is as follows:\n",
    " \n",
    " 1) for each  $g \\in \\tilde{P}_{n-1}$ obtiain the set $\\mathbb{E}(g)$ (using the Algorithm of Section $5.1$ )\n",
    " \n",
    "     1.1) for each  $g \\in \\tilde{P}_{n-1}$ we record $|\\mathbb{E}(g)|$ (which allows one to obtain the sequences $e_{n}$ and\n",
    "     $d_{n}$ which allow for $p_{n+1}=e_n \\cdot d_n$)\n",
    "     \n",
    " 2) for each set $\\mathbb{E}(g)$ obtain the set $\\{g+\\epsilon_{I} | I \\in \\mathbb{E}(g)\\}$\n",
    " \n",
    " 3) take the union as above to obtain $\\tilde{P}_{n}$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df2673cf-764a-4a22-9fe6-faf1f80f85ce",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80557401-3abf-4e62-bfc2-9afc9d447ad9",
   "metadata": {},
   "source": [
    "We obtain the sequence of $p_n$ for $1 \\le n\\le 6$.\n",
    "\n",
    "  $$1,2,10,154,10334,5399325.$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1c1d80a-00ae-44e2-8ce0-c07db37b3bbc",
   "metadata": {},
   "source": [
    "We also have that,"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a42b809-3bff-4b24-9795-734e9203d3b0",
   "metadata": {},
   "source": [
    "$$p_6=5399325=114*125+121*65+126*237+138*112+158*248+162*247+165*132+169*117+173*132+178*122+182*106+183*132+187*24+192*134+194*123+208*144+209*22+216*234+217*126+218*61+221*125+227*22+229*307+230*250+232*120+233*119+236*122+241*30+242*102+247*61+248*53+250*114+251*59+260*184+268*106+269*64+271*186+273*118+274*56+278*52+279*41+281*136+284*112+289*67+293*133+294*37+300*133+305*67+313*123+324*123+325*132+326*101+329*181+338*120+343*59+345*113+356*19+381*121+390*44+397*107+419*20+420*46+421*23+422*39+429*64+433*35+436*128+437*23+455*64+477*53+496*48+502*16+509*38+518*12+520*107+524*20+531*36+538*76+539*62+553*36+555*66+566*144+578*61+605*62+607*66+613*11+640*28+659*30+759*48+765*21+781*108+802*119+814*42+881*42+938*8+974*135+977*45+983*135+1002*45+1026*126+1029*63+1033*63+1068*96+1121*108+1145*22+1178*39+1189*39+1209*44+1212*11+1224*66+1257*44+1312*11+1322*32+1377*36+1409*16+1421*10+1450*54+1511*20+1666*21+1753*24+2110*42+2193*95+2285*4+2290*54+3544*12+3673*42+3820*48+3983*18+6280*1+6474*6+6702*14+6960*16+7250*9+7580*4$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a683c86d-6b1a-49a6-b8f7-e5098ad5a61d",
   "metadata": {},
   "source": [
    "where the decomposition of the remaining $p_{n}$ are stated in Section $5.1$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdf4e57b-30fa-4e17-9368-3bc7f96cf041",
   "metadata": {},
   "source": [
    "In \"4) Example $\\mathbb{E}(g)$ for $g \\in  \\tilde{P}_{3}$\" we provide a method to obtain $\\mathbb{E}(g)$ for any $g \\in \\tilde{P}_{n}$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a4921f6-e629-492d-b6ce-648fea27687c",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Structure"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8d6f413-bd09-41b9-8187-d4fa7faf93b1",
   "metadata": {},
   "source": [
    "The main content of this file is in the section #$\\tilde{P}_3,\\tilde{P}_4,\\tilde{P}_5$ and #Decomposition of $|\\tilde{P}_n|$ for $n\\le 5$.\n",
    "\n",
    "In  #$\\tilde{P}_3,\\tilde{P}_4,\\tilde{P}_5$ we have the implementation of the algorithm in Section $5.1$ in \"Functions\", and dataframes of $\\tilde{P}_3,\\tilde{P}_4,\\tilde{P}_5$ in \"Cases\".\n",
    "\n",
    "\n",
    "In section #$\\tilde{P}_6$ we apply the same procedure as in the previous cases, except we break the compution into parts due to its computational time."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ee0a9b6-8ab7-46f6-93c0-0a2fb3b31887",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Functions used throughout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ed66ac4c-d294-4fe1-b82f-f780dc1cb1ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "import time\n",
    "import ast\n",
    "import inspect"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc2e1d3b-5daa-4ccf-ab4e-43c627ea10e2",
   "metadata": {
    "tags": [],
    "toc-hr-collapsed": true
   },
   "source": [
    "# Table of contents"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6674d04-4ed9-4862-a954-6578ef212321",
   "metadata": {},
   "source": [
    "1. [$\\tilde{P}_3,\\tilde{P}_4,\\tilde{P}_5$](#s1)\n",
    "    1. [Functions](#s11)\n",
    "        1. [General notation](s111)\n",
    "        2. [To obtain $\\mathbb{E}(g)$ for $g \\in \\tilde{P}_{n-1}$](s112)\n",
    "        3. [Obtaining $\\tilde{P}_{n+1}$ from $\\mathbb{E}(g)$ from $g \\in \\tilde{P}_{n-1}$](s113)\n",
    "        4. [Analysis of $\\tilde{P}_{n}$](s114)\n",
    "    2. [Cases](#s12)\n",
    "        1. [$\\tilde{P}_{3}$ by $\\tilde{P}_{2}$](s121)\n",
    "        2. [$\\tilde{P}_{4}$ by $\\tilde{P}_{3}$](s122)\n",
    "        3. [$\\tilde{P}_{5}$ by $\\tilde{P}_{4}$](s123)\n",
    "    \n",
    "2. [Decomposition of $|\\tilde{P}_n|$ for $n \\le 5$](#s2)\n",
    "    1. [Functions](#s21)\n",
    "    2. [Cases](#s22)\n",
    "        1. [Decomposition of $|\\tilde{P}_3|$](#s221)\n",
    "        2. [Decomposition of $|\\tilde{P}_4|$](#s222)\n",
    "        3. [Decomposition of $|\\tilde{P}_5|$](#s223)\n",
    "        \n",
    "3. [Which $g \\in \\tilde{P}_{n-1}$ attain max and min $|\\mathbb{E}(g)|$](#s3)\n",
    "    1. [$U_3$](#s31)\n",
    "    2. [$L_3$](#s32)\n",
    "    \n",
    "\n",
    "4. [$\\tilde{P}_6$ and decomposition of $|\\tilde{P}_6|$ (Debt)](#s4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "431c2a14-6e49-47ec-a3d9-668b7ed92fbc",
   "metadata": {
    "tags": [],
    "toc-hr-collapsed": true
   },
   "source": [
    "# $\\tilde{P}_3,\\tilde{P}_4,\\tilde{P}_5$ <a name=\"s1\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be280e86-74e7-4066-bb2b-466dd64f7522",
   "metadata": {
    "tags": [],
    "toc-hr-collapsed": true
   },
   "source": [
    "## Functions <a name=\"s11\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d1ab5bc-946e-4163-bed5-d54372a078ad",
   "metadata": {},
   "source": [
    "Before recursively constructing $\\tilde{P}_{n}$ we first need to implement the notation and **procedure**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4dc4295a-d106-4967-9d42-22a30f51ead9",
   "metadata": {
    "tags": [],
    "toc-hr-collapsed": true
   },
   "source": [
    "### General notation:<a name=\"s111\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3daa77b8-9fa3-4fcd-afe8-761b6117cc51",
   "metadata": {},
   "source": [
    "The following functions allow us to store $g \\in \\tilde{P}_n$ as a dictionary (e.g. of the form {'1': 0, '2': 0, '12': 0}) in particular the keys as a string representation of $I \\in \\mathcal{P}_{n}^{+}$ and to change back to sets notation when needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "55cf9933-2266-41eb-bf1c-022de645e6bb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def set_to_string(s):\n",
    "    \"\"\"\n",
    "    Objective: Convert a set of integers to a sorted string representation.\n",
    "    \n",
    "    Input:\n",
    "        - s: A set of integers.\n",
    "        \n",
    "    Returns:\n",
    "        - result: A string containing the sorted integers from the input set.\n",
    "    \"\"\"\n",
    "    \n",
    "    \n",
    "    s = list(s)\n",
    "    s.sort()\n",
    "    # Convert the list of numbers to a list of strings\n",
    "    my_list=s\n",
    "    my_list = [str(i) for i in my_list]\n",
    "    # Use the join() function to convert the list of strings to a single string\n",
    "    result = ''.join(my_list)\n",
    "    return result\n",
    "\n",
    "def string_to_set(s):\n",
    "    \"\"\"\n",
    "    Objective: Convert a string of digits to a set of integers.\n",
    "    \n",
    "    Input:\n",
    "        - s: A string of digits.\n",
    "        \n",
    "    Returns:\n",
    "        - my_set: A set containing the integers parsed from the input string.\n",
    "    \"\"\"\n",
    "    my_list = list(s)\n",
    "    # Convert the list of strings to a list of integers\n",
    "    my_list = list(map(int,my_list))\n",
    "    # Convert the list to a set\n",
    "    my_set = set(my_list)\n",
    "    return my_set"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f4cf8b3-55b8-4442-bab8-5259ae0c8663",
   "metadata": {},
   "source": [
    "The following functions allow us to state $\\mathcal{P}_{n}^{+}=\\mathcal{P}_{n} \\setminus \\emptyset$ and $\\mathcal{P}(\\mathcal{P}_{n}^{+})$ (where $\\mathcal{P}_{n}$ is the powerset of $\\{1,\\dots ,n\\}$)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "42c764fc-7c14-4a1e-8b6f-3ea21bbdb9f5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_powerset_str(n):\n",
    "    \"\"\"\n",
    "    Objective: Generate the power set of integers from 1 to n and represent it as a list of sorted strings.\n",
    "    \n",
    "    Input:\n",
    "        - n: An integer representing the upper bound of the set of integers (1 to n).\n",
    "        \n",
    "    Returns:\n",
    "        - power_set_str: A list of sorted strings representing the power set of integers from 1 to n.\n",
    "    \"\"\"\n",
    "    s=set(range(1,n+1))\n",
    "    power_set = [set(x)  for r in range(len(s) + 1) for x in itertools.combinations(s, r)]\n",
    "    power_set_str=[set_to_string(x) for x in power_set if len(x)>0]\n",
    "\n",
    "    return power_set_str\n",
    "\n",
    "def get_n_np1_powersets(n):\n",
    "    \"\"\"\n",
    "    Objective: Generate the power sets of integers from 1 to n and 1 to (n+1), representing them as lists of sorted strings.\n",
    "    \n",
    "    Input:\n",
    "        - n: An integer representing the upper bound of the sets of integers (1 to n and 1 to n+1).\n",
    "        \n",
    "    Returns:\n",
    "        - power_set_str: A list of sorted strings representing the power set of integers from 1 to n.\n",
    "        - power_set_str_np1: A list of sorted strings representing the power set of integers from 1 to n+1.\n",
    "    \"\"\"\n",
    "    s=set(range(1,n+1))\n",
    "    power_set = [set(x)  for r in range(len(s) + 1) for x in itertools.combinations(s, r)]\n",
    "    power_set_str=[set_to_string(x) for x in power_set if len(x)>0]\n",
    "\n",
    "    np1=n+1\n",
    "    sp1=set(range(1,n+2))\n",
    "    power_set_np1 = [set(x)  for r in range(np1+1 + 1) for x in itertools.combinations(sp1, r)]\n",
    "    power_set_str_np1=[set_to_string(x) for x in power_set_np1 if len(x)>0]\n",
    "\n",
    "    return power_set_str,power_set_str_np1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7eb6c969-3029-484d-b914-5a4600b3625b",
   "metadata": {},
   "source": [
    "As sets are mutable we use the following function to put them into a starndard form and make them immutable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "78cd4a3c-12c5-4447-835e-c7f2f0d55e6a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def sorted_frozenset(s):\n",
    "    \"\"\"\n",
    "    Objective: Sort the elements of a set and return them as a frozenset.\n",
    "    \n",
    "    Input:\n",
    "        - s: A set of elements.\n",
    "        \n",
    "    Returns:\n",
    "        - r: A frozenset containing the sorted elements from the input set.\n",
    "    \"\"\"\n",
    "    t=sorted(s)\n",
    "    r=frozenset(t)\n",
    "    return r"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "952aa0f2-6df8-4492-b176-752a427ab709",
   "metadata": {
    "tags": []
   },
   "source": [
    "###  To obtain $\\mathbb{E}(g)$ for $g \\in \\tilde{P}_{n-1}$ <a name=\"s112\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "186fe984-7ae5-4486-9fca-83543f28dd15",
   "metadata": {},
   "source": [
    "In this section we provide a method to return for a given  $g \\in \\tilde{P}_{n-1}$ the set $\\mathbb{E}(g)$ (using the Algorithm of Section $5.1$ ) we do this through the function get_Eg(). We break this section into the following parts:\n",
    "\n",
    "1) Obtain a method to determine for $A,B \\in \\mathcal{P}_{n}^{+}$ such that $A \\subseteq B$ to state when $A \\subseteq B$ is $f$-minimal or $f$-maximal. \n",
    "\n",
    "2) Obtain a method to obtain the closure of any $I \\subseteq \\in \\mathcal{P}_{n}^{+}$.\n",
    "\n",
    "3) Implement the Algorithim of Section $5.1$ to obtain $\\mathbb{E}(g)$ for any $g$.\n",
    "\n",
    "4) Consider an example of $\\mathbb{E}(g)$ for $g \\in  \\tilde{P}_{3}$.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "638db21b-5270-41c7-889b-2bf89d5c92dc",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### 1) $f$-minimality or $f$-maximality"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9681e14-5254-4c26-9b64-0485561db7dd",
   "metadata": {
    "tags": []
   },
   "source": [
    "Let $f \\in \\tilde{P}_{n}$ and $A,B \\in \\mathcal{P}_{n}^{+}$ such that $A \\subseteq B$.  The following functions determine if $A \\subseteq B$ is $f$-minimal or $f$-maximal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b37abe83-df41-4ce9-b808-f58ad12c4909",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def fmin(A,B,f_dict):\n",
    "    \"\"\"\n",
    "    Objective: Determine if A is f-minimal with respect to B using a given dictionary of values.\n",
    "\n",
    "    Input:\n",
    "        - A: A string representing a set from the power set P_n.\n",
    "        - B: A string representing another set from the power set P_n where A is a subset of B.\n",
    "        - f_dict: A dictionary mapping set strings to corresponding values.\n",
    "\n",
    "    Returns:\n",
    "        - True if A is f-minimal with respect to B, False otherwise.\n",
    "    \"\"\"\n",
    "    \n",
    "    #A included in B\n",
    "    # global f_dict\n",
    "    \n",
    "    valA=f_dict[A]\n",
    "    valB=f_dict[B]\n",
    "    compBA=set_to_string(string_to_set(B)-string_to_set(A)) # B minus A\n",
    "    if len(compBA)==0: # to avoide when taking the complement gives empty set\n",
    "        return False\n",
    "    valBA=f_dict[compBA]\n",
    "    if valB==valA+valBA:\n",
    "        return True\n",
    "    else:\n",
    "        return False\n",
    "    \n",
    "def fmax(A,B,f_dict):\n",
    "    \"\"\"\n",
    "    Objective: Determine if A is f-maximal with respect to B using a given dictionary of values.\n",
    "\n",
    "    Input:\n",
    "        - A: A string representing a set from the power set P_n.\n",
    "        - B: A string representing another set from the power set P_n where A is a subset of B.\n",
    "        - f_dict: A dictionary mapping set strings to corresponding values.\n",
    "\n",
    "    Returns:\n",
    "        - True if A is f-maximal with respect to B, False otherwise.\n",
    "    \"\"\"\n",
    "    #A included in B\n",
    "    # global f_dict\n",
    "    \n",
    "    valA=f_dict[A]\n",
    "    valB=f_dict[B]\n",
    "    compBA=set_to_string(string_to_set(B)-string_to_set(A)) # B minus A\n",
    "    \n",
    "    #A has to be a subset of B first\n",
    "    if len(compBA)==0: # to avoide when taking the complement gives empty set\n",
    "        return False\n",
    "    valBA=f_dict[compBA]\n",
    "    if valB==valA+valBA+1:\n",
    "        return True\n",
    "    else:\n",
    "        return False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83b95cce-a5e1-451d-8f52-3e842beb6668",
   "metadata": {
    "tags": [],
    "toc-hr-collapsed": true
   },
   "source": [
    "#### 2) Implementation of closure of and $I \\subseteq \\mathcal{P}_{n}^{+}$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f175a29-5b08-4bfb-8ce5-c8ee9136ea42",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "Recall Definition $5.1.4$. That the closure of some $I \\subseteq \\mathcal{P}_{n}^{+}$ denoted as, $\\overline{I}$, with respect to $g$ is be the largest set in the following sequence of subsets of $\\mathcal{P}_{n}^+$,\n",
    "\n",
    "$$I=I_0 \\subseteq  \\cdots \\subseteq I_k $$\n",
    "\n",
    "where $I_{i}:=B_{i-1}\\cup I_{i-1}$ (we call this the $i$-partial closure of $I$) such that \n",
    "\n",
    "$$    B_{i-1}=  \\left\\{y \\in \\mathcal{P}_{n}^{+} \\mid \\forall x \\in I_{i-1} \\text{ such that } y \\supsetneq x \\text{ is $g$-minimal} \\text{ or }\n",
    "y \\subsetneq x  \\text{ is $g$-maximal} \\right\\}.$$ \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "The next function takes $I:=I_0 \\in \\mathcal{P}_{n}^{+}$ and returns $I_{1}$ as defined above.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b6b01230-5cc3-46e1-8e10-d4962994677b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def next_j(ji,f_dict):\n",
    "    \"\"\"\n",
    "    Objective: Get the $i+1$-partial closure of $I$ of ji with respect to f_dict. \n",
    "    \n",
    "    By iterating through elements in ji and checks conditions involving fmin and fmax to determine which elements should be included in the new set jip1.\n",
    "    \n",
    "    Input:\n",
    "    #ji is the ith recursion set on constructing the closure. \n",
    "    f_dict: the function ususal denoted as g\n",
    "    \n",
    "    Returns: closure of ji.\n",
    "    \"\"\"\n",
    "    # print(\"ji\",ji)\n",
    "    # global f\n",
    "    \n",
    "    my_list=[]    \n",
    "    \n",
    "    \n",
    "    for x in power_set_str:\n",
    "        for y in ji:\n",
    "            if fmin(y,x,f_dict) and string_to_set(y).issubset(string_to_set(x)):\n",
    "                # print(\"y is\",y)\n",
    "                my_list.append(x)\n",
    "                # print(f\"{x} containing {y} f min \")\n",
    "    for x in power_set_str:\n",
    "        for y in ji:\n",
    "            if fmax(x,y,f_dict) and string_to_set(x).issubset(string_to_set(y)):\n",
    "                my_list.append(x)\n",
    "                \n",
    "    \n",
    "    jip1=set(my_list).union(ji)\n",
    "    \n",
    "    return jip1    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbb20844-d0f7-47b3-9936-4a10c51a16b7",
   "metadata": {},
   "source": [
    "The following function allows us to go from $i$-partial closure of $I$ to the $i+1$-partial closure of $I$ and continue this until we end at the closure of $I$ wrt $g$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1694586d-c6d0-47d5-9447-d56f2120a8eb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def rec_j(T,S,f_dict):\n",
    "    \n",
    "    \n",
    "    \"\"\"\n",
    "    Objective: Given the $i$-partial closure of $S:=Ii$ to the $i+1$-partial closure of $T:=Ii+1$\n",
    "    Input:\n",
    "    T: (T=Ii)\n",
    "    S : S=Ii-1)\n",
    "    \n",
    "    f_dict : function\n",
    "    \n",
    "    Returns: The closure of S.\n",
    "    \"\"\"\n",
    "    # T=Ji and S=Ji-1, |T|\\ge |S|.\n",
    "    \n",
    "    \n",
    "    \"\"\"\n",
    "    We ask if the set T=S. If so them T is closed, if not we are not finished taking the closure.\n",
    "    \"\"\"\n",
    "    C=string_to_set(T)-string_to_set(S) #T \\S # New elements\n",
    "    C={str(x) for x in C}    \n",
    "    # print(\"C\",C,f\"S term {S}\",f\"T term {T}\")\n",
    "    \n",
    "    #We are done\n",
    "    if len(C)==0: #Check if closed set \n",
    "        return T # return closed set\n",
    "    \n",
    "    #Not done yet.\n",
    "    if len(C)>0:\n",
    "        \"\"\"We then take the next partial-closure of T wrt Definion 5.1.4\"\"\"\n",
    "        U=next_j(T,f_dict) #next_j(T)\n",
    "        \n",
    "        #We then repeat this function.\n",
    "        Z=rec_j(U,T,f_dict)\n",
    "    # print(T)\n",
    "    return Z"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff016ba4-4814-4235-a72a-293d33021cb0",
   "metadata": {},
   "source": [
    "Finally the last function combines next_j() and rec_j() to give the closure of $I$ with respect to $g$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "40620542-7e0b-43e0-b977-b35ae35cba51",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_barj(J,f_dict):\n",
    "    \"\"\"\n",
    "    Objective: Gets the closure of J \\subseteq \\mathcal{P}_{n}^{+} with respect to f in \\tilde{P}_n.\n",
    "    Input:\n",
    "    J: is a element of the \\mathcal{P}_{n}^{+}\n",
    "    f_dict in \\tilde{P}_n.\n",
    "    \n",
    "    Returns:closure of J\n",
    "    \"\"\"\n",
    "    \n",
    "    j0=J\n",
    "    j1=next_j(j0,f_dict)\n",
    "    T=rec_j(j1,j0,f_dict)\n",
    "    return T"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3087c13-eb85-4c60-a6a7-a3b036c317ce",
   "metadata": {},
   "source": [
    "Note that we can use get_barj() to immediatley obtain all primitve closed sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0b67380d-6b75-46c0-9f63-cea443553201",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PC=[(x,get_barj({x},g)) for x in power_set_str]\n",
    "# dict_PC=dict(PC)\n",
    "# set_PC=[sorted_frozenset(get_barj({x},g)) for x in power_set_str]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df89135a-553d-4e6a-9384-456cc950a673",
   "metadata": {},
   "source": [
    "#### 3) Algorithim from Section $5.1$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96ea0ae9-205e-497c-aaf0-a213290ea482",
   "metadata": {},
   "source": [
    "Algorithm: Obtain $\\mathbb{E}(g)$\n",
    "\n",
    "**Input:** $g \\in \\tilde{P}_{n}$\n",
    "\n",
    "**Output:** $\\mathbb{E}(g)$\n",
    "\n",
    "1. **Initialization:** Set $\\mathbb{E}(g) = \\{\\emptyset\\}$\n",
    "\n",
    "2. Define a recursive function: Recursion($\\overline{I}, y$)\n",
    "\n",
    "    1. Let $\\overline{J} := \\overline{I} \\cup \\overline{\\{y\\}}$\n",
    "    2. Add $\\overline{J}$ to $\\mathbb{E}(g)$\n",
    "    3. For each $w \\in \\mathcal{P}_{n}^{+} \\setminus \\overline{J}$, do the following:\n",
    "        1. Call Recursion($\\overline{J}, w$)\n",
    "\n",
    "3. **End Function**\n",
    "\n",
    "4. For each $x \\in \\mathcal{P}_{n}^{+}$, do the following:\n",
    "    1. Let $I := \\{x\\}$\n",
    "    2. Add $\\overline{I}$ to $\\mathbb{E}(g)$\n",
    "    3. For each $y \\in \\mathcal{P}_{n}^{+} \\setminus \\overline{I}$, do the following:\n",
    "        1. Call Recursion($\\overline{I}, y$)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25b6df6e-a2d2-493b-9661-cbf83db11298",
   "metadata": {},
   "source": [
    "The following function is the recusive part of above algorithm, in particular from step 4.C onwards. We do this separate the recursive part from the outer for loop for simplicity, as primitive closed sets are easy to obtain and so is $\\mathcal{P}_{n}^{+}$ and $\\emptyset$ which are always in $\\mathbb{E}(g)$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a56ed216-940d-4cb6-adef-5cd4178cd014",
   "metadata": {},
   "source": [
    "To enhance efficiency and reduce computation time of step 4.C. (so that we do not need to iterate through the powerset completely each time) we've introduced:\n",
    "\n",
    "- **Memory:** We utilize the \"Memory\" variable as a memoization technique to store previous results, thus preventing redundant calculations.\n",
    "\n",
    "These stored results are referred to as \"indicators.\" Here's how it works:\n",
    "\n",
    "- Before initiating a new recursion with a specific set of indicators (which track the current state), we check if these indicators are already present in Memory. If they are, we skip the associated calculations, saving processing time.\n",
    "- If a set of indicators is not found in Memory, it signifies that our algorithm hasn't encountered this specific combination previously. Consequently, we proceed with the recursion and store this set of indicators in \"Memory.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "45743dd0-2e96-482e-a730-d5dd0f92b5ee",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def Recur(Ibar,indicators,Eg,Memory,dict_PC):\n",
    "    \"\"\"\n",
    "    Objective: Recursively generate Eg while preventing repeated calculations using Memory.\n",
    "\n",
    "    Input:\n",
    "    - Ibar (set): The set closure of I for some I \\subseteq \\mathcal{P}{n}^{+}\n",
    "    \n",
    "    - indicators (tuple): A tuple of elements of \\mathcal{P}{n}^{+} indicators \n",
    "    indicators store recursive paths we have taking that is \n",
    "    e.g when we take y in the complement of the closure of x. \n",
    "    So for the algorithim stated above example indicators would be (x,y) (x,y,w)\n",
    "    We write indicators in a standard ordered form to help memoization process\n",
    "\n",
    "    - Memory (set): A set used for memoization to prevent repeated calculations storing tuples of indicators\n",
    "\n",
    "    - Eg (set): Begins as the set of primitive closed sets and ends at the full set Eg\n",
    "    \n",
    "    - dict_PC (dict): A dictionary recording the element of p_n^{+} and the primitive closed sets.\n",
    "\n",
    "    Returns:\n",
    "    - Eg (set): The updated set of generated primitive closed sets.\n",
    "    - Memory (set): The updated set of indicators used for memoization.\n",
    "    \"\"\"\n",
    "    # global dict_PC\n",
    "        \n",
    "    # To save recalulating we uses this series of checks, if fails checks end recursion turn.\n",
    "    \n",
    "    if indicators in Memory:# Memortisation for preventing repeated calculations.\n",
    "        # print(\"Fails Memory check\")\n",
    "        # print(f\"\\n At this failure we have \\n Eg: {len(Eg)} \\n Memory: {len(Memory)} \\n\")\n",
    "        return Eg,Memory\n",
    "    \n",
    "    #Steps 2A and 2B of algo\n",
    "    \n",
    "    y=indicators[-1]\n",
    "    # print(f\"term taken for primitive closed set we are unioning {y}\")\n",
    "    P_y=dict_PC[y] # the primitive closed set wrt y\n",
    "    Jbar= Ibar.union(P_y)\n",
    "    \n",
    "    # End if alread have Jbar\n",
    "    \n",
    "    if len(Jbar)==len(power_set_str): # We always have the power set in Eg\n",
    "        # print(f\"Fails as {Jbar} == poweset\")\n",
    "        # print(f\"\\n At this failure we have \\n Eg: {len(Eg)} \\n Memory: {len(Memory)} \\n\")\n",
    "        return Eg,Memory\n",
    "              \n",
    "    if Jbar in Eg:\n",
    "        # print(f\"Fails as {Jbar} in Eg\")\n",
    "        # print(f\"\\n At this failure we have \\n Eg: {len(Eg)} \\n Memory: {len(Memory)} \\n\")\n",
    "        return Eg,Memory\n",
    "    \n",
    "    #Repeat recursion step\n",
    "    else:\n",
    "        # print(\"Passes Checks \\n\")\n",
    "        #Add the new data to memory\n",
    "        \n",
    "        set_indicators=set([sorted_frozenset(tuple(indicators)),])\n",
    "        Memory=Memory.union(set_indicators)\n",
    "        \n",
    "        Jbar_union=set([sorted_frozenset(Jbar),])#Correct format for union\n",
    "        #Add new data to Eg\n",
    "        Eg=Eg.union(Jbar_union) # will ge an issue with where defines and globals.\n",
    "\n",
    "        #Move onto the next recursion.\n",
    "        power_complment=set(power_set_str)-Jbar\n",
    "        for w in power_complment: # pick one in the complement to to continus exhaustive method\n",
    "            new_indicators=indicators+(w,) #want to add it at end\n",
    "            Eg,Memory=Recur(Jbar,new_indicators,Eg,Memory,dict_PC)\n",
    "            # print(f\"Done with {new_indicators} \\n\")\n",
    "                \n",
    "    return Eg,Memory # will also be an issue."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fce58699-7c75-48ea-956c-a9003fb91790",
   "metadata": {},
   "source": [
    "\n",
    "We now state the full algorithm. Note we obtain the primitive closed sets, $\\mathcal{P}_{n}^{+}$ and $\\emptyset$ separately."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9afc331b-1936-40ec-af67-c09cfe031ddd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_Eg(dict_PC,power_set_str,set_PC):\n",
    "    \"\"\"\n",
    "    Objective: Compute the set of all closed sets, Eg, using a given dictionary of primitive closed sets (dict_PC),\n",
    "               the power set of elements (power_set_str), and a set of primitive closed sets (set_PC).\n",
    "               \n",
    "               We do this by building up Eg using Recur()\n",
    "               \n",
    "               We deal with the empty set separatley.\n",
    "\n",
    "    Input:\n",
    "    - dict_PC (dict): A dictionary where keys are elements of the power set (as strings) \n",
    "    and values are primitive closed sets.\n",
    "    - power_set_str (list or set): The power set of elements represented as a list or set.\n",
    "    - set_PC (set): A set of primitive closed sets.\n",
    "\n",
    "    Returns:\n",
    "    - Eg (set): The set of all closed sets.\n",
    "    \"\"\"\n",
    "    \n",
    "    #We start with what we know which are the primitve closed sets.\n",
    "    Eg={frozenset(power_set_str)}.union(set_PC) \n",
    "    \n",
    "\n",
    "    Memory={sorted_frozenset((\"0\"))}    # contains indicators which are tuples of x \\in powerset for primitives sets.\n",
    "    \n",
    "    for x in power_set_str: #(Step 1 of Algorthim)\n",
    "        Ibar=dict_PC[x] #primitive closed set #(Step 2 of Algorthim)\n",
    "        power_complment=set(power_set_str)-Ibar # \\mathcal{P}_{n}^{+} \\setminus \\overline{I}\n",
    "        for y in power_complment: #exhaustively getting all closed sets. #(Step 4 of Algorthim)\n",
    "            \n",
    "            #We introduce indicators which records that we are taking y in the complement of the closure of x\n",
    "            indicators=(x,y)\n",
    "            set_indicators=set([sorted_frozenset(tuple(indicators)),])\n",
    "            \n",
    "            Memory=Memory.union(set_indicators)            \n",
    "            Eg,Memory=Recur(Ibar,indicators,Eg,Memory,dict_PC)\n",
    "            \n",
    "    return Eg"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "086df9f3-7dd1-4fc8-afb2-98cc167cdf30",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### 4) Example $\\mathbb{E}(g)$ for $g \\in  \\tilde{P}_{3}$ <a name=\"s32\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edb87b2b-7fd0-43ed-b906-112b9ee610aa",
   "metadata": {},
   "source": [
    "Here we obtain the example in Section $5.1$ (which can be done by hand). This section is here to provide a check that the code written so far agrees with the theory."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25ebad35-623f-4087-83d8-5c396e78dd2c",
   "metadata": {},
   "source": [
    "Let $g \\in \\tilde{P}_3$,"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06cfd558-4a06-44a2-a7f2-f236d9b9b82f",
   "metadata": {},
   "source": [
    "{'1': 0, '2': 0, '3': 0, '12': 0, '13': 0, '23': 0, '123': 0}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4859514-865a-4363-9f37-7e735dd1cfee",
   "metadata": {},
   "source": [
    "We now obtain all the primitive closed sets of $\\mathbb{E}(g)$ from a single function input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "11494e57-0536-4a18-8b0a-2a5c90acf027",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df=Pn for some n\n",
    "def get_primitive_for_single_g(df,numb):\n",
    "    \"\"\"\n",
    "    Objective:\n",
    "    Input:\n",
    "    Returns: \n",
    "    \n",
    "    set_PC: set of primitive closed set\n",
    "    dict_PC: dictionary with keys as terms of powerset^{+} with value as the primitive closed assoicated.\n",
    "    \"\"\"\n",
    "    #Inputs: df=Pn, numb used to get a specific function from df\n",
    "    \n",
    "    # Pick function in Pn\n",
    "    g=df.iloc[numb].to_dict()\n",
    "\n",
    "    #get primitive closed sets\n",
    "    PC=[(x,get_barj({x},g)) for x in power_set_str]\n",
    "    dict_PC=dict(PC)\n",
    "    set_PC=[sorted_frozenset(get_barj({x},g)) for x in power_set_str]\n",
    "\n",
    "\n",
    "    return dict_PC,set_PC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a2b55de0-c473-402e-be70-539ada4f9aeb",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'get_Pnp1' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[12], line 5\u001b[0m\n\u001b[0;32m      3\u001b[0m n\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m\n\u001b[0;32m      4\u001b[0m power_set_str,power_set_str_np1\u001b[38;5;241m=\u001b[39mget_n_np1_powersets(n)\n\u001b[1;32m----> 5\u001b[0m P3\u001b[38;5;241m=\u001b[39m\u001b[43mget_Pnp1\u001b[49m(P2,\u001b[38;5;241m2\u001b[39m)\n\u001b[0;32m      6\u001b[0m df\u001b[38;5;241m=\u001b[39mP3\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28mprint\u001b[39m(P3)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'get_Pnp1' is not defined"
     ]
    }
   ],
   "source": [
    "#Initalisation: get P3\n",
    "P2=pd.DataFrame([{'1': 0, '2': 0, '12': 0},{'1': 0, '2': 0, '12': 1}])\n",
    "n=2\n",
    "power_set_str,power_set_str_np1=get_n_np1_powersets(n)\n",
    "P3=get_Pnp1(P2,2)\n",
    "df=P3\n",
    "print(P3)\n",
    "#    1  2  3  12  13  23  123\n",
    "# 0  0  0  0   0   1   0    1\n",
    "# 1  0  0  0   0   1   1    1\n",
    "# 2  0  0  0   0   0   0    1\n",
    "# 3  0  0  0   0   0   1    1\n",
    "# 4  0  0  0   0   0   0    0\n",
    "# 5  0  0  0   1   1   0    1\n",
    "# 6  0  0  0   1   1   1    2\n",
    "# 7  0  0  0   1   0   1    1\n",
    "# 8  0  0  0   1   1   1    1\n",
    "# 9  0  0  0   1   0   0    1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "d610c3f8-e850-4b11-bbb0-33c4e8852dd2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# pick a function\n",
    "numb=9\n",
    "\n",
    "#Get the powerset for P3\n",
    "n=3\n",
    "power_set_str,power_set_str_np1=get_n_np1_powersets(n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "e5bdd846-f320-4da3-a0f6-9668d7f7e659",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'1': 0, '2': 0, '3': 0, '12': 1, '13': 0, '23': 0, '123': 1} \n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(P3.iloc[numb].to_dict(),\"\\n\")\n",
    "# {'1': 0, '2': 0, '3': 0, '12': 1, '13': 0, '23': 0, '123': 1} "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "7e423a75-32b1-45c5-bf58-4bac29ef1315",
   "metadata": {},
   "outputs": [],
   "source": [
    "#get primitive closed data.\n",
    "dict_PC,set_PC=get_primitive_for_single_g(df,numb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "217df5e0-9e17-420e-aab8-91dbeb688bd4",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'1': {'1', '13'},\n",
       " '2': {'2', '23'},\n",
       " '3': {'1', '123', '13', '2', '23', '3'},\n",
       " '12': {'1', '12', '123', '13', '2', '23'},\n",
       " '13': {'13'},\n",
       " '23': {'23'},\n",
       " '123': {'1', '123', '13', '2', '23'}}"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"\\n Primitive closed sets\")\n",
    "dict_PC\n",
    "\n",
    "# {'1': {'1', '13'},\n",
    "#  '2': {'2', '23'},\n",
    "#  '3': {'1', '123', '13', '2', '23', '3'},\n",
    "#  '12': {'1', '12', '123', '13', '2', '23'},\n",
    "#  '13': {'13'},\n",
    "#  '23': {'23'},\n",
    "#  '123': {'1', '123', '13', '2', '23'}}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "834c2c20-2755-4def-9ed8-cad966ef77cc",
   "metadata": {},
   "source": [
    "We now obtain the set $\\mathbb{E}(g)$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "120e2524-bb19-4b5a-99d0-0bcffa228c0b",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{frozenset(),\n",
       " frozenset({'13'}),\n",
       " frozenset({'1', '13'}),\n",
       " frozenset({'23'}),\n",
       " frozenset({'1', '13', '23'}),\n",
       " frozenset({'2', '23'}),\n",
       " frozenset({'13', '23'}),\n",
       " frozenset({'13', '2', '23'}),\n",
       " frozenset({'1', '13', '2', '23'}),\n",
       " frozenset({'1', '123', '13', '2', '23'}),\n",
       " frozenset({'1', '12', '123', '13', '2', '23'}),\n",
       " frozenset({'1', '123', '13', '2', '23', '3'}),\n",
       " frozenset({'1', '12', '123', '13', '2', '23', '3'})}"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Eg=get_Eg(dict_PC,power_set_str,set_PC)\n",
    "# Eg.append(\"empty\")\n",
    "Eg\n",
    "# {frozenset(),\n",
    "#  frozenset({'13'}),\n",
    "#  frozenset({'1', '13'}),\n",
    "#  frozenset({'23'}),\n",
    "#  frozenset({'1', '13', '23'}),\n",
    "#  frozenset({'2', '23'}),\n",
    "#  frozenset({'13', '23'}),\n",
    "#  frozenset({'13', '2', '23'}),\n",
    "#  frozenset({'1', '13', '2', '23'}),\n",
    "#  frozenset({'1', '123', '13', '2', '23'}),\n",
    "#  frozenset({'1', '12', '123', '13', '2', '23'}),\n",
    "#  frozenset({'1', '123', '13', '2', '23', '3'}),\n",
    "#  frozenset({'1', '12', '123', '13', '2', '23', '3'})}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9664edb3-de9d-4c1e-9377-10e276305990",
   "metadata": {
    "tags": [],
    "toc-hr-collapsed": true
   },
   "source": [
    "### Obtaining $\\tilde{P}_{n+1}$ from $\\mathbb{E}(g)$ from $g \\in \\tilde{P}_{n-1}$ <a name=\"s113\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bec77b8-7ae7-4b2f-8ab1-8a01f6285488",
   "metadata": {},
   "source": [
    "In this section  for  $g \\in \\tilde{P}_{n-1}$ and $\\mathbb{E}(g)$ we obtain the set $\\{f=g+\\epsilon_{I} | I \\in \\mathbb{E}(g)\\}$. We then do this for all $g \\in \\tilde{P}_{n-1}$ and take the union in order to state $\\tilde{P}_{n}$. The main function of this section is get_Pnp1(), which takes $\\tilde{P}_{n-1}$ and returns $\\tilde{P}_{n}$.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f58c6ba5-a0b5-4dec-b219-1ad9ec7787b1",
   "metadata": {},
   "source": [
    "Let $f=g+\\epsilon_{I}$ for some $I \\in \\mathbb{E}(g)$then $f(A \\cup \\{n+1\\})= g(A)+\\epsilon_{I}(A)$ for $A \\in \\mathcal{P}_{n-1}^{+}$. The following function allow us to obtain $A \\cup \\{n+1\\} \\in \\mathcal{P}_{n}^{+}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "404f5141-50c4-4686-98f2-8270c194059e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def add_n_p_1_to_string(string,n):\n",
    "    \"\"\"\n",
    "    Objective: Adds n+1 to the input string.\n",
    "\n",
    "    Input:\n",
    "    - string (str): The input string to which n+1 will be added.\n",
    "    - n (int): The number to be added to the string.\n",
    "\n",
    "    Returns:\n",
    "    - np1string (str): A new string formed by concatenating the input string and (n+1).\n",
    "    \"\"\"\n",
    "    #Adds n+1 to strings subsets\n",
    "    np1string=string+(f\"{n+1}\")\n",
    "    return np1string"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6e1659f-3e22-400c-b087-d503af4ee28a",
   "metadata": {},
   "source": [
    "Let $I \\in \\mathbb{E}(g)$ but $I\\ne \\emptyset$ (we consider $I= \\emptyset$ separately) the following function returns the set of all $f=g+\\epsilon_{I}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f1802047-e709-4048-a244-ee25e2201ca5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_extension_of_f(f_dict,extender,power_set_str,n):\n",
    "    \"\"\"\n",
    "    Objective:\n",
    "    Input:\n",
    "    Returns:\n",
    "    \"\"\"\n",
    "    # Need to attatch n+1 to strings to build extension of function by epsilon.\n",
    "\n",
    "    # Build the extension of f by epsilon.\n",
    "    \n",
    "    # Add the (key,values) of f_dict\n",
    "    builder_ext_0=[(k, f_dict[k]) for k in f_dict]\n",
    "\n",
    "    #We add 1 to the function value of f_dict for x in extender\n",
    "    builder_ext_p1=[(add_n_p_1_to_string(x,n),f_dict[x]+1) for x in list(extender)]\n",
    "\n",
    "    #We add 0 to the function value of f_dict for x NOT in extender\n",
    "    compl_extender=set(power_set_str) - set(extender)\n",
    "    builder_ext_p2=[(add_n_p_1_to_string(x,n),f_dict[x]+0) for x in list(compl_extender)]\n",
    "    \n",
    "    # We include f({n+1})=0\n",
    "    builder_ext_np1=[(add_n_p_1_to_string(\"\",n),int(0)),]\n",
    "\n",
    "\n",
    "    #Compile together.\n",
    "    extension_f_dict={**dict(builder_ext_0),**dict(builder_ext_p1),**dict(builder_ext_p2),**dict(builder_ext_np1)}\n",
    "    \n",
    "    #Example\n",
    "    # get_extension_of_f(frozenset({'13', '23'}))\n",
    "\n",
    "    return extension_f_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2468614-e598-4e42-9d71-c1a6fdcbae9e",
   "metadata": {},
   "source": [
    "Let $I= \\emptyset \\in \\mathbb{E}(g)$ the following function returns $f=g+\\epsilon_{I}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b7c29017-cebe-4aea-a579-43f4c06ce537",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def empty_case_get_extension_of_f(f_dict,extender,power_set_str,n):\n",
    "    \"\"\"\n",
    "    Objective:\n",
    "    Input:\n",
    "    Returns:\n",
    "    \"\"\"\n",
    "    # Need to attatch n+1 to strings to build extension of function by epsilon.\n",
    "\n",
    "    # Build the extension of f by epsilon.\n",
    "    \n",
    "    # Add the (key,values) of f_dict\n",
    "    builder_ext_0=[(k, f_dict[k]) for k in f_dict]\n",
    "\n",
    "    builder_ext_p2=[(add_n_p_1_to_string(x,n),f_dict[x]+0) for x in list(power_set_str)]\n",
    "\n",
    "    builder_ext_np1=[(add_n_p_1_to_string(\"\",n),int(0)),]\n",
    "\n",
    "    \n",
    "    #Compile together.\n",
    "    extension_f_dict={**dict(builder_ext_0),**dict(builder_ext_p2),**dict(builder_ext_np1)}\n",
    "    \n",
    "    return extension_f_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93fb6407-f06a-4cfc-ad2c-526d2e4d4103",
   "metadata": {},
   "source": [
    "The following function applies get_extension_of_f() and empty_case_get_extension_of_f() to $\\mathbb{E}(g)$ for each $g \\in \\tilde{P}_{n-1}$ and returns $\\tilde{P}_{n}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "df4d0e84-d129-4cae-aea5-e7f81a2b05d5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def Build_Pn_1(list_df_Pn,power_set_str,n):\n",
    "    \"\"\"\n",
    "    Objective:\n",
    "    Input:\n",
    "    Returns:\n",
    "    \"\"\"\n",
    "    # Returns:packets of extensions for each f_dict from Pn, can check those with max size of closed sets (later)\n",
    "    \n",
    "    # global power_set_str\n",
    "    \n",
    "    P_n1=[]\n",
    "    for f_dict in list_df_Pn:\n",
    "\n",
    "        # Store primitive closed sets \n",
    "\n",
    "        PC=[(x,get_barj({x},f_dict)) for x in power_set_str]\n",
    "        dict_PC=dict(PC)\n",
    "        set_PC=[sorted_frozenset(get_barj({x},f_dict)) for x in power_set_str]\n",
    "\n",
    "        #Get all closed sets for extensions for f_dict\n",
    "        Eg=list(get_Eg(dict_PC,power_set_str,set_PC))\n",
    "        Eg.append(\"empty\")\n",
    "\n",
    "        #Build extensions for f_dict\n",
    "        extension_of_f_dict=[]\n",
    "        for term in Eg:\n",
    "            if term ==\"empty\":\n",
    "                extension_of_f_dict.append(empty_case_get_extension_of_f(f_dict,term,power_set_str,n))\n",
    "            else:\n",
    "                extension_of_f_dict.append(get_extension_of_f(f_dict,term,power_set_str,n))\n",
    "\n",
    "        #We record packets of extensions where we take +1\n",
    "        P_n1.append(extension_of_f_dict)\n",
    "    \n",
    "    return P_n1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57a1e3d5-96a9-45cd-9c96-226c900f349b",
   "metadata": {},
   "source": [
    "The next function puts the previous information into a dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "94d04f4a-271e-4975-a610-d737d1b232a6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def extend_sets_to_Pnp1(P_n1):\n",
    "    \"\"\"\n",
    "    Objective:\n",
    "    Input:\n",
    "    Returns:\n",
    "    \"\"\"\n",
    "    #Return the data frame of Pn+1\n",
    "    \n",
    "    final_P_n1=[]\n",
    "    for ls in P_n1:\n",
    "        final_P_n1=final_P_n1+ls # Adding all the extension packets together.\n",
    "    df=pd.DataFrame(final_P_n1)\n",
    "    # df = df.reindex(columns=power_set_str_np1)\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "216a58a0-8edc-4668-8f42-98877e5cb006",
   "metadata": {},
   "source": [
    "Finally get_Pnp1() compiles this information together and allows one to obtain $\\tilde{P}_{n-1}$ from $\\tilde{P}_{n}$ in a single function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "61fe8504-7cbc-4214-b55c-f80ecf31e9e1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# This function returns Pn+1\n",
    "def get_Pnp1(Pn,n):\n",
    "    \"\"\"\n",
    "    Objective:\n",
    "    Input:\n",
    "    Returns:\n",
    "    \"\"\"\n",
    "\n",
    "    #inputs\n",
    "    # Pn and n.\n",
    "\n",
    "    #New data created\n",
    "    np1=n+1\n",
    "    power_set_str,power_set_str_np1=get_n_np1_powersets(n)\n",
    "\n",
    "    #Load Pn as a list of dictionaries.\n",
    "    list_df_Pn=Pn.to_dict(orient='records')\n",
    "\n",
    "    #Create Pn+1 and hold as dataframe.\n",
    "    P_n1=Build_Pn_1(list_df_Pn,power_set_str,n)\n",
    "    Pnp1=extend_sets_to_Pnp1(P_n1)\n",
    "\n",
    "    Pnp1 = Pnp1.reindex(columns=power_set_str_np1)\n",
    "\n",
    "    inspect_Pn1(Pnp1)\n",
    "\n",
    "    return Pnp1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46375138-ad0e-4e55-aa80-b17daab3b5d4",
   "metadata": {
    "tags": [],
    "toc-hr-collapsed": true
   },
   "source": [
    "### Analysis of $\\tilde{P}_{n}$<a name=\"s114\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a9d71fb-7315-4419-bef9-affd55b192cd",
   "metadata": {},
   "source": [
    "The following function returns $|\\tilde{P}_n|$ and determines whether $\\tilde{P}_n$ is a set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "67ae9795-f32f-4459-9c28-fdd78fe4c676",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def inspect_Pn1(df):\n",
    "    \"\"\"\n",
    "    Objective:\n",
    "    Input:\n",
    "    Returns:\n",
    "    \"\"\"\n",
    "    print(\"Number of functions:\",len(df.index))\n",
    "    # df=df.unique()\n",
    "\n",
    "    duplicate_rows = df[df.duplicated()]\n",
    "    print(\"Are there duplicates?\",duplicate_rows.shape, \"if (0,-) then no common rows\")\n",
    "\n",
    "    # df.head()\n",
    "    return"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03a97b17-b29b-425e-9f65-0b747062b6d5",
   "metadata": {},
   "source": [
    "The following functions allow one to check every $f \\in \\tilde{P}_{n}$ are mildly super additive."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "6b65d1bb-b8a3-44d8-9b01-ae02fb506363",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def is_valid_function(f,power_set_str):\n",
    "    \"\"\"\n",
    "    Objective: Check if a given function satisfies the properties of a MSA condition\n",
    "    \n",
    "    Input:\n",
    "    f (dict): A dictionary representing the function, where keys are sets in power_set_str and values are integers.\n",
    "    power_set_str (list of str): A list of string representations of sets.\n",
    "    \n",
    "    Returns:\n",
    "    bool: True if the function satisfies the MSA properties, False otherwise.\n",
    "    \"\"\"\n",
    "    # define the condition to check for each function\n",
    "    Pn=[string_to_set(x) for x in power_set_str] #convert to sets\n",
    "    \n",
    "    # f(T) must equal 0 for elements T in Pn with size 1\n",
    "\n",
    "    for T in Pn:\n",
    "        if any(f[set_to_string(T)] != 0 for T in Pn if len(T) == 1):\n",
    "            return False\n",
    "    # for any I, J in Pn such that I and J have no common elements, 1 > f(I U J) - f(I) - f(J) or 0 = f(I U J) - f(I) - f(J)\n",
    "    for I in Pn:\n",
    "        for J in Pn:\n",
    "            if len(I.intersection(J)) == 0 and f[set_to_string(I.union(J))] - f[set_to_string(I)] - f[set_to_string(J)] < 0 or f[set_to_string(I.union(J))] - f[set_to_string(I)] - f[set_to_string(J)] > 1:\n",
    "            # for any I, J in Pn such that I and J have no common elements, 0 <= f(I U J) - f(I) - f(J) <= 1\n",
    "                return False\n",
    "    return True\n",
    "\n",
    "def check_functs_MSA(df,power_set_str):\n",
    "    \"\"\"\n",
    "    Objective: Check a DataFrame of functions for MSA compliance.\n",
    "    \n",
    "    Input:\n",
    "    df (pd.DataFrame): A DataFrame where each row represents a function as a dictionary.\n",
    "    power_set_str (list of str): A list of string representations of sets.\n",
    "    \n",
    "    Returns:\n",
    "    pd.DataFrame: A DataFrame containing the functions that do not satisfy MSA properties.\n",
    "    \"\"\"\n",
    "\n",
    "    num_rows=df.shape[0]\n",
    "\n",
    "    bad_f_in_df=[] #indices of functions fail to be MSA in df\n",
    "    for i in range(0,num_rows):\n",
    "        f=df.loc[i].to_dict()\n",
    "        if  is_valid_function(f,power_set_str)==False:\n",
    "            bad_f_in_df.append(f)\n",
    "    \n",
    "    bad_df=pd.DataFrame(bad_f_in_df)\n",
    "    \n",
    "    return bad_df #indices of functions that are not MSA"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d552dcec-7edf-4984-86c8-f73b6d5f4291",
   "metadata": {
    "tags": [],
    "toc-hr-collapsed": true
   },
   "source": [
    "## Cases <a name=\"s12\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50d8cc6f-1dc1-480c-ace4-143801cb223f",
   "metadata": {},
   "source": [
    "We now determine $\\tilde{P}_{n}$ by $\\tilde{P}_{n-1}$ for $n \\le 5$ recursively using the get_Pnp1() described in Section \"Functions\","
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f323cfe-d650-4f13-858a-26ba3ad73953",
   "metadata": {
    "tags": [],
    "toc-hr-collapsed": true
   },
   "source": [
    "### $\\tilde{P}_{3}$ by $\\tilde{P}_{2}$<a name=\"s121\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31a0826b-5d87-4539-b7ad-9171df2a0908",
   "metadata": {},
   "source": [
    "It is clear that $\\tilde{P}_{2}$ consists of exactly two functions. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "a3b210af-29e7-49ec-b1cf-e6aa08813037",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of functions: 10\n",
      "Are there duplicates? (0, 7) if (0,-) then no common rows\n",
      "   1  2  3  12  13  23  123\n",
      "0  0  0  0   0   0   1    1\n",
      "1  0  0  0   0   1   1    1\n",
      "2  0  0  0   0   1   0    1\n",
      "3  0  0  0   0   0   0    1\n",
      "4  0  0  0   0   0   0    0\n",
      "5  0  0  0   1   1   1    2\n",
      "6  0  0  0   1   1   0    1\n",
      "7  0  0  0   1   1   1    1\n",
      "8  0  0  0   1   0   1    1\n",
      "9  0  0  0   1   0   0    1\n"
     ]
    }
   ],
   "source": [
    "#Easy to know $P_2$\n",
    "P2=pd.DataFrame([{'1': 0, '2': 0, '12': 0},{'1': 0, '2': 0, '12': 1}])\n",
    "\n",
    "n=2\n",
    "power_set_str,power_set_str_np1=get_n_np1_powersets(n)\n",
    "P3=get_Pnp1(P2,2)\n",
    "print(P3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c403d0a-8acc-481d-bc27-bc942375e9c4",
   "metadata": {
    "tags": [],
    "toc-hr-collapsed": true
   },
   "source": [
    "### $\\tilde{P}_{4}$ by $\\tilde{P}_{3}$<a name=\"s122\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "6c271174-ad92-4738-b34b-29234e062c8d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of functions: 154\n",
      "Are there duplicates? (0, 15) if (0,-) then no common rows\n"
     ]
    }
   ],
   "source": [
    "n=3\n",
    "power_set_str,power_set_str_np1=get_n_np1_powersets(n)\n",
    "P4=get_Pnp1(P3,3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58495063-b7c8-40bf-995e-5de3b4548245",
   "metadata": {
    "tags": [
     "p3"
    ],
    "toc-hr-collapsed": true
   },
   "source": [
    "All the functions in P4 are MSA? Yes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "ad7f30ea-6127-4071-8e49-cbccd6a74066",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0, 0)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "power4=get_powerset_str(4) #powerset on 4 elements\n",
    "check_functs_MSA(P4,power4).shape\n",
    "#Yes (0, 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c99dd4c3-6ed0-40a1-85d6-5e1165e07cf4",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### $\\tilde{P}_{5}$ by $\\tilde{P}_{4}$<a name=\"s123\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "b75fd0fa-23f4-4ae6-893a-533692d9c75d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of functions: 10334\n",
      "Are there duplicates? (0, 31) if (0,-) then no common rows\n"
     ]
    }
   ],
   "source": [
    "n=4\n",
    "power_set_str,power_set_str_np1=get_n_np1_powersets(n) #gives powerset on 4 and 5 elements respectively\n",
    "P5=get_Pnp1(P4,4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "923a32b6-d5c9-4def-96cc-445206895206",
   "metadata": {},
   "source": [
    "We check if all functions are MSA: Yes they are"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "6eba7f17-0273-4bbd-aaaf-4740da3d6331",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0, 0)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# power5=get_powerset_str(5) #powerset on 4 elements\n",
    "# check_functs_MSA(P5,power5).shape\n",
    "# #Yes (0, 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a569ffb3-9104-4903-8fa5-8282e768eac0",
   "metadata": {
    "tags": [],
    "toc-hr-collapsed": true
   },
   "source": [
    "# Decomposition of $|\\tilde{P}_n|$ for $n \\le 5$  <a name=\"s2\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72a01b82-8fdb-4ff2-83ed-db2f4dc41df8",
   "metadata": {},
   "source": [
    "As stated in the thesis we now give the sequences $e_{n}$ and $d_{n}$ and state $p_{n+1}=e_n \\cdot d_n$. To do so for each  $g \\in \\tilde{P}_{n-1}$ we record $|\\mathbb{E}(g)|$ when we construct $\\tilde{P}_{n}$ from $\\tilde{P}_{n-1}$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2f68e18-305b-4280-8b30-0decda313fa9",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Functions <a name=\"s21\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cec7cfce-1538-4411-8218-a5ff3e5c1da5",
   "metadata": {},
   "source": [
    "The following function gets $\\mathbb{E}(g)$ similar to get_primitive_for_single_g(df,numb)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "e8b32f80-cb9a-4f25-baa1-ab28f589a190",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_Eg_for_single_g(df,numb,power_set_str):\n",
    "\n",
    "    #Obj: puts Eg into correct format to check if topolgoy\n",
    "    #Inputs: df=Pn, numb used to get a specific function from df\n",
    "    \n",
    "    # Pick function in Pn\n",
    "    g=df.iloc[numb].to_dict()\n",
    "\n",
    "    #get primitive closed sets\n",
    "    PC=[(x,get_barj({x},g)) for x in power_set_str]\n",
    "    dict_PC=dict(PC)\n",
    "    set_PC=[sorted_frozenset(get_barj({x},g)) for x in power_set_str]\n",
    "\n",
    "    #Get all closed sets for extensions for g\n",
    "    Eg=list(get_Eg(dict_PC,power_set_str,set_PC))\n",
    "    Eg.append(frozenset()) #rembmer the empty set\n",
    "    return Eg"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e27617f2-cabf-48f6-a2d5-cbda72d88ab9",
   "metadata": {},
   "source": [
    "The following function is similar to get_Eg_for_single_g but instead of specifying a function of Pn with Pn:=df and numb to select position, we take function as a dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "fd166050-336f-44ef-b757-a6ff5934fbea",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_Eg_for_single_g_dfisfunct(funct,power_set_str):\n",
    "    # print(power_set_str)\n",
    "    #Obj: puts Eg into correct format to check if topolgoy\n",
    "    #Inputs: df=Pn, numb used to get a specific function from df\n",
    "    \n",
    "    # Pick function in Pn\n",
    "    g=funct\n",
    "    \n",
    "    # print(power_set_str)\n",
    "\n",
    "    #get primitive closed sets\n",
    "    PC=[(x,get_barj({x},g)) for x in power_set_str]\n",
    "    dict_PC=dict(PC)\n",
    "    set_PC=[sorted_frozenset(get_barj({x},g)) for x in power_set_str]\n",
    "    # print(\"len set_PC\",len(set_PC))\n",
    "    \n",
    "    #Get all closed sets for extensions for g\n",
    "    Eg=list(get_Eg(dict_PC,power_set_str,set_PC))\n",
    "    Eg.append(frozenset()) #rembmer the empty set\n",
    "    return Eg"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c6fe4dd-f23a-4401-a272-9c7993902302",
   "metadata": {},
   "source": [
    "The following function allows us to record the size of $\\mathbb{E}(g)$ for each $f \\in \\tilde{P}_{n-1}$ by adding the key:value pair of the form 'Eg_Size':$|\\mathbb{E}(g)|$ to each $f=g+\\epsilon_{I} \\in \\tilde{P}_{n}$. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "2bfa801d-8f12-4685-b40e-05ccd53416d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def eg_column_Build_Pn_1(list_df_Pn,power_set_str,n):\n",
    "    \n",
    "    \"\"\"\n",
    "    Obj:Use Build_Pn_1 and modify change to include Eg size column in functions\n",
    "    \n",
    "    Return:\n",
    "    P_n1: is a list of dataframes considing with f in P_{n+1} and \n",
    "    we record the size of E(g) of the g in P_{n-1} which extends to this f\n",
    "    \"\"\"\n",
    "    \n",
    "    # Returns:packets of extensions for each f_dict from Pn, can check those with max size of closed sets (later)\n",
    "    \n",
    "    P_n1=[]\n",
    "    for index,f_dict in enumerate(list_df_Pn):\n",
    "\n",
    "        # Store primitive closed sets \n",
    "        PC=[(x,get_barj({x},f_dict)) for x in power_set_str]\n",
    "        dict_PC=dict(PC)\n",
    "        set_PC=[sorted_frozenset(get_barj({x},f_dict)) for x in power_set_str]\n",
    "\n",
    "        #Get all closed sets for extensions for f_dict\n",
    "        Eg=list(get_Eg(dict_PC,power_set_str,set_PC))\n",
    "        Eg.append(\"empty\")\n",
    "        \n",
    "        Eg_col_kvpair={\"Eg_Size\":len(Eg)}\n",
    "\n",
    "        #Build extensions for f_dict\n",
    "        extension_of_f_dict=[]\n",
    "        for term in Eg:\n",
    "            if term ==\"empty\":                    \n",
    "                funct=empty_case_get_extension_of_f(f_dict,term,power_set_str,n) #dictionary\n",
    "                funct.update(Eg_col_kvpair)\n",
    "                extension_of_f_dict.append(funct)\n",
    "            else:\n",
    "                funct=get_extension_of_f(f_dict,term,power_set_str,n)\n",
    "                funct.update(Eg_col_kvpair)\n",
    "                extension_of_f_dict.append(funct)\n",
    "\n",
    "        #We record packets of extensions where we take +1\n",
    "        P_n1.append(extension_of_f_dict)\n",
    "    \n",
    "    return P_n1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15b4df1c-2dd2-44f9-bc08-9506fed83867",
   "metadata": {},
   "source": [
    "The following function take the previous list of dataframes for $\\tilde{P}_{n}$ with the column 'Eg_Size':$|\\mathbb{E}(g)|$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "3056b840-f5f1-4019-ae1c-d4d12a8000a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_Eg_column_Pn(df):\n",
    "    df=df.copy()\n",
    "    \n",
    "    #Get Eg for the row in df.\n",
    "\n",
    "    # Create a new column called \"new_column\"\n",
    "    #--------\n",
    "    power_set_str_n=get_powerset_str(n+1)\n",
    "    # print(\"Power\",power_set_str_n)\n",
    "    # print(\"Current Pn-1 \\n\",df)\n",
    "\n",
    "    # Insert data into the new column\n",
    "    for i, row in df.iterrows():   \n",
    "        funct=dict(row)\n",
    "        funct.pop('Eg_Size', None)\n",
    "        # print(funct)\n",
    "        Eg=get_Eg_for_single_g_dfisfunct(funct,power_set_str_n)\n",
    "        # print(Eg)\n",
    "        df.at[i, 'Eg_Size'] = int(len(Eg))\n",
    "        # print(len(Eg))\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56c204df-e093-44e2-8956-34780cd7945d",
   "metadata": {},
   "source": [
    "We repeat the above process but for the dataframe consisting of a subset of $\\tilde{P}_{n-1}$. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "147ba091-ba60-47bc-a6db-60b48788f016",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_Pnp1_for_single_funct(Pn,n):\n",
    "    \n",
    "    #inputs\n",
    "    # Pn:dataframe and n.\n",
    "    \n",
    "    \"\"\"\n",
    "    Returns a dataframe of extensions in Pn+1 for dataframe of functions in Pn.\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    #Example\n",
    "    n=3\n",
    "    power_set_str,power_set_str_np1=get_n_np1_powersets(n)\n",
    "    # P3\n",
    "\n",
    "    funct_list=[{'1': 0,'2': 0,\"3\":0,\"12\":0,\"23\":0,\"13\":0,\"123\":0}]\n",
    "    Pn=pd.DataFrame(funct_list)\n",
    "\n",
    "    #The following should be functions in P4 that are extendions of functions in funct_list\n",
    "    f_P3=get_Pnp1_for_single_funct(Pn,n)\n",
    "    print(f_P3)\n",
    "    \"\"\"\n",
    "\n",
    "    #New data created\n",
    "    power_set_str,power_set_str_np1=get_n_np1_powersets(n)\n",
    "\n",
    "    #Load Pn as a list of dictionaries.\n",
    "    list_df_Pn=Pn.to_dict(orient='records')\n",
    "\n",
    "    #Create Pn+1 and hold as dataframe.\n",
    "    P_n1=eg_column_Build_Pn_1(list_df_Pn,power_set_str,n)\n",
    "    Pnp1=extend_sets_to_Pnp1(P_n1) #<---- ISSUE\n",
    "    \n",
    "    # power_set_str_np1\n",
    "    \n",
    "    columns=list(power_set_str_np1)+[\"Eg_Size\"]\n",
    "    Pnp1 = Pnp1.reindex(columns=columns)\n",
    "\n",
    "\n",
    "    inspect_Pn1(Pnp1)\n",
    "\n",
    "    return Pnp1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e4709a2-cbcb-4277-9b6d-128443bbdc18",
   "metadata": {},
   "source": [
    "We give an application of the function get_Pnp1_for_single_funct() in the following example. Consider a function in $\\tilde{P}_{3}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "id": "78350656-c698-4c52-839b-666580faf4fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of functions: 10\n",
      "Are there duplicates? (0, 7) if (0,-) then no common rows\n",
      "   1  2  3  12  13  23  123\n",
      "0  0  0  0   0   1   0    1\n",
      "1  0  0  0   0   1   1    1\n",
      "2  0  0  0   0   0   0    1\n",
      "3  0  0  0   0   0   1    1\n",
      "4  0  0  0   0   0   0    0\n",
      "5  0  0  0   1   1   0    1\n",
      "6  0  0  0   1   1   1    2\n",
      "7  0  0  0   1   0   1    1\n",
      "8  0  0  0   1   1   1    1\n",
      "9  0  0  0   1   0   0    1\n"
     ]
    }
   ],
   "source": [
    "#Get P3\n",
    "# n=3\n",
    "# power_set_str,power_set_str_np1=get_n_np1_powersets(n)\n",
    "\n",
    "P2=pd.DataFrame([{'1': 0, '2': 0, '12': 0},{'1': 0, '2': 0, '12': 1}])\n",
    "n=2\n",
    "power_set_str,power_set_str_np1=get_n_np1_powersets(n)\n",
    "P3=get_Pnp1(P2,2)\n",
    "print(P3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "21457b30-275e-414b-a82c-7e627073c368",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The functions we are extending\n",
      "   1  2  3  12  23  13  123\n",
      "0  0  0  0   0   0   0    0\n"
     ]
    }
   ],
   "source": [
    "# You can take subsets of rows as a database, then extend. Better than below\n",
    "funct_list=[{'1': 0,'2': 0,\"3\":0,\"12\":0,\"23\":0,\"13\":0,\"123\":0}]\n",
    "Pn=pd.DataFrame(funct_list)\n",
    "print(\"The functions we are extending\")\n",
    "print(Pn)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79ee069b-89e4-44f1-b43a-c50442cd9ff9",
   "metadata": {},
   "source": [
    "We see that there are $19=|\\mathbb{E}(g)|$ extensions of this function in $\\tilde{P}_{4}$. Do not conflate Eg_size in the dataframe below with how we $e_n$ and $d_n$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "bfbf2679-784a-4998-a201-426906eabac7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of functions: 19\n",
      "Are there duplicates? (0, 16) if (0,-) then no common rows\n",
      "    1  2  3  4  12  13  14  23  24  34  123  124  134  234  1234  Eg_Size\n",
      "0   0  0  0  0   0   0   0   0   0   0    0    1    1    0     1       19\n",
      "1   0  0  0  0   0   0   0   0   1   0    0    1    0    1     1       19\n",
      "2   0  0  0  0   0   0   1   0   1   1    0    1    1    1     1       19\n",
      "3   0  0  0  0   0   0   0   0   0   0    0    1    0    0     1       19\n",
      "4   0  0  0  0   0   0   1   0   1   0    0    1    1    1     1       19\n",
      "5   0  0  0  0   0   0   0   0   0   0    0    0    1    0     1       19\n",
      "6   0  0  0  0   0   0   0   0   0   1    0    1    1    1     1       19\n",
      "7   0  0  0  0   0   0   0   0   1   1    0    1    1    1     1       19\n",
      "8   0  0  0  0   0   0   0   0   0   0    0    0    0    1     1       19\n",
      "9   0  0  0  0   0   0   0   0   0   0    0    0    1    1     1       19\n",
      "10  0  0  0  0   0   0   0   0   0   0    0    1    0    1     1       19\n",
      "11  0  0  0  0   0   0   0   0   0   0    0    1    1    1     1       19\n",
      "12  0  0  0  0   0   0   1   0   0   0    0    1    1    1     1       19\n",
      "13  0  0  0  0   0   0   0   0   0   1    0    0    1    1     1       19\n",
      "14  0  0  0  0   0   0   0   0   1   0    0    1    1    1     1       19\n",
      "15  0  0  0  0   0   0   1   0   0   1    0    1    1    1     1       19\n",
      "16  0  0  0  0   0   0   1   0   0   0    0    1    1    0     1       19\n",
      "17  0  0  0  0   0   0   0   0   0   0    0    0    0    0     1       19\n",
      "18  0  0  0  0   0   0   0   0   0   0    0    0    0    0     0       19\n"
     ]
    }
   ],
   "source": [
    "n=3\n",
    "power_set_str,power_set_str_np1=get_n_np1_powersets(n)\n",
    "#The following should be functions in P4 that are extendions of functions in funct_list\n",
    "f_P3=get_Pnp1_for_single_funct(Pn,n)\n",
    "print(f_P3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "619c0d5d-db86-4209-bd70-1d2cd5d3ad5f",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Cases <a name=\"s22\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72a4ad39-b8c7-4218-84fc-db34a27dab24",
   "metadata": {},
   "source": [
    "Recall the following.\n",
    "\n",
    "\n",
    "Let $(p_{n})_{n \\in \\mathbb{N}}$ denote the sequence of terms $p_{n}:=|\\tilde{P}_n|$. Let $\\boldsymbol{e}_{n}:=(e_{n,i})$ denote the increasing sequence of distinct terms $|\\mathbb{E}(g)|$ for $g \\in \\tilde{P}_{n}$, and $\\boldsymbol{d}_{n}:=(d_{n,i})$ denote the sequence where $d_{n,i}$ denotes the number of $g \\in \\tilde{P}_{n}$ which give the value $e_{n,i}$.\n",
    "\n",
    "  $$p_{n+1} =\\boldsymbol{e}_n \\cdot \\boldsymbol{d}_{n}=\\sum e_{n,i}\n",
    " \\times d_{n,i}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c1ff652-f5c2-40d6-8ef0-751df2b1e3c8",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Decomposition of $|\\tilde{P}_3|$<a name=\"s221\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93be86db-8138-498c-a18b-9f4b7ff94b58",
   "metadata": {},
   "source": [
    "We consider the extension of $\\tilde{P}_{2}$ to $\\tilde{P}_{3}$ and record the size of each $|\\mathbb{E}(g)|$ for $ \\in \\tilde{P}_{2}$. We then obtain the sequences $e_{2}$ and $d_{2}$ (possiblly not ordered by increasing size), and state\n",
    "\n",
    "$$p_3=e_2 \\cdot d_2$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58741dd8-66db-4988-af79-e9d3f5714799",
   "metadata": {},
   "source": [
    "First get $\\tilde{P}_2$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "id": "e0867ff0-577e-4342-951c-6056e0e5f2fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "n=1\n",
    "power_set_str,power_set_str_np1=get_n_np1_powersets(n)\n",
    "P2=pd.DataFrame([{'1': 0, '2': 0, '12': 0},{'1': 0, '2': 0, '12': 1}])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "id": "f67a7d81-49d8-493d-adec-46c7a5ded6e8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "power_set_str=get_powerset_str(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "id": "b76cecd6-98e2-4107-a8eb-b566f1a6d047",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "e_n : d_n\n",
      "5.0    2\n",
      "Name: Eg_Size, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "P2_eg=get_Eg_column_Pn(P2)\n",
    "P2_eg=P2_eg.sort_values('Eg_Size') \n",
    "\n",
    "df=P2_eg\n",
    "print(f\"e_n : d_n\")\n",
    "print(df['Eg_Size'].value_counts()) #count number of 13s and 19s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "id": "1bccdc5d-f781-4b96-8bf1-974a8c65ee33",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   1  2  12  Eg_Size\n",
      "0  0  0   0      5.0\n",
      "1  0  0   1      5.0\n"
     ]
    }
   ],
   "source": [
    "print(P2_eg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62e2271e-9bad-45af-b77f-07ca442bcc17",
   "metadata": {
    "tags": []
   },
   "source": [
    "The decomposition of $p_3=5\\cdot1 +5 \\cdot 1$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c6e8399-9308-42a8-b921-876f29145111",
   "metadata": {
    "tags": [],
    "toc-hr-collapsed": true
   },
   "source": [
    "### Decomposition of $|\\tilde{P}_4|$<a name=\"s222\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ba24b8b-c571-417b-84d4-e5a4f755f7d3",
   "metadata": {},
   "source": [
    "We consider the extension of $\\tilde{P}_{3}$ to $\\tilde{P}_{4}$ and record the size of each $|\\mathbb{E}(g)|$ for $ \\in \\tilde{P}_{3}$. We then obtain the sequences $e_{3}$ and $d_{3}$  and state\n",
    "\n",
    "$$p_4=e_3 \\cdot d_3.$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e5142c4-163f-4a41-a6a4-873b35af2bd3",
   "metadata": {},
   "source": [
    "First get $\\tilde{P}_3$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "id": "eab323b8-69ed-4307-8a8f-a935bf4b2b3b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of functions: 10\n",
      "Are there duplicates? (0, 7) if (0,-) then no common rows\n",
      "   1  2  3  12  13  23  123\n",
      "0  0  0  0   0   1   0    1\n",
      "1  0  0  0   0   1   1    1\n",
      "2  0  0  0   0   0   0    1\n",
      "3  0  0  0   0   0   1    1\n",
      "4  0  0  0   0   0   0    0\n",
      "5  0  0  0   1   1   0    1\n",
      "6  0  0  0   1   1   1    2\n",
      "7  0  0  0   1   0   1    1\n",
      "8  0  0  0   1   1   1    1\n",
      "9  0  0  0   1   0   0    1\n"
     ]
    }
   ],
   "source": [
    "P2=pd.DataFrame([{'1': 0, '2': 0, '12': 0},{'1': 0, '2': 0, '12': 1}])\n",
    "n=2\n",
    "power_set_str,power_set_str_np1=get_n_np1_powersets(n)\n",
    "P3=get_Pnp1(P2,2)\n",
    "print(P3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "id": "52d636f4-31dd-45fe-ab81-b6cea22aa8ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "e_n : d_n\n",
      "13.0    6\n",
      "19.0    4\n",
      "Name: Eg_Size, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "power_set_str=get_powerset_str(3)\n",
    "\n",
    "P3_eg=get_Eg_column_Pn(P3)\n",
    "P3_eg=P3_eg.sort_values('Eg_Size') # get those 13,then 19\n",
    "\n",
    "df=P3_eg\n",
    "print(f\"e_n : d_n\")\n",
    "print(df['Eg_Size'].value_counts()) #count number of 13s and 19s"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14be0ff2-dfd1-4138-8770-c043590658b2",
   "metadata": {},
   "source": [
    "In particular $p_4 = 13 \\cdot 6 + 19 \\cdot 4$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a69e0b9-5ecd-430b-ad37-b3441d06e314",
   "metadata": {},
   "source": [
    "For example we can extract exactly all those function which have $|\\mathbb{E}(g)|=13$ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "id": "cb82c095-b193-47be-a508-face0a06bf98",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   1  2  3  12  13  23  123  Eg_Size\n",
      "0  0  0  0   0   1   0    1     13.0\n",
      "1  0  0  0   0   1   1    1     13.0\n",
      "3  0  0  0   0   0   1    1     13.0\n",
      "5  0  0  0   1   1   0    1     13.0\n",
      "7  0  0  0   1   0   1    1     13.0\n",
      "9  0  0  0   1   0   0    1     13.0\n",
      "(6, 8)\n"
     ]
    }
   ],
   "source": [
    "y=P3_eg[P3_eg.Eg_Size == 13]\n",
    "print(y)\n",
    "print(y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cc6df3e-0b49-4cd0-8d8c-e39170348d6d",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Remark"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b6f3efa-66ba-4c40-9c98-e705863064c1",
   "metadata": {},
   "source": [
    "Similar to section \"4) Example $\\mathbb{E}(g)$ for $g \\in  \\tilde{P}_{3}$\" we can obtain $E(g)$ for a fixed function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "6876349f-73fa-4ad4-aa97-ee6e4993205d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'1': 0, '2': 0, '3': 0, '12': 0, '13': 0, '23': 1, '123': 1} \n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[frozenset({'1', '12', '123', '13', '2', '23', '3'}),\n",
       " frozenset({'13'}),\n",
       " frozenset({'12', '123', '13', '2', '3'}),\n",
       " frozenset({'13', '3'}),\n",
       " frozenset({'12', '2'}),\n",
       " frozenset({'12', '123', '13', '2', '23', '3'}),\n",
       " frozenset({'12'}),\n",
       " frozenset({'12', '13'}),\n",
       " frozenset({'12', '13', '3'}),\n",
       " frozenset({'1', '12', '123', '13', '2', '3'}),\n",
       " frozenset({'12', '13', '2'}),\n",
       " frozenset({'12', '13', '2', '3'}),\n",
       " frozenset()]"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Get Eg for the function in P3\n",
    "#3  0  0  0   0   0   0    1      19\n",
    "numb =3\n",
    "df=P3\n",
    "g=df.iloc[numb].to_dict()\n",
    "print(g,\"\\n\")\n",
    "\n",
    "EEg_example=get_Eg_for_single_g(df,numb,power_set_str)\n",
    "EEg_example"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "500b3eb8-c6b2-4988-81aa-71d9c3f4de90",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### Decomposition of $|\\tilde{P}_5|$<a name=\"s223\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9cbfc75-7579-4851-8817-eef763733a9f",
   "metadata": {},
   "source": [
    "We consider the extension of $\\tilde{P}_{4}$ to $\\tilde{P}_{5}$ and record the size of each $|\\mathbb{E}(g)|$ for $ \\in \\tilde{P}_{4}$. We then obtain the sequences $e_{4}$ and $d_{4}$ (possiblly not ordered by increasing size), and state\n",
    "\n",
    "$$p_5=e_4 \\cdot d_4$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2bb7f98-fc01-4d4f-9a0a-375fac8ab532",
   "metadata": {},
   "source": [
    "First get $\\tilde{P}_4$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "0cf9b4f8-a757-4370-9968-7a6e0239faa3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of functions: 154\n",
      "Are there duplicates? (0, 15) if (0,-) then no common rows\n"
     ]
    }
   ],
   "source": [
    "n=3\n",
    "power_set_str,power_set_str_np1=get_n_np1_powersets(n)\n",
    "P4=get_Pnp1(P3,3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "4eca9027-6926-4bfa-9240-12197938febb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "e_n : d_n\n",
      "42.0     24\n",
      "45.0     24\n",
      "47.0     24\n",
      "82.0     16\n",
      "46.0     12\n",
      "54.0     12\n",
      "111.0    12\n",
      "99.0     10\n",
      "69.0      8\n",
      "133.0     8\n",
      "167.0     4\n",
      "Name: Eg_Size, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "power_set_str=get_powerset_str(4)\n",
    "\n",
    "P4_eg=get_Eg_column_Pn(P4)\n",
    "P4_eg=P4_eg.sort_values('Eg_Size') # get those 13,then 19\n",
    "\n",
    "\n",
    "df=P4_eg\n",
    "print(f\"e_n : d_n\")\n",
    "print(df['Eg_Size'].value_counts()) #count number of 13s and 19s"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cefd3a1-a574-49a7-9d1a-03ceebadde0e",
   "metadata": {},
   "source": [
    "$p_5 = 42 · 24 + 45 · 24 + 46 · 12 + 47 · 24 + 54 · 12 + 69 · 8 + 82 · 16 + 99 · 10 + 111 · 12 + 133 · 8 + 167 · 4$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04f2fbeb-00ba-45bc-97e7-28b5771ab5b8",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Which $g \\in \\tilde{P}_{n}$ attain max and min $|\\mathbb{E}(g)|$<a name=\"s3\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07b76524-b80a-411c-b53f-5ca7dfe8d111",
   "metadata": {},
   "source": [
    "Let $(p_{n})_{n \\in \\mathbb{N}}$ denote the sequence of terms $p_{n}:=|\\tilde{P}_n|$, and let $\\boldsymbol{e}_{n}:=(e_{n,i})$ denote the increasing sequence of distinct terms $|\\mathbb{E}(g)|$ for $g \\in \\tilde{P}_{n}$.\n",
    "\n",
    "Recall Section $5.2$ of the thesis. In particular we have the sequences $\\mathcal{U}=(U_n)$ and $\\mathcal{L}=(L_n)$ defined by the maximal and minimal term of $e_n$.\n",
    "\n",
    "In this section we ask which $g \\in \\tilde{P}_{n}$ attain $U_n$ and $L_n$ (there is more than one in each case). In particular we would like to know those examples which have a particularly nice form."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74788a58-9246-42dd-bba8-b0204b5f55c0",
   "metadata": {
    "tags": []
   },
   "source": [
    "## $U_3$ and $z \\in \\tilde{P_n}$<a name=\"s31\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af72990d-4a4a-4a80-91ac-43f298885e0e",
   "metadata": {},
   "source": [
    "The all zero function $z\\in \\tilde{P_n}$ we conjecture always attains $U_n=|\\mathbb{E}(z)|$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3b0c04f-48c3-4217-a013-0cdb040e8d1e",
   "metadata": {},
   "source": [
    "Consider {'1': 0,'2': 0,\"12\":0} in $\\tilde{P}_2$, this obtaiin $U_2$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "95dc2009-70e1-477f-abe8-4d62c9f7025d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "e_n : d_n\n",
      "5.0    1\n",
      "Name: Eg_Size, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "n=1\n",
    "power_set_str,power_set_str_np1=get_n_np1_powersets(n)\n",
    "P2=pd.DataFrame([{'1': 0, '2': 0, '12': 0},{'1': 0, '2': 0, '12': 1}])\n",
    "\n",
    "#one function selected\n",
    "P2=pd.DataFrame([{'1': 0, '2': 0, '12': 0}])\n",
    "\n",
    "power_set_str=get_powerset_str(2)\n",
    "P2_eg=get_Eg_column_Pn(P2)\n",
    "P2_eg=P2_eg.sort_values('Eg_Size') \n",
    "\n",
    "df=P2_eg\n",
    "print(f\"e_n : d_n\")\n",
    "print(df['Eg_Size'].value_counts()) #count number of 13s and 19s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "2692c732-6b7c-4e8f-8257-466b5a1dfba7",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   1  2  12  Eg_Size\n",
      "0  0  0   0      5.0\n"
     ]
    }
   ],
   "source": [
    "print(P2_eg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22580ec0-355f-4dd8-96f7-f741e9993969",
   "metadata": {
    "tags": []
   },
   "source": [
    "The all zero function $z$ is a good candidate as it also has maximal $|\\mathbb{E}(z)|$ in $P_{5}$ (determined separately)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01b8ff40-9890-48e2-a1f7-63eee303cb66",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "power_set_str=get_powerset_str(5)\n",
    "#Find index of zero function\n",
    "# Take a g in P5 and calculate Eg\n",
    "# P5.iloc[1].to_dict()\n",
    "all_zero_funct={'1': 0,'2': 0,'3': 0,'4': 0,'5': 0,'12': 0,'13': 0,'14': 0,'15': 0,'23': 0,'24': 0,'25': 0,'34': 0,'35': 0,'45': 0,'123': 0,'124': 0,'125': 0,'134': 0,'135': 0,'145': 0,'234': 0,'235': 0,'245': 0,'345': 0,'1234': 0,'1235': 0,'1245': 0,'1345': 0,'2345': 0,'12345': 0}\n",
    "df=P5\n",
    "my_dict=all_zero_funct\n",
    "index = df.loc[df.eq(my_dict).all(1)].index[0]\n",
    "print(index)#5166 #works\n",
    "# df.loc[5166]#\n",
    "# Get Eg for this. #Should be maximum\n",
    "# Eg_zeroP5=get_Eg_for_single_g(df,index,power_set_str)\n",
    "# print(len(Eg_zeroP5)) #7580"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5ec796c-d49c-44ae-9099-b6fa42ba9290",
   "metadata": {},
   "source": [
    "Consider $z \\in \\tilde{P}_n$, what other functions also give $U_n$? "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62ee6088-f497-4e3d-b58a-508cab228d10",
   "metadata": {},
   "source": [
    "Simplist case to consider is $\\tilde{P}_3$. We determine $f \\in \\tilde{P}_3$ and determine $\\mathbb{E}(f)$ for each, then extract those $f$ such that we have\n",
    "$|\\mathbb{E}(f)|=U_3$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "ed531b01-cbab-41b5-a1b5-d1a23a51d083",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of functions: 10\n",
      "Are there duplicates? (0, 7) if (0,-) then no common rows\n",
      "   1  2  3  12  13  23  123\n",
      "0  0  0  0   0   0   1    1\n",
      "1  0  0  0   0   1   1    1\n",
      "2  0  0  0   0   1   0    1\n",
      "3  0  0  0   0   0   0    1\n",
      "4  0  0  0   0   0   0    0\n",
      "5  0  0  0   1   1   1    2\n",
      "6  0  0  0   1   1   0    1\n",
      "7  0  0  0   1   1   1    1\n",
      "8  0  0  0   1   0   1    1\n",
      "9  0  0  0   1   0   0    1\n"
     ]
    }
   ],
   "source": [
    "#get P3\n",
    "P2=pd.DataFrame([{'1': 0, '2': 0, '12': 0},{'1': 0, '2': 0, '12': 1}])\n",
    "n=2\n",
    "power_set_str,power_set_str_np1=get_n_np1_powersets(n)\n",
    "P3=get_Pnp1(P2,2)\n",
    "print(P3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "0ba9040c-e393-46c9-ab30-9de7b884ade9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "e_n : d_n\n",
      "13.0    6\n",
      "19.0    4\n",
      "Name: Eg_Size, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "power_set_str=get_powerset_str(3)\n",
    "\n",
    "P3_eg=get_Eg_column_Pn(P3)\n",
    "P3_eg=P3_eg.sort_values('Eg_Size') # get those 13,then 19\n",
    "\n",
    "df=P3_eg\n",
    "print(f\"e_n : d_n\")\n",
    "print(df['Eg_Size'].value_counts()) #count number of 13s and 19s"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e02622b-5bcc-4b85-88c7-6595f8a3d4c7",
   "metadata": {},
   "source": [
    "There are $4$ which give $U_3$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "f45881ad-5fc7-4ff5-9da2-2ebbd9cf9e56",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   1  2  3  12  13  23  123  Eg_Size\n",
      "3  0  0  0   0   0   0    1     19.0\n",
      "4  0  0  0   0   0   0    0     19.0\n",
      "5  0  0  0   1   1   1    2     19.0\n",
      "7  0  0  0   1   1   1    1     19.0\n",
      "(4, 8)\n"
     ]
    }
   ],
   "source": [
    "y=P3_eg[P3_eg.Eg_Size == 19]\n",
    "print(y)\n",
    "print(y.shape)\n",
    "#Get Eg for the function in P3\n",
    "#3  0  0  0   0   0   0    1      19"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0033bc49-52dc-464b-9625-5baabd135a5b",
   "metadata": {},
   "source": [
    "Remove \"Eg_Size\" column and turn to a list of dictionaries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "97544926-ad69-4b61-a781-f0b65f5bb174",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\RhysL\\AppData\\Local\\Temp\\ipykernel_12024\\1174555318.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  y.drop('Eg_Size', axis=1, inplace=True)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>12</th>\n",
       "      <th>13</th>\n",
       "      <th>23</th>\n",
       "      <th>123</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   1  2  3  12  13  23  123\n",
       "3  0  0  0   0   0   0    1\n",
       "4  0  0  0   0   0   0    0\n",
       "5  0  0  0   1   1   1    2\n",
       "7  0  0  0   1   1   1    1"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.drop('Eg_Size', axis=1, inplace=True)\n",
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "f60cc642-b35b-4ac3-95cd-e9abd4dc00f7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'1': 0, '2': 0, '3': 0, '12': 0, '13': 0, '23': 0, '123': 1},\n",
       " {'1': 0, '2': 0, '3': 0, '12': 0, '13': 0, '23': 0, '123': 0},\n",
       " {'1': 0, '2': 0, '3': 0, '12': 1, '13': 1, '23': 1, '123': 2},\n",
       " {'1': 0, '2': 0, '3': 0, '12': 1, '13': 1, '23': 1, '123': 1}]"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df=y\n",
    "list_of_dicts = df.to_dict(orient='records')\n",
    "list_of_dicts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f40bb6a5-828f-4f69-9bc6-19a8a404f238",
   "metadata": {},
   "source": [
    "We now determine $\\mathbb{E}(g)$ for each of the above functions. We then ask is $\\mathbb{E}(g)$ are equal. Recall get_Eg_for_single_g_dfisfunct(). \n",
    "\n",
    "\n",
    "As the order of the output of get_Eg_for_single_g_dfisfunct() can vary we sort everything into a standard format"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4f00f84-f77f-4819-a490-0dd9f3b5a721",
   "metadata": {},
   "source": [
    "In particular we pick the $f \\in \\tilde{P}_{3}$  which obtain $|\\mathbb{E}(f)|=19=U_3$ and $\\mathbb{E}(f)$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "d42459e2-d6e4-4f44-a33e-ecad1310eb07",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'1': 0, '2': 0, '3': 0, '12': 0, '13': 0, '23': 0, '123': 1}\n",
      "[['12'], ['1', '12', '13', '2', '23'], ['1', '12', '123', '13', '2', '23', '3'], ['1', '12', '13', '2', '23', '3'], ['23'], ['12', '13', '2', '23'], ['1', '12', '13'], ['12', '13'], ['12', '23'], ['12', '13', '2', '23', '3'], ['13', '23'], ['12', '13', '23', '3'], ['1', '12', '13', '23', '3'], ['1', '12', '13', '23'], ['12', '13', '23'], ['13', '23', '3'], ['13'], ['12', '2', '23'], []] \n",
      "\n",
      "{'1': 0, '2': 0, '3': 0, '12': 0, '13': 0, '23': 0, '123': 0}\n",
      "[['12', '123', '13'], ['12', '123', '2', '23'], ['1', '12', '123', '13', '2', '23', '3'], ['12', '123'], ['1', '12', '123', '13', '2', '23'], ['123', '13'], ['12', '123', '13', '23', '3'], ['12', '123', '13', '2', '23', '3'], ['123', '23'], ['123', '13', '23'], ['12', '123', '23'], ['12', '123', '13', '23'], ['1', '12', '123', '13', '23'], ['123', '13', '23', '3'], ['12', '123', '13', '2', '23'], ['1', '12', '123', '13', '23', '3'], ['1', '12', '123', '13'], ['123'], []] \n",
      "\n",
      "{'1': 0, '2': 0, '3': 0, '12': 1, '13': 1, '23': 1, '123': 2}\n",
      "[['2', '23', '3'], ['1', '2', '23', '3'], ['1', '3'], ['1', '12', '123', '13', '2', '23', '3'], ['1', '12', '13', '2', '23', '3'], ['1', '13', '3'], ['1', '12', '13', '2', '3'], ['3'], ['1', '12', '2', '23', '3'], ['1'], ['1', '13', '2', '3'], ['1', '2', '3'], ['2', '3'], ['2'], ['1', '12', '2', '3'], ['1', '12', '2'], ['1', '13', '2', '23', '3'], ['1', '2'], []] \n",
      "\n",
      "{'1': 0, '2': 0, '3': 0, '12': 1, '13': 1, '23': 1, '123': 1}\n",
      "[['1', '12', '123', '2'], ['1', '12', '123', '2', '3'], ['1', '12', '123', '13', '2', '23', '3'], ['1', '123', '2', '23', '3'], ['1', '123', '3'], ['123', '3'], ['123', '2', '23', '3'], ['123', '2', '3'], ['1', '12', '123', '13', '2', '3'], ['123', '2'], ['1', '123', '2'], ['1', '123'], ['1', '12', '123', '2', '23', '3'], ['1', '123', '13', '2', '23', '3'], ['1', '123', '13', '3'], ['1', '123', '2', '3'], ['1', '123', '13', '2', '3'], ['123'], []] \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Eg_cases=[]\n",
    "sorted_Eg_cases = []\n",
    "\n",
    "for g in list_of_dicts:\n",
    "    print(g)\n",
    "    #[{'1': 0, '2': 0, '3': 0, '12': 0, '13': 0, '23': 0, '123': 1},\n",
    "    EEg_example=get_Eg_for_single_g_dfisfunct(g,power_set_str)\n",
    " # [frozenset({'12'}), frozenset({'2', '23', '13', '12', '1'}), frozenset({'2', '23', '3', '13', '12', '1', '123'}), \n",
    " #  frozenset({'2', '23', '3', '13', '12', '1'}), frozenset({'23'}), frozenset({'13', '12', '2', '23'}),\n",
    " #  frozenset({'1', '13', '12'}), frozenset({'13', '12'}), frozenset({'12', '23'}), frozenset({'2', '23', '3', '13', '12'}), \n",
    " #  frozenset({'13', '23'}), frozenset({'13', '12', '23', '3'}), frozenset({'23', '3', '13', '12', '1'}), frozenset({'1', '13', '12', '23'}), \n",
    " #  frozenset({'13', '12', '23'}), frozenset({'13', '23', '3'}), frozenset({'13'}), frozenset({'12', '2', '23'}), frozenset()] \n",
    "    \n",
    "    e=EEg_example\n",
    "    sorted_e=[] #elements are\n",
    "    # [['12'], ['1', '12', '13', '2', '23'], ['1', '12', '123', '13', '2', '23', '3'], ['1', '12', '13', '2', '23', '3'],\n",
    "    #  ['23'], ['12', '13', '2', '23'], ['1', '12', '13'], ['12', '13'], ['12', '23'], ['12', '13', '2', '23', '3'], ['13', '23'],\n",
    "    #  ['12', '13', '23', '3'], ['1', '12', '13', '23', '3'], \n",
    "    #  ['1', '12', '13', '23'], ['12', '13', '23'], ['13', '23', '3'], ['13'], ['12', '2', '23'], []]\n",
    "    for fs in e:\n",
    "        sorted_fs=sorted(list(fs))\n",
    "        sorted_e.append(sorted_fs)\n",
    "        \n",
    "    sorted_Eg_cases.append(sorted_e)\n",
    "    print(sorted_e,\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2812f8f4-2b4e-4069-8c2c-96cfa60a8a2a",
   "metadata": {},
   "source": [
    "We see that not all $\\mathbb{E}(g)$ are equal. Let $f \\in \\tilde{P}_n$ do we always have a surjection  $\\mathbb{E}(z) \\rightarrow \\mathbb{E}(f)$ (no otherwise these would be equal)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23b3a38a-d7e2-47d6-b675-3c03bc46d216",
   "metadata": {},
   "source": [
    "What is $\\mathbb{E}(z)$. \n",
    "\n",
    "{'1': 0, '2': 0, '3': 0, '12': 0, '13': 0, '23': 0, '123': 0}\n",
    "[['12', '123', '13'], ['12', '123', '2', '23'], ['1', '12', '123', '13', '2', '23', '3'], ['12', '123'], ['1', '12', '123', '13', '2', '23'], ['123', '13'], ['12', '123', '13', '23', '3'], ['12', '123', '13', '2', '23', '3'], ['123', '23'], ['123', '13', '23'], ['12', '123', '23'], ['12', '123', '13', '23'], ['1', '12', '123', '13', '23'], ['123', '13', '23', '3'], ['12', '123', '13', '2', '23'], ['1', '12', '123', '13', '23', '3'], ['1', '12', '123', '13'], ['123'], []] "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5275e56f-031a-4687-b7cf-7d1facf2b95c",
   "metadata": {
    "tags": []
   },
   "source": [
    "## $L_3$ <a name=\"s32\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72ebbeb6-1fc1-4875-8dec-379f58862c99",
   "metadata": {},
   "source": [
    "Lets pick the $f \\in \\tilde{P}_{3}$  which obtain $|\\mathbb{E}(f)|=13=L_3$. In this section we obtain to $f$ and the $\\mathbb{E}(f)$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "a06ab357-ad48-404b-ba3a-6769a2ea8e94",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   1  2  3  12  13  23  123  Eg_Size\n",
      "0  0  0  0   0   0   1    1     13.0\n",
      "1  0  0  0   0   1   1    1     13.0\n",
      "2  0  0  0   0   1   0    1     13.0\n",
      "6  0  0  0   1   1   0    1     13.0\n",
      "8  0  0  0   1   0   1    1     13.0\n",
      "9  0  0  0   1   0   0    1     13.0\n",
      "(6, 8)\n"
     ]
    }
   ],
   "source": [
    "y=P3_eg[P3_eg.Eg_Size == 13]\n",
    "print(y)\n",
    "print(y.shape)\n",
    "#Get Eg for the function in P3\n",
    "#3  0  0  0   0   0   0    1      19"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "0f720e99-fc74-4cc3-b7f8-32c67400c1db",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\RhysL\\AppData\\Local\\Temp\\ipykernel_12024\\1624874608.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  y.drop('Eg_Size', axis=1, inplace=True)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'1': 0, '2': 0, '3': 0, '12': 0, '13': 0, '23': 1, '123': 1},\n",
       " {'1': 0, '2': 0, '3': 0, '12': 0, '13': 1, '23': 1, '123': 1},\n",
       " {'1': 0, '2': 0, '3': 0, '12': 0, '13': 1, '23': 0, '123': 1},\n",
       " {'1': 0, '2': 0, '3': 0, '12': 1, '13': 1, '23': 0, '123': 1},\n",
       " {'1': 0, '2': 0, '3': 0, '12': 1, '13': 0, '23': 1, '123': 1},\n",
       " {'1': 0, '2': 0, '3': 0, '12': 1, '13': 0, '23': 0, '123': 1}]"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.drop('Eg_Size', axis=1, inplace=True)\n",
    "# y\n",
    "df=y\n",
    "list_of_dicts = df.to_dict(orient='records')\n",
    "list_of_dicts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "c0399b96-4624-4d19-a7ca-e943928d3518",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'1': 0, '2': 0, '3': 0, '12': 0, '13': 0, '23': 1, '123': 1}\n",
      "[['12'], ['12', '123', '13', '2', '3'], ['12', '13', '2', '3'], ['13', '3'], ['12', '13', '3'], ['1', '12', '123', '13', '2', '23', '3'], ['12', '13', '2'], ['12', '2'], ['12', '123', '13', '2', '23', '3'], ['12', '13'], ['1', '12', '123', '13', '2', '3'], ['13'], []] \n",
      "\n",
      "{'1': 0, '2': 0, '3': 0, '12': 0, '13': 1, '23': 1, '123': 1}\n",
      "[['12', '3'], ['12'], ['1', '12', '123', '2', '3'], ['12', '123', '2', '23', '3'], ['1', '12', '123', '13', '2', '23', '3'], ['1', '12', '123', '2', '23', '3'], ['12', '123', '3'], ['12', '123', '2', '3'], ['1', '12', '123', '13', '2', '3'], ['3'], ['1', '12', '123', '13', '3'], ['1', '12', '123', '3'], []] \n",
      "\n",
      "{'1': 0, '2': 0, '3': 0, '12': 0, '13': 1, '23': 0, '123': 1}\n",
      "[['12', '23', '3'], ['23', '3'], ['12'], ['1', '12', '123', '13', '2', '23', '3'], ['23'], ['1', '12', '23', '3'], ['1', '12', '123', '23', '3'], ['1', '12', '123', '2', '23', '3'], ['1', '12'], ['1', '12', '123', '13', '23', '3'], ['1', '12', '23'], ['12', '23'], []] \n",
      "\n",
      "{'1': 0, '2': 0, '3': 0, '12': 1, '13': 1, '23': 0, '123': 1}\n",
      "[['1', '123', '23'], ['1', '123', '13', '23', '3'], ['1', '12', '123', '13', '2', '23', '3'], ['1', '123', '2', '23', '3'], ['23'], ['1', '123', '2', '23'], ['1', '123', '23', '3'], ['1', '12', '123', '2', '23'], ['1', '12', '123', '2', '23', '3'], ['1', '123', '13', '2', '23', '3'], ['1', '23'], ['1'], []] \n",
      "\n",
      "{'1': 0, '2': 0, '3': 0, '12': 1, '13': 0, '23': 1, '123': 1}\n",
      "[['13', '2'], ['123', '13', '2'], ['2'], ['1', '12', '123', '13', '2', '23', '3'], ['123', '13', '2', '23', '3'], ['1', '123', '13', '2', '23', '3'], ['1', '12', '123', '13', '2', '3'], ['1', '12', '123', '13', '2'], ['1', '123', '13', '2'], ['123', '13', '2', '3'], ['13'], ['1', '123', '13', '2', '3'], []] \n",
      "\n",
      "{'1': 0, '2': 0, '3': 0, '12': 1, '13': 0, '23': 0, '123': 1}\n",
      "[['1', '12', '123', '13', '2', '23', '3'], ['1', '123', '13', '2', '23'], ['1', '12', '123', '13', '2', '23'], ['1', '13'], ['23'], ['1', '13', '2', '23'], ['2', '23'], ['1', '123', '13', '2', '23', '3'], ['13', '2', '23'], ['1', '13', '23'], ['13', '23'], ['13'], []] \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Eg_cases=[]\n",
    "sorted_Eg_cases = []\n",
    "\n",
    "for g in list_of_dicts:\n",
    "    print(g)\n",
    "# {'1': 0, '2': 0, '3': 0, '12': 0, '13': 0, '23': 1, '123': 1}\n",
    "    EEg_example=get_Eg_for_single_g_dfisfunct(g,power_set_str)\n",
    "\n",
    "    \n",
    "    e=EEg_example\n",
    "    sorted_e=[] #elements are\n",
    "  # [['12'], ['12', '123', '13', '2', '3'], ['12', '13', '2', '3'], ['13', '3'], ['12', '13', '3'], ['1', '12', '123', '13', '2', '23', '3'], ['12', '13', '2'], ['12', '2'], ['12', '123', '13', '2', '23', '3'], ['12', '13'], ['1', '12', '123', '13', '2', '3'], ['13'], []] \n",
    "    for fs in e:\n",
    "        sorted_fs=sorted(list(fs))\n",
    "        sorted_e.append(sorted_fs)\n",
    "        \n",
    "    sorted_Eg_cases.append(sorted_e)\n",
    "    print(sorted_e,\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "179bc021-ce00-465e-97fb-90acba6edbf2",
   "metadata": {},
   "source": [
    "We conjecture that:\n",
    "    \n",
    "    {'1': 0, '2': 0, '3': 0, '12': 0, '13': 1, '23': 1, '123': 1}\n",
    "[['12', '3'], ['12'], ['1', '12', '123', '2', '3'], ['12', '123', '2', '23', '3'], ['1', '12', '123', '13', '2', '23', '3'], ['1', '12', '123', '2', '23', '3'], ['12', '123', '3'], ['12', '123', '2', '3'], ['1', '12', '123', '13', '2', '3'], ['3'], ['1', '12', '123', '13', '3'], ['1', '12', '123', '3'], []] \n",
    "\n",
    "Generalises to a function which always gives $L_{n}$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "140b70ac-d156-4ad8-9b95-d28921016c43",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": [],
    "toc-hr-collapsed": true
   },
   "source": [
    "# $\\tilde{P}_6$ and decomposition of $|\\tilde{P}_6|$ (Debt)<a name=\"s4\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1703341e-52ac-4c75-85d6-0abdb3f2062c",
   "metadata": {},
   "source": [
    "The following code is an application of previous cases. I leave this section as debt. I f you wish to understand the details email me and i will fill organise it for you then"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5f3d98e-81ae-450e-a18b-2ac6954ffb1b",
   "metadata": {},
   "source": [
    "As P6=get_Pnp1(P5,5) takes too long we break the construction into parts. During this process we record the number of extension each function in P5 has, this will be used to give the sum\n",
    "\n",
    "$$|P_6|=\\sum_{g \\in P_5} |\\rho^{-1}(g)|$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9712713-e66b-4388-aed9-8d94a1e70f1d",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Functions <a name=\"s21\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "64633e16-52d5-4bc4-83e3-47899c59b0fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_Eg_column_Pn(df):\n",
    "    df=df.copy()\n",
    "\n",
    "    # Create a new column called \"new_column\"\n",
    "    df['Eg_Size'] = None\n",
    "\n",
    "    # Insert data into the new column\n",
    "    for i, row in df.iterrows():    \n",
    "        Eg=get_Eg_for_single_g(df,i,power_set_str)\n",
    "        df.at[i, 'Eg_Size'] = int(len(Eg))\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "71dee92b-8100-44db-873a-0d0e50a46e83",
   "metadata": {},
   "outputs": [],
   "source": [
    "def eg_column_Build_Pn_1(list_df_Pn,power_set_str,n):\n",
    "    \n",
    "    \"\"\"\n",
    "    Obj:Use Build_Pn_1 and modify change to include Eg size column in functions\n",
    "    \"\"\"\n",
    "    \n",
    "    # Returns:packets of extensions for each f_dict from Pn, can check those with max size of closed sets (later)\n",
    "    \n",
    "    P_n1=[]\n",
    "    for index,f_dict in enumerate(list_df_Pn):\n",
    "\n",
    "        # Store primitive closed sets \n",
    "        PC=[(x,get_barj({x},f_dict)) for x in power_set_str]\n",
    "        dict_PC=dict(PC)\n",
    "        set_PC=[sorted_frozenset(get_barj({x},f_dict)) for x in power_set_str]\n",
    "\n",
    "        #Get all closed sets for extensions for f_dict\n",
    "        Eg=list(get_Eg(dict_PC,power_set_str,set_PC))\n",
    "        Eg.append(\"empty\")\n",
    "        \n",
    "        Eg_col_kvpair={\"Eg_Size\":len(Eg)}\n",
    "\n",
    "        #Build extensions for f_dict\n",
    "        extension_of_f_dict=[]\n",
    "        for term in Eg:\n",
    "            if term ==\"empty\":                    \n",
    "                funct=empty_case_get_extension_of_f(f_dict,term,power_set_str,n) #dictionary\n",
    "                funct.update(Eg_col_kvpair)\n",
    "                extension_of_f_dict.append(funct)\n",
    "            else:\n",
    "                funct=get_extension_of_f(f_dict,term,power_set_str,n)\n",
    "                funct.update(Eg_col_kvpair)\n",
    "                extension_of_f_dict.append(funct)\n",
    "\n",
    "        #We record packets of extensions where we take +1\n",
    "        P_n1.append(extension_of_f_dict)\n",
    "    \n",
    "    return P_n1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "c676118b-d859-4167-a324-137ab3548612",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_Pnp1_for_single_funct(Pn,n):\n",
    "    \n",
    "    #inputs\n",
    "    # Pn:dataframe and n.\n",
    "    \n",
    "    \"\"\"\n",
    "    Returns a dataframe of extensions in Pn+1 for dataframe of functions in Pn.\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    #Example\n",
    "    n=3\n",
    "    power_set_str,power_set_str_np1=get_n_np1_powersets(n)\n",
    "    # P3\n",
    "\n",
    "    funct_list=[{'1': 0,'2': 0,\"3\":0,\"12\":0,\"23\":0,\"13\":0,\"123\":0}]\n",
    "    Pn=pd.DataFrame(funct_list)\n",
    "\n",
    "    #The following should be functions in P4 that are extendions of functions in funct_list\n",
    "    f_P3=get_Pnp1_for_single_funct(Pn,n)\n",
    "    print(f_P3)\n",
    "    \"\"\"\n",
    "\n",
    "    #New data created\n",
    "    power_set_str,power_set_str_np1=get_n_np1_powersets(n)\n",
    "\n",
    "    #Load Pn as a list of dictionaries.\n",
    "    list_df_Pn=Pn.to_dict(orient='records')\n",
    "\n",
    "    #Create Pn+1 and hold as dataframe.\n",
    "    P_n1=eg_column_Build_Pn_1(list_df_Pn,power_set_str,n)\n",
    "    Pnp1=extend_sets_to_Pnp1(P_n1) #<---- ISSUE\n",
    "    \n",
    "    power_set_str_np1\n",
    "    \n",
    "    columns=list(power_set_str_np1)+[\"Eg_Size\"]\n",
    "    Pnp1 = Pnp1.reindex(columns=columns)\n",
    "\n",
    "\n",
    "    inspect_Pn1(Pnp1)\n",
    "\n",
    "    return Pnp1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "412261ec-dbb6-44ea-8032-7e8511e866ad",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "f1c7587f-14a6-4a93-b242-6bde6b8837ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Pnp1_from_Pn_part(df,n,calc_start,calc_end):\n",
    "    # df where we are extending from\n",
    "    # calc_start=0\n",
    "    # calc_end=5\n",
    "\n",
    "    rows=range(calc_start,calc_end) #What functions we are extending\n",
    "    Pn=df.iloc[rows] #takes database of rows 0,1..,calc_up_to\n",
    "\n",
    "    # print(\"The functions we are extending\")\n",
    "    # print(Pn,\"\\n\")\n",
    "\n",
    "    part_Pn=get_Pnp1_for_single_funct(Pn,n) #Extensions of Pn\n",
    "    # print(part_Pn.shape)\n",
    "    \n",
    "    strt=f\"P{n+1}_parts\\{calc_start}_{calc_end}_Part_fromP{n}.xlsx\"\n",
    "    part_Pn.to_excel(strt) #Stores in excel\n",
    "    return"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56b7c195-b947-4bd4-a0ed-3e754f5e6a32",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Example of functions used and process for P3 <a name=\"s22\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e59ed4d4-bb26-4cae-aac9-bf7a3812e775",
   "metadata": {
    "tags": [],
    "toc-hr-collapsed": true
   },
   "source": [
    "### Single function extension example"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8574aff-1105-4598-89cd-5e3963b3ac94",
   "metadata": {},
   "source": [
    "Here we see get_Pnp1_for_single_funct in action for P3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "f4180022-ed73-47df-a563-6e3fd0f3f13a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The functions we are extending\n",
      "   1  2  3  12  23  13  123\n",
      "0  0  0  0   0   0   0    0\n",
      "Number of functions: 19\n",
      "Are there duplicates? (0, 16) if (0,-) then no common rows\n",
      "    1  2  3  4  12  13  14  23  24  34  123  124  134  234  1234  Eg_Size\n",
      "0   0  0  0  0   0   0   1   0   1   1    0    1    1    1     1       19\n",
      "1   0  0  0  0   0   0   0   0   0   0    0    1    0    1     1       19\n",
      "2   0  0  0  0   0   0   0   0   0   0    0    0    1    1     1       19\n",
      "3   0  0  0  0   0   0   0   0   0   0    0    1    1    1     1       19\n",
      "4   0  0  0  0   0   0   1   0   1   0    0    1    1    1     1       19\n",
      "5   0  0  0  0   0   0   0   0   1   1    0    1    1    1     1       19\n",
      "6   0  0  0  0   0   0   1   0   0   0    0    1    1    1     1       19\n",
      "7   0  0  0  0   0   0   0   0   0   0    0    0    0    1     1       19\n",
      "8   0  0  0  0   0   0   0   0   0   0    0    1    1    0     1       19\n",
      "9   0  0  0  0   0   0   0   0   0   0    0    0    0    0     1       19\n",
      "10  0  0  0  0   0   0   0   0   0   0    0    0    1    0     1       19\n",
      "11  0  0  0  0   0   0   0   0   0   1    0    0    1    1     1       19\n",
      "12  0  0  0  0   0   0   0   0   1   0    0    1    0    1     1       19\n",
      "13  0  0  0  0   0   0   1   0   0   1    0    1    1    1     1       19\n",
      "14  0  0  0  0   0   0   1   0   0   0    0    1    1    0     1       19\n",
      "15  0  0  0  0   0   0   0   0   0   0    0    1    0    0     1       19\n",
      "16  0  0  0  0   0   0   0   0   0   1    0    1    1    1     1       19\n",
      "17  0  0  0  0   0   0   0   0   1   0    0    1    1    1     1       19\n",
      "18  0  0  0  0   0   0   0   0   0   0    0    0    0    0     0       19\n"
     ]
    }
   ],
   "source": [
    "#Testing functions example\n",
    "n=3\n",
    "power_set_str,power_set_str_np1=get_n_np1_powersets(n)\n",
    "# P3\n",
    "\n",
    "# You can take subsets of rows as a database, then extend. Better than below\n",
    "funct_list=[{'1': 0,'2': 0,\"3\":0,\"12\":0,\"23\":0,\"13\":0,\"123\":0}]\n",
    "Pn=pd.DataFrame(funct_list)\n",
    "print(\"The functions we are extending\")\n",
    "print(Pn)\n",
    "\n",
    "#The following should be functions in P4 that are extendions of functions in funct_list\n",
    "f_P3=get_Pnp1_for_single_funct(Pn,n)\n",
    "print(f_P3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "422773f9-4214-4122-8645-613b20e09768",
   "metadata": {
    "tags": [],
    "toc-hr-collapsed": true
   },
   "source": [
    "### Building P4 from P3 example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f445619-ff65-49de-8783-2cd4aaef6ddf",
   "metadata": {},
   "outputs": [],
   "source": [
    "n=3\n",
    "power_set_str,power_set_str_np1=get_n_np1_powersets(n)\n",
    "# P3\n",
    "df=P3 #Total dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46372f5d-8e2e-405e-914d-359cd6e5dec5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "calc_start=0\n",
    "calc_end=5\n",
    "\n",
    "rows=range(calc_start,calc_end) #What functions we are extending\n",
    "Pn=df.iloc[rows] #takes database of rows 0,1..,calc_up_to\n",
    "\n",
    "print(\"The functions we are extending\")\n",
    "print(Pn,\"\\n\")\n",
    "\n",
    "part_Pn=get_Pnp1_for_single_funct(Pn,n) #Extensions of Pn\n",
    "print(part_Pn)\n",
    "strt=f\"P3_parts_example\\{calc_start}_{calc_end}_PartP4.xlsx\"\n",
    "part_Pn.to_excel(strt) #Stores in excel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85c946b1-0817-4d01-bca7-f7c383bfb59d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "calc_start=5\n",
    "calc_end=10\n",
    "\n",
    "rows=range(calc_start,calc_end) #What functions we are extending\n",
    "Pn=df.iloc[rows] #takes database of rows 0,1..,calc_up_to\n",
    "\n",
    "print(\"The functions we are extending\")\n",
    "print(Pn,\"\\n\")\n",
    "\n",
    "part_Pn=get_Pnp1_for_single_funct(Pn,n) #Extensions of Pn\n",
    "print(part_Pn)\n",
    "strt=f\"P3_parts_example\\{calc_start}_{calc_end}_PartP4.xlsx\"\n",
    "part_Pn.to_excel(strt) #Stores in excel"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd25f5a9-15b1-4136-a3ab-70aa6750ff38",
   "metadata": {},
   "source": [
    " We now merge the parts together."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ccf1533-81a9-447d-a6ab-ac28b9043b02",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "P4_part1=pd.read_excel('P3_parts_example/0_5_PartP4.xlsx', index_col=0) \n",
    "P4_part2=pd.read_excel('P3_parts_example/5_10_PartP4.xlsx', index_col=0) \n",
    "\n",
    "merged_df = pd.concat([P4_part1, P4_part2], ignore_index=True)\n",
    "\n",
    "merged_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "048f9c19-25d8-4d57-a0f7-309b5947810f",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": [],
    "toc-hr-collapsed": true
   },
   "source": [
    "## Building P6 from P5 <a name=\"s23\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f2b06f0-7f06-4555-bdf9-cc0185db0d0e",
   "metadata": {},
   "source": [
    "You can run the following cell to get the data for P6 (take about 4hrs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "5e69e520-3348-4cd9-8b15-adc3f8fc81ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Part builder cell for P5 to P6 #At this rate it will take 1.43hrs<time<6hrs to run this calculation\n",
    "\n",
    "n=5\n",
    "power_set_str,power_set_str_np1=get_n_np1_powersets(n)\n",
    "df=P5 #Total dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86a5e17d-54a2-4ef2-bf98-d881ed65e971",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": [],
    "toc-hr-collapsed": true
   },
   "source": [
    "### Test case"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "0acd366b-7cfb-4a76-af17-294f1d7d9c27",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of functions: 47417\n",
      "Are there duplicates? (0, 64) if (0,-) then no common rows\n",
      "Perf timer 107.10247100000004\n",
      "Process timer 73.859375\n"
     ]
    }
   ],
   "source": [
    "start_perf,start_process = time.perf_counter(),time.process_time()\n",
    "Pnp1_from_Pn_part(df,n,0,100)\n",
    "end_perf,end_process = time.perf_counter(),time.process_time()\n",
    "\n",
    "print(f\"Perf timer {end_perf-start_perf}\") #This method returns the time in seconds.\n",
    "print(f\"Process timer {end_process-start_process}\") #measures the time the process takes, including time that the process is blocked"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8884b85e-0d12-45e4-ac4a-e616e46f900c",
   "metadata": {
    "tags": [],
    "toc-hr-collapsed": true
   },
   "source": [
    "### Running to construct P6"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47b3ca8d-e235-4c62-b663-1a0f8d59686d",
   "metadata": {},
   "source": [
    "Constructs a series of 101 xlxs files to store dataframe of parts of P6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "9fed5a9b-a41d-4272-98c0-1ec7180b9b32",
   "metadata": {},
   "outputs": [],
   "source": [
    "def runner(start,end): \n",
    "    start_perf,start_process = time.perf_counter(),time.process_time()\n",
    "    Pnp1_from_Pn_part(df,n,start,end)\n",
    "    end_perf,end_process = time.perf_counter(),time.process_time()\n",
    "\n",
    "    print(f\"Perf timer {end_perf-start_perf}\") #This method returns the time in seconds.\n",
    "    print(f\"Process timer {end_process-start_process}\") #measures the time the process takes, including time that the process is blocked\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad84ca6d-1e2b-47da-9c7f-7f92a452077a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "for i in range(0, 10300, 100):\n",
    "    start=i\n",
    "    end=start+100\n",
    "    runner(start,end)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "7791920b-6a0e-459e-9247-e51d24747d33",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of functions: 13087\n",
      "Are there duplicates? (0, 64) if (0,-) then no common rows\n"
     ]
    }
   ],
   "source": [
    "calc_start=10300\n",
    "calc_end=10334\n",
    "\n",
    "rows=range(calc_start,calc_end) #What functions we are extending\n",
    "Pn=df.iloc[rows] #takes database of rows 0,1..,calc_up_to\n",
    "\n",
    "part_Pn=get_Pnp1_for_single_funct(Pn,n) #Extensions of Pn\n",
    "strt=f\"P6_parts\\{calc_start}_{calc_end}_Part_fromP5.xlsx\"\n",
    "part_Pn.to_excel(strt) #Stores in excel"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac3ff57a-f012-4976-8d36-7ac6462bc07d",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": [],
    "toc-hr-collapsed": true
   },
   "source": [
    "## Analyse P6 <a name=\"s24\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccd1800c-72e0-4550-ae18-51dff2697142",
   "metadata": {},
   "source": [
    "Here we obtain $|\\tilde{P}_{6}|$.\n",
    "\n",
    "To do so we load \n",
    "\n",
    "By loading all files in P6_parts and taking the shape.\n",
    "As part of this calculation we can also decompose this number into the sum,\n",
    "\n",
    "$$|P_6|=\\sum_{g \\in P_5} |\\rho^{-1}(g)|\\cdot 1_{g}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfaff27b-e70d-476c-867a-f40c357d3058",
   "metadata": {},
   "source": [
    "The following loads the data for the parts of $P6$ (takes about 1.5hrs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "84fcbd31-ff13-4285-b070-fd0cca060f38",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "P6_parts/0_100_Part_fromP5.xlsx\n",
      "P6_parts/100_200_Part_fromP5.xlsx\n",
      "P6_parts/200_300_Part_fromP5.xlsx\n",
      "P6_parts/300_400_Part_fromP5.xlsx\n",
      "P6_parts/400_500_Part_fromP5.xlsx\n",
      "P6_parts/500_600_Part_fromP5.xlsx\n",
      "P6_parts/600_700_Part_fromP5.xlsx\n",
      "P6_parts/700_800_Part_fromP5.xlsx\n",
      "P6_parts/800_900_Part_fromP5.xlsx\n",
      "P6_parts/900_1000_Part_fromP5.xlsx\n",
      "P6_parts/1000_1100_Part_fromP5.xlsx\n",
      "P6_parts/1100_1200_Part_fromP5.xlsx\n",
      "P6_parts/1200_1300_Part_fromP5.xlsx\n",
      "P6_parts/1300_1400_Part_fromP5.xlsx\n",
      "P6_parts/1400_1500_Part_fromP5.xlsx\n",
      "P6_parts/1500_1600_Part_fromP5.xlsx\n",
      "P6_parts/1600_1700_Part_fromP5.xlsx\n",
      "P6_parts/1700_1800_Part_fromP5.xlsx\n",
      "P6_parts/1800_1900_Part_fromP5.xlsx\n",
      "P6_parts/1900_2000_Part_fromP5.xlsx\n",
      "P6_parts/2000_2100_Part_fromP5.xlsx\n",
      "P6_parts/2100_2200_Part_fromP5.xlsx\n",
      "P6_parts/2200_2300_Part_fromP5.xlsx\n",
      "P6_parts/2300_2400_Part_fromP5.xlsx\n",
      "P6_parts/2400_2500_Part_fromP5.xlsx\n",
      "P6_parts/2500_2600_Part_fromP5.xlsx\n",
      "P6_parts/2600_2700_Part_fromP5.xlsx\n",
      "P6_parts/2700_2800_Part_fromP5.xlsx\n",
      "P6_parts/2800_2900_Part_fromP5.xlsx\n",
      "P6_parts/2900_3000_Part_fromP5.xlsx\n",
      "P6_parts/3000_3100_Part_fromP5.xlsx\n",
      "P6_parts/3100_3200_Part_fromP5.xlsx\n",
      "P6_parts/3200_3300_Part_fromP5.xlsx\n",
      "P6_parts/3300_3400_Part_fromP5.xlsx\n",
      "P6_parts/3400_3500_Part_fromP5.xlsx\n",
      "P6_parts/3500_3600_Part_fromP5.xlsx\n",
      "P6_parts/3600_3700_Part_fromP5.xlsx\n",
      "P6_parts/3700_3800_Part_fromP5.xlsx\n",
      "P6_parts/3800_3900_Part_fromP5.xlsx\n",
      "P6_parts/3900_4000_Part_fromP5.xlsx\n",
      "P6_parts/4000_4100_Part_fromP5.xlsx\n",
      "P6_parts/4100_4200_Part_fromP5.xlsx\n",
      "P6_parts/4200_4300_Part_fromP5.xlsx\n",
      "P6_parts/4300_4400_Part_fromP5.xlsx\n",
      "P6_parts/4400_4500_Part_fromP5.xlsx\n",
      "P6_parts/4500_4600_Part_fromP5.xlsx\n",
      "P6_parts/4600_4700_Part_fromP5.xlsx\n",
      "P6_parts/4700_4800_Part_fromP5.xlsx\n",
      "P6_parts/4800_4900_Part_fromP5.xlsx\n",
      "P6_parts/4900_5000_Part_fromP5.xlsx\n",
      "P6_parts/5000_5100_Part_fromP5.xlsx\n",
      "P6_parts/5100_5200_Part_fromP5.xlsx\n",
      "P6_parts/5200_5300_Part_fromP5.xlsx\n",
      "P6_parts/5300_5400_Part_fromP5.xlsx\n",
      "P6_parts/5400_5500_Part_fromP5.xlsx\n",
      "P6_parts/5500_5600_Part_fromP5.xlsx\n",
      "P6_parts/5600_5700_Part_fromP5.xlsx\n",
      "P6_parts/5700_5800_Part_fromP5.xlsx\n",
      "P6_parts/5800_5900_Part_fromP5.xlsx\n",
      "P6_parts/5900_6000_Part_fromP5.xlsx\n",
      "P6_parts/6000_6100_Part_fromP5.xlsx\n",
      "P6_parts/6100_6200_Part_fromP5.xlsx\n",
      "P6_parts/6200_6300_Part_fromP5.xlsx\n",
      "P6_parts/6300_6400_Part_fromP5.xlsx\n",
      "P6_parts/6400_6500_Part_fromP5.xlsx\n",
      "P6_parts/6500_6600_Part_fromP5.xlsx\n",
      "P6_parts/6600_6700_Part_fromP5.xlsx\n",
      "P6_parts/6700_6800_Part_fromP5.xlsx\n",
      "P6_parts/6800_6900_Part_fromP5.xlsx\n",
      "P6_parts/6900_7000_Part_fromP5.xlsx\n",
      "P6_parts/7000_7100_Part_fromP5.xlsx\n",
      "P6_parts/7100_7200_Part_fromP5.xlsx\n",
      "P6_parts/7200_7300_Part_fromP5.xlsx\n",
      "P6_parts/7300_7400_Part_fromP5.xlsx\n",
      "P6_parts/7400_7500_Part_fromP5.xlsx\n",
      "P6_parts/7500_7600_Part_fromP5.xlsx\n",
      "P6_parts/7600_7700_Part_fromP5.xlsx\n",
      "P6_parts/7700_7800_Part_fromP5.xlsx\n",
      "P6_parts/7800_7900_Part_fromP5.xlsx\n",
      "P6_parts/7900_8000_Part_fromP5.xlsx\n",
      "P6_parts/8000_8100_Part_fromP5.xlsx\n",
      "P6_parts/8100_8200_Part_fromP5.xlsx\n",
      "P6_parts/8200_8300_Part_fromP5.xlsx\n",
      "P6_parts/8300_8400_Part_fromP5.xlsx\n",
      "P6_parts/8400_8500_Part_fromP5.xlsx\n",
      "P6_parts/8500_8600_Part_fromP5.xlsx\n",
      "P6_parts/8600_8700_Part_fromP5.xlsx\n",
      "P6_parts/8700_8800_Part_fromP5.xlsx\n",
      "P6_parts/8800_8900_Part_fromP5.xlsx\n",
      "P6_parts/8900_9000_Part_fromP5.xlsx\n",
      "P6_parts/9000_9100_Part_fromP5.xlsx\n",
      "P6_parts/9100_9200_Part_fromP5.xlsx\n",
      "P6_parts/9200_9300_Part_fromP5.xlsx\n",
      "P6_parts/9300_9400_Part_fromP5.xlsx\n",
      "P6_parts/9400_9500_Part_fromP5.xlsx\n",
      "P6_parts/9500_9600_Part_fromP5.xlsx\n",
      "P6_parts/9600_9700_Part_fromP5.xlsx\n",
      "P6_parts/9700_9800_Part_fromP5.xlsx\n",
      "P6_parts/9800_9900_Part_fromP5.xlsx\n",
      "P6_parts/9900_10000_Part_fromP5.xlsx\n",
      "P6_parts/10000_10100_Part_fromP5.xlsx\n",
      "P6_parts/10100_10200_Part_fromP5.xlsx\n",
      "P6_parts/10200_10300_Part_fromP5.xlsx\n",
      "P6_parts/10300_10334_Part_fromP5.xlsx\n",
      "There are 5399325 functions in P6 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Store number of rows for each data packet of P6\n",
    "P6_size_list=[]\n",
    "Eg_numb_functs_init_counter=[]\n",
    "\n",
    "#Main Body of calculations\n",
    "for i in range(0, 10300, 100):\n",
    "    #Stores number for this packet in P6_size_list\n",
    "    start=i\n",
    "    end=start+100\n",
    "    P6_part=pd.read_excel(f'P6_parts/{start}_{end}_Part_fromP5.xlsx', index_col=0)\n",
    "    print(f'P6_parts/{start}_{end}_Part_fromP5.xlsx')\n",
    "    row_numb=P6_part.shape[0]\n",
    "    P6_size_list.append(row_numb)\n",
    "\n",
    "    # In this packet gets Eg_Size and get counter for eg sizes and number of functions in Eg_numb_functs_init_counter\n",
    "    values = P6_part['Eg_Size'].value_counts(ascending=True).keys().tolist()\n",
    "    counts = P6_part['Eg_Size'].value_counts(ascending=True).tolist()\n",
    "    zipped=list(zip(values,counts))\n",
    "    Eg_numb_functs_init_counter.extend(zipped)\n",
    "    \n",
    "# It remains to do 10300-10334\n",
    "#Stores number for this packet in P6_size_list\n",
    "start,end=(10300,10334)\n",
    "P6_part=pd.read_excel(f'P6_parts/{start}_{end}_Part_fromP5.xlsx', index_col=0)\n",
    "print(f'P6_parts/{start}_{end}_Part_fromP5.xlsx')\n",
    "row_numb=P6_part.shape[0]\n",
    "P6_size_list.append(row_numb)\n",
    "\n",
    "# In this packet gets Eg_Size and get counter for eg sizes and number of functions in Eg_numb_functs_init_counter\n",
    "values = P6_part['Eg_Size'].value_counts(ascending=True).keys().tolist()\n",
    "counts = P6_part['Eg_Size'].value_counts(ascending=True).tolist()\n",
    "zipped=list(zip(values,counts))\n",
    "Eg_numb_functs_init_counter.extend(zipped)\n",
    "\n",
    "#output-------------------------------------------------------------------------\n",
    "\n",
    "#Final answer\n",
    "P6_size=sum(P6_size_list)\n",
    "print(f\"There are {P6_size} functions in P6 \\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b948efa-6cdc-4fa2-a7df-4988ca3e5a91",
   "metadata": {
    "tags": []
   },
   "source": [
    "We get Eg_numb_functs_init_counter as an output also this records the decomposition of the sum $|P6|$. Eg_numb_functs_init_counter consists of (x,y) where x=|Eg| and y= x \\cdot the number of times x appeared in that packet. Therefore it is neccsary to sum to get (x,Y) where Y isx \\cdot the number of times x appeared in all packets. To get the number of times x appears it is necessary to divide Y by x."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "538f4675-80f2-4c83-bd79-e8e74cd9597d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Which decomposes as follows:\n",
      " 5399325=114*125+121*65+126*237+138*112+158*248+162*247+165*132+169*117+173*132+178*122+182*106+183*132+187*24+192*134+194*123+208*144+209*22+216*234+217*126+218*61+221*125+227*22+229*307+230*250+232*120+233*119+236*122+241*30+242*102+247*61+248*53+250*114+251*59+260*184+268*106+269*64+271*186+273*118+274*56+278*52+279*41+281*136+284*112+289*67+293*133+294*37+300*133+305*67+313*123+324*123+325*132+326*101+329*181+338*120+343*59+345*113+356*19+381*121+390*44+397*107+419*20+420*46+421*23+422*39+429*64+433*35+436*128+437*23+455*64+477*53+496*48+502*16+509*38+518*12+520*107+524*20+531*36+538*76+539*62+553*36+555*66+566*144+578*61+605*62+607*66+613*11+640*28+659*30+759*48+765*21+781*108+802*119+814*42+881*42+938*8+974*135+977*45+983*135+1002*45+1026*126+1029*63+1033*63+1068*96+1121*108+1145*22+1178*39+1189*39+1209*44+1212*11+1224*66+1257*44+1312*11+1322*32+1377*36+1409*16+1421*10+1450*54+1511*20+1666*21+1753*24+2110*42+2193*95+2285*4+2290*54+3544*12+3673*42+3820*48+3983*18+6280*1+6474*6+6702*14+6960*16+7250*9+7580*4\n"
     ]
    }
   ],
   "source": [
    "#He now obtain Eg_numb_functs from Eg_numb_functs_init_counter\n",
    "\n",
    "# This is a List of tuples where have  joined e.g (13, 6),(13, 19) to (13, 25) from Eg_numb_functs_init_counter\n",
    "\n",
    "from collections import defaultdict\n",
    "\n",
    "sums = defaultdict(int)\n",
    "\n",
    "for first, second in Eg_numb_functs_init_counter:\n",
    "    sums[first] += second\n",
    "\n",
    "Eg_numb_functs = [(first, second) for first, second in sums.items()]\n",
    "Eg_numb_functs = sorted(Eg_numb_functs, key=lambda x: x[0])#order so in increasing with respect to eg size\n",
    "\n",
    "#Build a string using a for loop, using the tuples in Eg_numb_functs\n",
    "f1,f2=Eg_numb_functs[0]\n",
    "\n",
    "f2_div=int(f2/f1)\n",
    "\n",
    "stringer=f'{f1}*{f2_div}'\n",
    "\n",
    "for item in Eg_numb_functs[1:]: #first item in stringer already\n",
    "    eg,numb=item #eg,numb not all the same\n",
    "    num_div=int(numb/eg)\n",
    "    s=f'+{eg}*{num_div}'\n",
    "    stringer += s\n",
    "    \n",
    "#Output\n",
    "print(f\"Which decomposes as follows:\\n {P6_size}={stringer}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "4d9539b3-a3bb-4ff7-982e-82777d3694ed",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5399325"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Check output calculates 5399325 #Yes\n",
    "s=114*125+121*65+126*237+138*112+158*248+162*247+165*132+169*117+173*132+178*122+182*106+183*132+187*24+192*134+194*123+208*144+209*22+216*234+217*126+218*61+221*125+227*22+229*307+230*250+232*120+233*119+236*122+241*30+242*102+247*61+248*53+250*114+251*59+260*184+268*106+269*64+271*186+273*118+274*56+278*52+279*41+281*136+284*112+289*67+293*133+294*37+300*133+305*67+313*123+324*123+325*132+326*101+329*181+338*120+343*59+345*113+356*19+381*121+390*44+397*107+419*20+420*46+421*23+422*39+429*64+433*35+436*128+437*23+455*64+477*53+496*48+502*16+509*38+518*12+520*107+524*20+531*36+538*76+539*62+553*36+555*66+566*144+578*61+605*62+607*66+613*11+640*28+659*30+759*48+765*21+781*108+802*119+814*42+881*42+938*8+974*135+977*45+983*135+1002*45+1026*126+1029*63+1033*63+1068*96+1121*108+1145*22+1178*39+1189*39+1209*44+1212*11+1224*66+1257*44+1312*11+1322*32+1377*36+1409*16+1421*10+1450*54+1511*20+1666*21+1753*24+2110*42+2193*95+2285*4+2290*54+3544*12+3673*42+3820*48+3983*18+6280*1+6474*6+6702*14+6960*16+7250*9+7580*4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4775c84-bb6d-439b-8a08-5a103b6db822",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": [],
    "toc-hr-collapsed": true
   },
   "source": [
    "# Leftovers (Please ignore)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b2aeffe-7291-4f7d-b542-901fff919010",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def get_Pnp1_for_single_funct(Pn,n):\n",
    "    \n",
    "#     #inputs\n",
    "#     # Pn:dataframe and n.\n",
    "    \n",
    "#     \"\"\"\n",
    "#     Returns a dataframe of extensions in Pn+1 for dataframe of functions in Pn.\n",
    "#     \"\"\"\n",
    "#     \"\"\"\n",
    "#     #Example\n",
    "#     n=3\n",
    "#     power_set_str,power_set_str_np1=get_n_np1_powersets(n)\n",
    "#     # P3\n",
    "\n",
    "#     funct_list=[{'1': 0,'2': 0,\"3\":0,\"12\":0,\"23\":0,\"13\":0,\"123\":0}]\n",
    "#     Pn=pd.DataFrame(funct_list)\n",
    "\n",
    "#     #The following should be functions in P4 that are extendions of functions in funct_list\n",
    "#     f_P3=get_Pnp1_for_single_funct(Pn,n)\n",
    "#     print(f_P3)\n",
    "#     \"\"\"\n",
    "\n",
    "#     #New data created\n",
    "#     power_set_str,power_set_str_np1=get_n_np1_powersets(n)\n",
    "\n",
    "#     #Load Pn as a list of dictionaries.\n",
    "#     list_df_Pn=Pn.to_dict(orient='records')\n",
    "\n",
    "#     #Create Pn+1 and hold as dataframe.\n",
    "#     P_n1=eg_column_Build_Pn_1(list_df_Pn,power_set_str,n)\n",
    "#     #As previous was done in E(g) parts, joint together\n",
    "#     Pnp1=extend_sets_to_Pnp1(P_n1)\n",
    "        \n",
    "#     Pnp1=get_Eg_column_Pn(Pnp1)\n",
    "    \n",
    "#     columns=list(power_set_str_np1)+[\"Eg_Size\"]\n",
    "#     Pnp1 = Pnp1.reindex(columns=columns)\n",
    "    \n",
    "#     # inspect_Pn1(Pnp1)\n",
    "\n",
    "#     return Pnp1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a709733b-7718-467e-a62f-f477342ec486",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def check_all_funct_topspace(df,power_set_str):\n",
    "#     length=[]\n",
    "#     for i,g in enumerate(df.to_dict(\"records\")): #list of dictionaries of functions\n",
    "#         check_top=check_top_for_single_g(df,i,power_set_str)\n",
    "#         if check_top == False:\n",
    "#             print(f\"Function {i} does NOT give a topolgical space: {check_top}\")\n",
    "#         else:\n",
    "#             length.append(check_top)\n",
    "            \n",
    "#     if len(df.to_dict(\"records\"))==len(length):\n",
    "#         print(\"Every function produces a topological space\" )\n",
    "#     else:\n",
    "#         print(\"something is not a topological space\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a54482ac-172c-4846-91a8-0373067a01ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def is_finite_topological_space(X, T):\n",
    "    \n",
    "#     \"\"\"\n",
    "#         # Does this account for unions? \n",
    "\n",
    "#     # X = {'a', 'b', 'c'}\n",
    "#     # T = [X, set(), {'a'}, { 'c'}, {'b'}]\n",
    "#     # print(is_finite_topological_space(X, T)) # prints False\n",
    "\n",
    "#     # This condition is not met because the union of {'a'} and {'c'} is {'a','c'} which is not in T.\n",
    "#     \"\"\"\n",
    "    \n",
    "#     if not all(isinstance(x, set) for x in T):\n",
    "#         # T must be a collection of sets\n",
    "#         print(x)\n",
    "        \n",
    "#         return False\n",
    "#     if not all(x.issubset(X) for x in T):\n",
    "#         # Each element of T must be a subset of X\n",
    "#         print(\"here1\")\n",
    "\n",
    "#         return False\n",
    "#     if not all(x.intersection(y) in T for x in T for y in T):\n",
    "#         print(\"here2\")\n",
    "\n",
    "#         # The intersection of any two elements of T must be in T\n",
    "#         return False\n",
    "#     if not all(x.union(y) in T for x in T for y in T):\n",
    "#         # The union of any two elements of T must be in T\n",
    "#         print(\"here3\")\n",
    "#         return False\n",
    "    \n",
    "#     if not X in T:\n",
    "#         print(\"here4\")\n",
    "#         print(x)\n",
    "\n",
    "#         # X must be in T\n",
    "#         return False\n",
    "#     if not set() in T:\n",
    "#         print(\"here5\")\n",
    "\n",
    "#         # The empty set must be in T\n",
    "#         return False\n",
    "#     return True\n",
    "\n",
    "# def check_top_for_single_g(df,numb,power_set_str):\n",
    "#     #Inputs:\n",
    "#     # df=Pn\n",
    "#     # numb=choice of function\n",
    "#     # power_set_str for df\n",
    "    \n",
    "#     # Output:True or False\n",
    "    \n",
    "#     Eg=get_Eg_for_single_g(df,numb,power_set_str)\n",
    "#     Top=list(set(x) for x in Eg)\n",
    "#     Top.append(set())\n",
    "    \n",
    "#     X_Eg=set(power_set_str)\n",
    "    \n",
    "#     return is_finite_topological_space(X_Eg, Top)\n",
    "\n",
    "\n",
    "# def check_df_duplicates(df1,df2,How,value):\n",
    "#         \"\"\"\n",
    "#     Objective:\n",
    "#     Input:\n",
    "#     Returns:\n",
    "#     \"\"\"\n",
    "#     #Compare df1,df2 if that common rows\n",
    "    \n",
    "#     # how is either \"right\" or \"left\"\n",
    "#     # for left gives those in df1 and says True or false if also in df2\n",
    "    \n",
    "#     #     example\n",
    "#     #     df1 = pd.DataFrame({'team' : ['A', 'B', 'C', 'D', 'E'], \n",
    "#     #                     'points' : [12, 15, 22, 29, 24]}) \n",
    "#     #  #create second DataFrame\n",
    "#     # df2 = pd.DataFrame({'team' : ['A', 'D', 'F', 'G', 'H'],\n",
    "#     #                     'points' : [12, 29, 15, 19, 10]})\n",
    "    \n",
    "#     #merge two dataFrames and add indicator column\n",
    "#     all_df = pd.merge(df1, df2, how=How, indicator='exists')\n",
    "\n",
    "#     #add column to show if each row in first DataFrame exists in second\n",
    "#     all_df['exists'] = np.where(all_df.exists == 'both', True, False)\n",
    "\n",
    "#     #view updated DataFrame\n",
    "#     # print (all_df)\n",
    "\n",
    "#     m=all_df.loc[all_df['exists'] == True]\n",
    "    \n",
    "#     #Number of terms common\n",
    "#     num_items = m.loc[m['exists'] == value].shape[0]\n",
    "#     print(f\"Number of those in both: {num_items} \\n\")\n",
    "\n",
    "#     return m    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82245cc3-6aef-4d47-9238-309d07fdd4ea",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Examples for understanding code"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "812f3d77-e016-4f04-a3ce-3f15d6394da2",
   "metadata": {},
   "source": [
    "Are primitive closed sets a basis for the topology? (Maybe not relevant)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc939b0b-3234-4592-8b67-cadea14c5757",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "Asides:\n",
    "- Is Eg a matriod? No does not satisfy downward closure. In our sepcific example f(12)=1,f(123)=1 else 0 Take {1} in the closure of 1 i.e {1,13}, {1} is not a closed set.\n",
    "- Are primite closed sets a basis for the topology? Is there a way to construct $\\mathbb{E}(g)$ using primitive (using minimal basis)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6ae0abb-2438-4c3d-b8e3-9b22e1c8d0ad",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# dict_PC,set_PC=get_primitive_for_single_g(df,numb)\n",
    "\n",
    "# # print(dict_PC) #Remembers what the terms of powerset is\n",
    "# set_PC.append(frozenset()) # add empty set: this is the set of primitive closed sets\n",
    "# # for i in set_PC:\n",
    "# #     print(set(i))\n",
    "    \n",
    "# X_Eg= {'13', '23', '123', '12', '2', '3', '1'} #total space ie powerset\n",
    "# Prim_Top= list(set(x) for x in set_PC) # primitive set as a topology? is it a basis?\n",
    "\n",
    "# def is_basis(X, T, B):\n",
    "#     if not all(isinstance(x, set) for x in B):\n",
    "#         # B must be a collection of sets\n",
    "#         return False\n",
    "#     if not all(x.issubset(X) for x in B):\n",
    "#         # Each element of B must be a subset of X\n",
    "#         return False\n",
    "    \n",
    "    \n",
    "#     if not all(x.intersection(y) == set() for x in B for y in B if x != y):\n",
    "#         # The intersection of any two distinct elements of B must be empty\n",
    "#         return False\n",
    "    \n",
    "#     if not all(x.union(y) in T for x in B for y in B):\n",
    "#         # The union of any two elements of T must be in T\n",
    "#         return False\n",
    "    \n",
    "#     if not all(x.issubset(y) or y.issubset(x) for x in B for y in T):\n",
    "#         # Each element of B must be a subset of some element of T or conversely\n",
    "#         return False\n",
    "#     return True\n",
    "\n",
    "# # Example usage\n",
    "# X = {'a', 'b', 'c'}\n",
    "# T = [X, set(), {'a', 'b'}, {'b', 'c'}, {'b'}]\n",
    "# B = [{'a', 'b'}, {'b', 'c'}]\n",
    "# print(is_basis(X, T, B)) # prints True\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e67f6d6-6df5-45ad-9f97-3a91381e76c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get all functions in P3 stored a data frame\n",
    "\n",
    "#Construct P3\n",
    "pre_functs=[[\"12\",\"123\"],[\"13\",\"123\"],[\"23\",\"123\"],[\"12\",\"13\",\"123\"],[\"12\",\"23\",\"123\"],[\"13\",\"23\",\"123\"],[\"12\",\"13\",\"23\",\"123\"],[],[\"123\"]]\n",
    "ds=[]\n",
    "for vals in pre_functs:\n",
    "    builder1=[(x,1) for x in vals]\n",
    "    builder0=[(x,0) for x in power3 if x not in vals]\n",
    "    f_dict={**dict(builder1),**dict(builder0)}\n",
    "    ds.append(f_dict)\n",
    "big={'123': 2, '1': 0, '2': 0, '3': 0, '12': 1, '13': 1, '23': 1} #remaining case, with 2 value\n",
    "dp=ds+[big,]\n",
    "\n",
    "#Build dataframe\n",
    "P3 = pd.DataFrame(dp)\n",
    "\n",
    "#order columns of P3\n",
    "power3=get_powerset_str(3) #powerset on 3 elements #for \n",
    "column_order=power3 #as strings\n",
    "P3 = P3.reindex(columns=column_order)\n",
    "\n",
    "print(P3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50638e8e-56f6-42ea-a952-78841485fa15",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Checking if the calculations for P5, are the same as our old calculations for P5 in xlsx.\n",
    "\n",
    "# #Previous data P5 in correct format\n",
    "# dataframe2 = pd.read_excel('P4_P5_xlsx\\P_5.xlsx')\n",
    "\n",
    "# #Make sure columns are in the same order as P5\n",
    "\n",
    "# #Reordering columns\n",
    "# column_order=[int(x) for x in power_set_str_np1]\n",
    "# dataframe2 = dataframe2.reindex(columns=column_order)\n",
    "\n",
    "# #Making sure columns are the same, in string format\n",
    "# dataframe2=string_cols(dataframe2)\n",
    "\n",
    "# # print(dataframe2.head)\n",
    "# print(dataframe2.shape) #(10334, 31)\n",
    "\n",
    "# # n=4\n",
    "# # power_set_str,alt_power_set_str_np1=get_n_np1_powersets(n)\n",
    "# # check_functs_MSA(dataframe2,power_set_str_np1).shape #(0,0) i.e all are msa\n",
    "\n",
    "# Comparision of P5 and dataframe 2. We see they are the same.\n",
    "\n",
    "# compar=check_df_duplicates(P5,dataframe2,\"left\",True) #True:10334 , False:0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8bcb6fb-157f-48ad-a4f5-02b4fee4c27d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # The calculations for P4, are the same as our old calculations for P4 in xlsx.\n",
    "\n",
    "# dataframe1 = pd.read_excel('P4_P5_xlsx\\P_4.xlsx')\n",
    "# #Making sure columns are the same, in string format and order as P4 above\n",
    "# dataframe1=string_cols(dataframe1)\n",
    "\n",
    "# # #Comparision of P4 and xlsx version\n",
    "# compar=check_df_duplicates(P4,dataframe1,\"left\",True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b88dc3c7-0fb0-44e6-8253-15e1f937bbb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# n=3\n",
    "# # Create a set with 3 elements\n",
    "# # s = {1, 2, 3}\n",
    "# s=set(range(1,n+1))\n",
    "\n",
    "# # Generate all possible subsets of the set\n",
    "# power_set = [set(x)  for r in range(len(s) + 1) for x in itertools.combinations(s, r)]\n",
    "\n",
    "# # Print the power set\n",
    "# print(power_set)\n",
    "\n",
    "# power_set_str=[set_to_string(x) for x in power_set]\n",
    "# power_set_str=[set_to_string(x) for x in power_set if len(x)>0]\n",
    "# power_set_str\n",
    "\n",
    "# #Get a function in P3\n",
    "\n",
    "# val1_loc=[\"12\",\"123\"] #where it take 1\n",
    "# val0_loc= [x for x in power_set_str if x not in val1_loc] #where it takes 0\n",
    "# # print(val0_loc)\n",
    "\n",
    "# builder1=[(x,1) for x in val1_loc]\n",
    "# builder0=[(x,0) for x in val0_loc]\n",
    "# builder0\n",
    "# dict(builder0)\n",
    "# dict(builder1)\n",
    "# f_dict={**dict(builder1),**dict(builder0)}\n",
    "# f_dict\n",
    "\n",
    "# A,B=\"1\",\"123\"\n",
    "# print(f\"Is {A} subset of {B}, f minimal? : {fmin(A,B,f_dict)}\")\n",
    "# print(f\"Is {A} subset of {B}, f maximal? : {fmax(A,B,f_dict)}\")\n",
    "\n",
    "#Testing for obtaining primitive sets:\n",
    "# #Example\n",
    "# for x in power_set_str:\n",
    "#     print(x,get_barj({x},f_dict))\n",
    "\n",
    "#As all the closed sets, checked by hand:\n",
    "# 3 T term {'13', '123', '1', '3', '23', '2'}\n",
    "# 1 {'1', '13'}\n",
    "# 2 {'23', '2'}\n",
    "# 12 {'1', '12', '123', '13', '2', '23'} \n",
    "# 13 {'13'} # should be just 13\n",
    "# 23 {'23'} # should be 23\n",
    "# 123 {'13', '123', '1', '23', '2'} # should not have 3 or 12\n",
    "\n",
    "# Store primitive closed sets \n",
    "\n",
    "# PC=[(x,get_barj({x},f_dict)) for x in power_set_str]\n",
    "# dict_PC=dict(PC)\n",
    "# set_PC=[sorted_frozenset(get_barj({x},f_dict)) for x in power_set_str]\n",
    "# # set(set_PC)\n",
    "# Eg={sorted_frozenset(power_set_str)}.union(set_PC)\n",
    "\n",
    "# Using main functions:\n",
    "# Eg=list(get_Eg(dict_PC,power_set_str,set_PC))\n",
    "# # print(len(x)) # I get like 13 so this is an issue.\n",
    "# Eg\n",
    "# Eg.append(\"empty\")\n",
    "# Eg\n",
    "\n",
    "\n",
    "#Rows are not in the same order:\n",
    "# # print(P4.equals(dataframe1))\n",
    "\n",
    "#Checking if duplicates, #https://stackoverflow.com/questions/48647534/find-difference-between-two-data-frames?noredirect=1&lq=1\n",
    "# df = P4.merge(dataframe1, how = 'inner' ,indicator=False)\n",
    "\"\"\" \n",
    "# first dataframe\n",
    "df1 = pd.DataFrame({\n",
    "    'Age': ['20', '14', '56', '28', '10'],\n",
    "    'Weight': [59, 29, 73, 56, 48]})\n",
    "display(df1)\n",
    "\n",
    "# second dataframe\n",
    "df2 = pd.DataFrame({\n",
    "    'Age': ['16', '20', '24', '40', '22'],\n",
    "    'Weight': [55, 59, 73, 85, 56]})\n",
    "display(df2)\n",
    "\n",
    "#Common rows:\n",
    "df = df1.merge(df2, how = 'inner' ,indicator=False)\n",
    "print(df)\n",
    "\"\"\"\n",
    "\n",
    "\"The above method only works for those data frames that don't already have duplicates themselves. For example:\"\n",
    "no_dups=pd.concat([P4,dataframe1]).drop_duplicates(keep=False)\n",
    "no_dups.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6909d71f-36f7-427c-80b0-af89bc3a05e7",
   "metadata": {
    "tags": [],
    "toc-hr-collapsed": true
   },
   "source": [
    "## E(g) is a topolgical space <a name=\"s4\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60cd9cf7-747b-4202-b2a1-978cbbf0d882",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": [],
    "toc-hr-collapsed": true
   },
   "source": [
    "### Example f in P3 form a Topological space <a name=\"s41\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "580cf815-a85f-4c17-acd8-1d9b93849e69",
   "metadata": {},
   "source": [
    "The set $\\mathbb{E}(g)$ is the topology assoicated to the function $g$ and can be used to construct the extensions of $g$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11afd118-d73e-41e0-a898-67184ba96369",
   "metadata": {},
   "source": [
    "We now have:\n",
    "- A function/method to get $\\mathbb{E}(g)$.\n",
    "- A function to check if $\\mathbb{E}(g)$ is a finite topology.\n",
    "- Check if all function in P3,P4,P5 generate a topology: Answer P3,P4 yes, examples of P5 yes\n",
    "\n",
    "What we could do next:\n",
    "- A way to determine the size of $\\mathbb{E}(g)$ (using basis).\n",
    "- Are all Top(g) for $g \\in P_n$ homeomorphic? (topologies are different sizes)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19c02253-1861-4545-a7ec-d524e0581875",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Specific Example from P3 is topological space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "e0cbb07e-026b-4f98-bde2-34061b09c1c9",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'1': 0, '2': 0, '3': 0, '12': 0, '13': 0, '23': 0, '123': 0} \n",
      "\n",
      "{'12', '123', '23'}\n",
      "{'12', '1', '123', '13'}\n",
      "{'123', '3', '23', '13'}\n",
      "{'3', '2', '23', '12', '1', '123', '13'}\n",
      "{'123'}\n",
      "{'3', '23', '12', '123', '13'}\n",
      "{'2', '23', '12', '1', '123', '13'}\n",
      "{'12', '123', '23', '13'}\n",
      "{'12', '123', '13'}\n",
      "{'23', '12', '1', '123', '13'}\n",
      "{'12', '23', '123', '2'}\n",
      "{'12', '123'}\n",
      "{'123', '23'}\n",
      "{'3', '23', '2', '12', '123', '13'}\n",
      "{'123', '13'}\n",
      "{'23', '2', '12', '123', '13'}\n",
      "{'123', '23', '13'}\n",
      "{'3', '23', '12', '1', '123', '13'}\n",
      "set()\n",
      "\n",
      "  All closed sets\n"
     ]
    }
   ],
   "source": [
    "power_set_str=get_powerset_str(3)\n",
    "\n",
    "df=P3\n",
    "numb=4\n",
    "#Get data\n",
    "Eg=get_Eg_for_single_g(df,numb,power_set_str)\n",
    "# print(power_set_str)\n",
    "print(P3.iloc[numb].to_dict(),\"\\n\")\n",
    "\n",
    "for term in Eg:\n",
    "    print(set(term))\n",
    "    \n",
    "print(\"\\n  All closed sets\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abb349a0-98f1-4201-af27-60a38a21983d",
   "metadata": {},
   "source": [
    "Check if Eg is a finite topological space on the powerset for an example g in P3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28575e50-b390-4bc5-9a0a-759606995780",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_Eg= {'13', '23', '123', '12', '2', '3', '1'} #total space ie powerset\n",
    "Top=list(set(x) for x in Eg) #topology\n",
    "\n",
    "print(is_finite_topological_space(X_Eg, Top))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52a7c114-a8a1-454e-88d6-75f0c220a1a2",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Consider Top(g) for two different g,g' in P3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bda02a9-abbd-46d2-8ab9-78580bafcace",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Setup data\n",
    "power_set_str=get_powerset_str(3)\n",
    "df=P3\n",
    "X_Eg= set(power_set_str) #total space ie powerset\n",
    "\n",
    "#Inputs\n",
    "numb1=0\n",
    "numb2=1\n",
    "\n",
    "#Top space 1\n",
    "Eg1=get_Eg_for_single_g(df,numb,power_set_str)\n",
    "Top1=list(set(x) for x in Eg1) #topology\n",
    "print(is_finite_topological_space(X_Eg, Top1))\n",
    "#------------------------------\n",
    "#Top space 2\n",
    "Eg2=get_Eg_for_single_g(df,numb2,power_set_str)\n",
    "Top2=list(set(x) for x in Eg2) #topology\n",
    "print(is_finite_topological_space(X_Eg, Top2))\n",
    "#------------------------------\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4f2db93-ca23-46b3-a4f9-76e4c132804a",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": [],
    "toc-hr-collapsed": true
   },
   "source": [
    "### Check if all function in P3,P4,P5 generate a topological space: Yes <a name=\"s42\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14c8e0f6-a8e7-4bea-9f36-324fb2e37861",
   "metadata": {
    "tags": []
   },
   "source": [
    "### P3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3492c0c7-9d9b-4f70-b8f2-d88967224c22",
   "metadata": {},
   "outputs": [],
   "source": [
    "#inputs\n",
    "df=P3\n",
    "power_set_str=get_powerset_str(3)\n",
    "check_all_funct_topspace(df,power_set_str)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2524d1a-961d-416b-b30f-59476b66e14a",
   "metadata": {
    "tags": []
   },
   "source": [
    "### P4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "473e1069-5d02-4497-acee-2729441e1377",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#inputs\n",
    "df=P4\n",
    "power_set_str=get_powerset_str(4)\n",
    "check_all_funct_topspace(df,power_set_str) #False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb5149c1-f805-426f-8719-0b5914af8c3b",
   "metadata": {
    "tags": []
   },
   "source": [
    "### P5 (where Eg for a function allow for calc of part of P6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0f69c5a-613c-4e92-ba29-3f5a151aa6dd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Take long: \n",
    "\n",
    "#inputs\n",
    "df=P5\n",
    "power_set_str=get_powerset_str(5)\n",
    "check_all_funct_topspace(df,power_set_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc848042-cdd3-4222-8a95-95d5e9a1ddb6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Works for some choosen example.\n",
    "\n",
    "df=P5\n",
    "power_set_str=get_powerset_str(5)\n",
    "numb=0 # function indexed at 0\n",
    "\n",
    "Eg=get_Eg_for_single_g(df,numb,power_set_str) # set of closed sets\n",
    "X_Eg= set(power_set_str) #total space ie powerset\n",
    "Top=list(set(x) for x in Eg) #topology from closed sets Eg #Size: 1178\n",
    "\n",
    "print(len(Top))\n",
    "\n",
    "#Check whether topological space:\n",
    "print(is_finite_topological_space(X_Eg, Top)) #The total powerset on 4 elements is not included.\n",
    "\n",
    "#Result:\n",
    "#numb=0\n",
    "#1178 #Top size\n",
    "#True"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "toc-autonumbering": false,
  "toc-showcode": false,
  "toc-showmarkdowntxt": false,
  "toc-showtags": false
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
